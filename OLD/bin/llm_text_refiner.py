# scripts/llm_text_refiner.py
import os, sys, re

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langdetect import detect_langs
from langchain_ollama import ChatOllama

from OLD.src import bootstrap
from OLD.src.error_handler import install_global_exception_handler
from OLD.src.config import OLLAMA_BASE, OLLAMA_MODEL
from OLD.db.schema import TABLE_FILES

from OLD.ocr import correct_text
from OLD.llm.prompt_loader import get_prompt_by_lang
from OLD.llm.scorer import score_text_quality
from OLD.llm import detect_language
from OLD.src.utils import debug_print

# REM: ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«è¨­å®š
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

# REM: ä¾‹å¤–ç™ºç”Ÿæ™‚ã®ãƒ­ã‚°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨˜éŒ²ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ã‚’æœ‰åŠ¹åŒ–
install_global_exception_handler()

def refine_text_with_llm(raw_text: str,
                         model: str = OLLAMA_MODEL,
                         force_lang: str = None):
    # 1) OCRèª¤å­—è£œæ­£
    corrected = correct_text(raw_text)

    # 2) è¨€èªåˆ¤å®šï¼å¼·åˆ¶ï¼ˆã“ã“ã§ã¯æ—¥æœ¬èªå›ºå®šï¼‰
    lang = detect_language(corrected, force_lang)
    lang = "ja"

    # 3) ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šæ–‡å­—æ•°ã¨æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    text_len       = len(corrected)
    token_estimate = int(text_len * 1.6)
    debug_print(f"ğŸ§  LLMæ•´å½¢ã‚’é–‹å§‹ï¼ˆæ–‡å­—æ•°: {text_len}, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: {token_estimate}ï¼‰", flush=True)
    debug_print(f"ğŸ”¤ æ•´å½¢è¨€èª: {lang}", flush=True)

    # 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—
    _, base_prompt = get_prompt_by_lang(lang)

    # 5) æ—¥æœ¬èªå›ç­”ã‚’å¼·åˆ¶
    user_prompt = "æ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n" + base_prompt

    # 6) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆï¼ˆ{TEXT} ã¨ {input_text} ã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
    prompt = user_prompt.replace("{TEXT}", corrected).replace("{input_text}", corrected)
    debug_print(f"ğŸ§¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆå¾Œã®æ–‡å­—æ•°: {len(prompt)}", flush=True)

    # 7) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ‹¡å¼µ
    generation_kwargs = {
        "max_new_tokens": 1024,
        "min_length": max(1, int(len(corrected) * 0.8)),
        "temperature": 0.7,
        # "top_p": 0.9,  # å¿…è¦ãªã‚‰è¿½åŠ 
    }

    # 8) Ollama LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    llm = ChatOllama(
        model=model,
        base_url=OLLAMA_BASE,
        **generation_kwargs
    )

    # 9) LangChain ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    prompt_template = PromptTemplate.from_template(user_prompt)
    chain = prompt_template | llm | StrOutputParser()

    # 10) æ¨è«–å®Ÿè¡Œ
    refined_text = chain.invoke({"TEXT": corrected})

    # 11) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
    score = score_text_quality(corrected, refined_text, lang)
    return refined_text, lang, score
