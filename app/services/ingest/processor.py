# app/services/ingest/processor.py
# Ingestå‡¦ç†ã®æ–°ã—ã„å®Ÿè£…ï¼ˆå…±é€šOCRã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨ï¼‰

import functools
import time
import unicodedata
import os
import logging
from typing import Dict, Generator, List

from src.config import OLLAMA_MODEL
from fileio.file_embedder import embed_and_insert
from llm.chunker import split_into_chunks
from llm.refiner import (
    normalize_empty_lines,
    refine_text_with_llm,
    build_prompt,
)
from ocr.spellcheck import correct_text
from db.handler import (
    insert_file_full,
    update_file_text,
    update_file_status,
    get_file_text,
)

# æ–°ã—ã„å…±é€šOCRã‚µãƒ¼ãƒ“ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from app.services.ocr import OCREngineFactory

# å®šæ•°
TOK_LIMIT = 8192
CHUNK_SIZE = 500
OVERLAP_SIZE = 50
LOGGER = logging.getLogger("ingest_processor")

# è‹±èªãƒ†ãƒ³ãƒ—ãƒ¬èª¤åæ˜ ãªã©ã«è©²å½“ã™ã‚‹å…¸å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆloweræ¯”è¼ƒå‰æï¼‰
TEMPLATE_PATTERNS = [
    "certainly", "please provide", "reformat", "i will help",
    "note that this is a translation", "based on your ocr output"
]

class IngestProcessor:
    """Ingestå‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.ocr_factory = OCREngineFactory()
    
    def process_file(
        self,
        *,
        file_path: str,
        file_name: str,
        index: int,
        total_files: int,
        refine_prompt_key: str,
        refine_prompt_text: str = None,
        embed_models: List[str],
        overwrite_existing: bool,
        quality_threshold: float,
        llm_timeout_sec: int,
        abort_flag: Dict[str, bool],
        ocr_engine_id: str = None,  # æ–°è¦è¿½åŠ : OCRã‚¨ãƒ³ã‚¸ãƒ³æŒ‡å®š
        ocr_settings: Dict = None,  # æ–°è¦è¿½åŠ : OCRè¨­å®š
    ) -> Generator[Dict, None, None]:
        """1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã‚¤ãƒ™ãƒ³ãƒˆã‚’ yield"""
        t0 = time.perf_counter()

        # â”€â”€ â‘  DB ä»®ç™»éŒ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            file_id = insert_file_full(file_path, "", "", 0.0)
        except Exception as exc:
            LOGGER.exception("insert_file_full")
            yield {"file": file_name, "step": "DBç™»éŒ²å¤±æ•—", "detail": str(exc)}
            return

        yield {
            "file": file_name,
            "file_id": file_id,
            "step": "ãƒ•ã‚¡ã‚¤ãƒ«ç™»éŒ²å®Œäº†",
            "index": index,
            "total": total_files,
        }

        # â”€â”€ â‘¡ ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆæ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨ï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        yield {"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºé–‹å§‹"}
        
        try:
            # OCRå‡¦ç†ã‚’ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦å®Ÿè¡Œã—ã€é€²æ—ã‚’ç›´æ¥yield
            pages = []
            for event_or_result in self._extract_text_with_ocr_generator(
                file_path, 
                file_name, 
                ocr_engine_id, 
                ocr_settings
            ):
                if isinstance(event_or_result, dict) and "step" in event_or_result:
                    # é€²æ—ã‚¤ãƒ™ãƒ³ãƒˆã‚’å³åº§ã«yield
                    yield event_or_result
                elif isinstance(event_or_result, list):
                    # æœ€çµ‚çµæœï¼ˆãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆï¼‰ã‚’å—ã‘å–ã‚Š
                    pages = event_or_result
        except Exception as exc:
            LOGGER.exception("extract_text_with_ocr")
            update_file_status(file_id, status="error", note=str(exc))
            yield {"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—", "detail": str(exc)}
            return

        if not pages:
            update_file_status(file_id, status="error", note="no text")
            yield {"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆç„¡ã—"}
            return

        yield {"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†", "pages": len(pages)}

        # â”€â”€ â‘¢ å…¨æ–‡ç”Ÿæˆ â†’ raw_text ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        page_blocks = [
            normalize_empty_lines(correct_text(unicodedata.normalize("NFKC", p)))
            for p in pages
        ]
        full_text = "\n\n".join(page_blocks)
        update_file_text(file_id, raw_text=full_text)

        token_est = int(len(full_text) * 1.6)
        use_page = token_est > TOK_LIMIT
        unit_iter = enumerate(page_blocks, 1) if use_page else [("all", full_text)]

        refined_pages: List[str] = []
        file_min_score: float = 1.0

        # â”€â”€ â‘£ LLM æ•´å½¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # EMLãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯æ®µè½åˆ¥æ•´å½¢å‡¦ç†ã‚’é©ç”¨
        ext = os.path.splitext(file_path)[1].lower()
        if ext == ".eml":
            yield {"file": file_name, "step": "ğŸ“§ EMLãƒ•ã‚¡ã‚¤ãƒ«æ®µè½æ•´å½¢ãƒ¢ãƒ¼ãƒ‰é©ç”¨"}
            
            # å¼•ç”¨è¨˜å·ã®é™¤å»ã¨æ®µè½åˆ†å‰²
            corrected = full_text.replace(">>", "").replace("> >", "").replace("> ", "")
            paragraphs = [p.strip() for p in corrected.split("\n\n") if len(p.strip()) > 30]
            
            yield {"file": file_name, "step": f"æ®µè½åˆ†å‰²å®Œäº†: {len(paragraphs)}å€‹ã®æ®µè½"}
            
            refined_parts = []
            for i, para in enumerate(paragraphs):
                if abort_flag["flag"]:
                    return
                    
                yield {"file": file_name, "step": f"ğŸ“‘ æ®µè½ {i+1}/{len(paragraphs)} ã‚’æ•´å½¢ä¸­..."}
                
                try:
                    # æ®µè½ã”ã¨ã«LLMæ•´å½¢
                    refined, _, score, _ = refine_text_with_llm(
                        para, 
                        model=OLLAMA_MODEL,
                        force_lang=refine_prompt_key,
                        abort_flag=abort_flag
                    )
                    
                    # ä¸æ­£ãªæ•´å½¢çµæœã‚’ãƒã‚§ãƒƒã‚¯
                    if self._is_invalid_llm_output(refined):
                        yield {"file": file_name, "step": f"âš ï¸ æ®µè½ {i+1}: ä¸æ­£ãªæ•´å½¢çµæœã‚’ã‚¹ã‚­ãƒƒãƒ—"}
                        continue
                        
                    refined_parts.append(refined)
                    file_min_score = min(file_min_score, score)
                    
                    yield {"file": file_name, "step": f"âœ… æ®µè½ {i+1}/{len(paragraphs)} æ•´å½¢å®Œäº†"}
                    
                except Exception as e:
                    LOGGER.warning(f"æ®µè½ {i+1} æ•´å½¢å¤±æ•—: {e}")
                    yield {"file": file_name, "step": f"âš ï¸ æ®µè½ {i+1} æ•´å½¢å¤±æ•—: {e}"}
                    continue
            
            if refined_parts:
                refined_pages = ["\n\n".join(refined_parts)]
                yield {"file": file_name, "step": f"EMLæ®µè½æ•´å½¢å®Œäº†: {len(refined_parts)}æ®µè½ã‚’çµ±åˆ"}
            else:
                yield {"file": file_name, "step": "âš ï¸ æœ‰åŠ¹ãªæ®µè½ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"}
                refined_pages = [full_text]  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        else:
            # é€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
            for pg, block in unit_iter:
                if abort_flag["flag"]:
                    return

                label = f"page{pg}" if use_page else "all"
                # ç·¨é›†ãƒšã‚¤ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ã†ï¼ˆ{TEXT}ç½®æ›ã®ã¿ï¼‰
                if refine_prompt_text:
                    cleaned = normalize_empty_lines(correct_text(block))
                    prompt_text = refine_prompt_text.replace("{TEXT}", cleaned)
                else:
                    prompt_text = build_prompt(block, refine_prompt_key)

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¦‹å‡ºã—ï¼ˆå…¨æ–‡ç”¨ï¼‰
                yield {"file": file_name, "step": f"ä½¿ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨æ–‡ part:{label}"}
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨æ–‡
                yield {
                    "file": file_name,
                    "step": "prompt_text",
                    "part": label,
                    "content": prompt_text,
                }

                # é€²è¡Œä¸­ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆâ€»å»ƒæ­¢ã—ãªã„ï¼‰
                yield {"file": file_name, "step": f"LLMæ•´å½¢ä¸­ part:{label}"}

                job = functools.partial(
                    refine_text_with_llm,
                    block,
                    model=OLLAMA_MODEL,
                    force_lang=refine_prompt_key,
                    abort_flag=abort_flag,
                )
                try:
                    refined, _, score, _ = job() if llm_timeout_sec == 0 else \
                        self._run_with_timeout(job, llm_timeout_sec)
                except TimeoutError:
                    update_file_status(file_id, status="error", note="llm timeout")
                    yield {"file": file_name, "step": "LLMã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ", "part": label}
                    return
                except Exception as exc:
                    LOGGER.exception("LLM refine")
                    update_file_status(file_id, status="error", note=str(exc))
                    yield {"file": file_name, "step": "LLMæ•´å½¢å¤±æ•—", "detail": str(exc)}
                    return

                refined_pages.append(refined)
                file_min_score = min(file_min_score, score)

                # æ•´å½¢çµæœè¦‹å‡ºã—ï¼ˆå…¨æ–‡ï¼‰
                yield {"file": file_name, "step": f"LLMæ•´å½¢çµæœå…¨æ–‡ part:{label}"}
                # æ•´å½¢çµæœå…¨æ–‡
                yield {
                    "file": file_name,
                    "step": "refined_text",
                    "part": label,
                    "content": refined,
                }

        if abort_flag["flag"] or not refined_pages:
            return

        # â”€â”€ â‘¤ ãƒãƒ£ãƒ³ã‚¯åˆ†å‰² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        final_text = "\n\n".join(refined_pages)
        chunks = split_into_chunks(final_text, chunk_size=CHUNK_SIZE, overlap=OVERLAP_SIZE)
        yield {"file": file_name, "step": f"ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº† chunks={len(chunks)}"}

        # â”€â”€ â‘¥ ãƒ™ã‚¯ãƒˆãƒ«åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        embed_job = functools.partial(
            embed_and_insert,
            texts=chunks,
            file_name=file_path,
            model_keys=embed_models,
            quality_score=file_min_score,
            file_id=file_id,
        )
        try:
            embed_job()
            yield {"file": file_name, "step": "ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†"}
        except Exception as exc:
            LOGGER.exception("embed_and_insert")
            update_file_status(file_id, status="error", note=str(exc))
            yield {"file": file_name, "step": "ãƒ™ã‚¯ãƒˆãƒ«åŒ–å¤±æ•—", "detail": str(exc)}
            return

        # â”€â”€ â‘¦ refined_text ä¿å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        cur = get_file_text(file_id) or {}
        need_upd = (
            overwrite_existing or not cur.get("refined_text") or
            (cur.get("quality_score") or 0.0) < quality_threshold or
            file_min_score < (cur.get("quality_score") or 1.0)
        )
        if need_upd:
            update_file_text(file_id, refined_text=final_text, quality_score=file_min_score)
            yield {"file": file_name, "step": "å…¨æ–‡ä¿å­˜å®Œäº†ï¼ˆä¸Šæ›¸ãå®Ÿæ–½ï¼‰"}
        else:
            yield {"file": file_name, "step": "å…¨æ–‡ä¿å­˜ã‚¹ã‚­ãƒƒãƒ—"}

        update_file_status(file_id, status="done")
        yield {
            "file": file_name,
            "step": "file_done",
            "elapsed": round(time.perf_counter() - t0, 2),
        }

    def _extract_text_with_ocr(
        self, 
        file_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None,
        progress_yield=None
    ) -> List[str]:
        """æ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == ".pdf":
            return self._extract_pdf_with_ocr(file_path, file_name, ocr_engine_id, ocr_settings, progress_yield)
        elif ext == ".docx":
            return self._extract_docx_with_ocr(file_path, file_name, ocr_engine_id, ocr_settings, progress_yield)
        elif ext == ".txt":
            if progress_yield:
                progress_yield({"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."})
            return self._extract_text_from_txt(file_path)
        elif ext == ".csv":
            if progress_yield:
                progress_yield({"file": file_name, "step": "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."})
            return self._extract_text_from_csv(file_path)
        elif ext == ".json":
            if progress_yield:
                progress_yield({"file": file_name, "step": "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."})
            return self._extract_text_from_json(file_path)
        elif ext == ".eml":
            if progress_yield:
                progress_yield({"file": file_name, "step": "EMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."})
            return self._extract_text_from_eml(file_path)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")

    def _extract_pdf_with_ocr(
        self, 
        pdf_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None,
        progress_yield=None
    ) -> List[str]:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆæ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨ï¼‰"""
        import fitz
        
        doc = fitz.open(pdf_path)
        text_list = []
        total_pages = len(doc)
        
        # OCRã‚¨ãƒ³ã‚¸ãƒ³ã®æ±ºå®š
        if not ocr_engine_id:
            ocr_engine_id = self.ocr_factory.settings_manager.get_default_engine()
        
        if progress_yield:
            progress_yield({"file": file_name, "step": f"OCRã‚¨ãƒ³ã‚¸ãƒ³: {ocr_engine_id} ã‚’ä½¿ç”¨"})

        for page_number in range(total_pages):
            if progress_yield:
                progress_yield({"file": file_name, "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1}/{total_pages} å‡¦ç†ä¸­..."})

            page = doc.load_page(page_number)

            # PyMuPDFæŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆå±¤ï¼‰
            pdf_text = page.get_text().strip()

            # æ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ã§OCRæŠ½å‡º
            ocr_result = self.ocr_factory.process_with_settings(
                engine_id=ocr_engine_id,
                pdf_path=pdf_path,
                page_num=page_number,
                custom_params=ocr_settings or {}
            )
            
            if ocr_result["success"]:
                ocr_text = ocr_result["text"].strip()
                if progress_yield:
                    progress_yield({
                        "file": file_name, 
                        "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1} OCRå®Œäº† (å‡¦ç†æ™‚é–“: {int(ocr_result['processing_time'])}ç§’)"
                    })
            else:
                ocr_text = f"OCRã‚¨ãƒ©ãƒ¼: {ocr_result['error']}"
                if progress_yield:
                    progress_yield({
                        "file": file_name, 
                        "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1} OCRã‚¨ãƒ©ãƒ¼: {ocr_result['error']}"
                    })

            # çµåˆã—ã¦1ãƒšãƒ¼ã‚¸åˆ†ã¨ã—ã¦æ ¼ç´
            merged = f"ã€PDFæŠ½å‡ºã€‘\n{pdf_text}\n\nã€OCRæŠ½å‡ºã€‘\n{ocr_text}"
            text_list.append(merged)

        doc.close()
        return text_list

    def _extract_docx_with_ocr(
        self, 
        docx_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None,
        progress_yield=None
    ) -> List[str]:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ§‹é€ ã¨OCRã‚’çµ±åˆã—ã¦æŠ½å‡º"""
        import docx
        import subprocess
        import tempfile
        
        structured_text = []

        if progress_yield:
            progress_yield({"file": file_name, "step": "Wordæ–‡æ›¸ã®æ§‹é€ ã‚’æŠ½å‡ºä¸­..."})

        # è¡¨ã‚„æ®µè½ãªã©ã€è«–ç†æ§‹é€ ãƒ™ãƒ¼ã‚¹ã§æŠ½å‡º
        doc = docx.Document(docx_path)
        structured_text.extend([p.text for p in doc.paragraphs if p.text])

        for table in doc.tables:
            for row in table.rows:
                row_text = [cell.text.strip().replace("\n", " ") for cell in row.cells if cell.text.strip()]
                if row_text:
                    structured_text.append(" | ".join(row_text))

        # OCRãƒ™ãƒ¼ã‚¹ï¼ˆLibreOfficeçµŒç”±ã§PDFåŒ– â†’ æ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨ï¼‰
        with tempfile.TemporaryDirectory() as tmpdir:
            if progress_yield:
                progress_yield({"file": file_name, "step": "Wordæ–‡æ›¸ã‚’PDFã«å¤‰æ›ä¸­..."})
                
            try:
                subprocess.run([
                    "libreoffice", "--headless", "--convert-to", "pdf",
                    "--outdir", tmpdir, docx_path
                ], check=True)
            except subprocess.CalledProcessError:
                raise RuntimeError("LibreOfficeã«ã‚ˆã‚‹PDFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")

            basename = os.path.splitext(os.path.basename(docx_path))[0]
            pdf_path = os.path.join(tmpdir, f"{basename}.pdf")
            if os.path.exists(pdf_path):
                ocr_text = self._extract_pdf_with_ocr(pdf_path, file_name, ocr_engine_id, ocr_settings, progress_yield)
            else:
                raise FileNotFoundError(f"PDFå¤‰æ›å¾Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")

        # çµ±åˆï¼šã‚¿ã‚°ã‚’ä»˜ã‘ã¦ LLM æ•´å½¢ã«æ¸¡ã™
        structured_content = "\n".join(structured_text)
        ocr_content = "\n".join(ocr_text)
        
        # ä¸¡å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿”ã™
        combined_text = f"<structured_text>\n{structured_content}\n</structured_text>\n\n<ocr_text>\n{ocr_content}\n</ocr_text>"
        return [combined_text]

    def _extract_text_from_txt(self, txt_path: str) -> List[str]:
        """TXTãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        with open(txt_path, encoding="utf-8") as f:
            return [line.strip() for line in f if line.strip()]

    def _extract_text_from_csv(self, csv_path: str) -> List[str]:
        """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        import csv
        results = []
        try:
            with open(csv_path, encoding="utf-8") as f:
                reader = csv.DictReader(f)
                if "text" not in reader.fieldnames:
                    raise ValueError("CSVã«'text'ã‚«ãƒ©ãƒ ãŒå¿…è¦ã§ã™")
                for row in reader:
                    if row.get("text"):
                        results.append(row["text"].strip())
            return results
        except Exception as e:
            os.makedirs("logs", exist_ok=True)
            log_path = os.path.join("logs", "invalid_csv_log.txt")
            from datetime import datetime
            now = datetime.utcnow().isoformat() + "Z"
            with open(log_path, "a", encoding="utf-8") as logf:
                logf.write(f"{now}: Skipped {csv_path} - {e}\n")
            return []

    def _extract_text_from_json(self, json_path: str) -> List[str]:
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        import json
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
            if not isinstance(data, list):
                raise ValueError("JSONã¯ãƒªã‚¹ãƒˆå½¢å¼ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            return [item["text"] for item in data if "text" in item]

    def _extract_text_from_eml(self, file_path: str) -> List[str]:
        """EMLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        from email import policy
        from email.parser import BytesParser

        with open(file_path, "rb") as f:
            msg = BytesParser(policy=policy.default).parse(f)

        text_blocks = []
        for part in msg.walk():
            if part.get_content_type() == "text/plain":
                content = part.get_content()
                lines = content.splitlines()
                # å¼•ç”¨è¡Œï¼ˆ>>ã§å§‹ã¾ã‚‹è¡Œï¼‰ã¯é™¤å»
                lines = [line for line in lines if not line.strip().startswith(">>")]
                filtered = "\n".join(lines).strip()
                if filtered:
                    text_blocks.append(filtered)
        return text_blocks

    def _is_invalid_llm_output(self, text: str) -> bool:
        """LLMæ•´å½¢å¾Œã®å‡ºåŠ›ãŒãƒ†ãƒ³ãƒ—ãƒ¬ãƒ»è‹±èªãƒ»ç„¡æ„å‘³ãªã©ã®ä¸æ­£å†…å®¹ã‹ã‚’åˆ¤å®š"""
        try:
            from langdetect import detect
        except ImportError:
            # langdetectãŒãªã„å ´åˆã¯åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã®ã¿
            if len(text.strip()) < 30:
                return True
            lower = text.lower()
            return any(p in lower for p in TEMPLATE_PATTERNS)

        if len(text.strip()) < 30:
            return True

        lower = text.lower()
        if any(p in lower for p in TEMPLATE_PATTERNS):
            return True

        try:
            if detect(text) == "en":
                return True
        except Exception:
            return True

        return False

    def _extract_text_with_ocr_generator(
        self, 
        file_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None
    ) -> Generator:
        """OCRå‡¦ç†ã‚’ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ã¨ã—ã¦å®Ÿè¡Œã—ã€é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å ±å‘Š"""
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == ".pdf":
            yield from self._extract_pdf_with_ocr_generator(file_path, file_name, ocr_engine_id, ocr_settings)
        elif ext == ".docx":
            yield from self._extract_docx_with_ocr_generator(file_path, file_name, ocr_engine_id, ocr_settings)
        elif ext == ".txt":
            yield {"file": file_name, "step": "ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."}
            yield self._extract_text_from_txt(file_path)
        elif ext == ".csv":
            yield {"file": file_name, "step": "CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."}
            yield self._extract_text_from_csv(file_path)
        elif ext == ".json":
            yield {"file": file_name, "step": "JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."}
            yield self._extract_text_from_json(file_path)
        elif ext == ".eml":
            yield {"file": file_name, "step": "EMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ä¸­..."}
            yield self._extract_text_from_eml(file_path)
        else:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {ext}")

    def _extract_pdf_with_ocr_generator(
        self, 
        pdf_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None
    ) -> Generator:
        """PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ç‰ˆã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—å ±å‘Šï¼‰"""
        import fitz
        
        doc = fitz.open(pdf_path)
        text_list = []
        total_pages = len(doc)
        
        # OCRã‚¨ãƒ³ã‚¸ãƒ³ã®æ±ºå®š
        if not ocr_engine_id:
            ocr_engine_id = self.ocr_factory.settings_manager.get_default_engine()
        
        yield {"file": file_name, "step": f"OCRã‚¨ãƒ³ã‚¸ãƒ³: {ocr_engine_id} ã‚’ä½¿ç”¨"}

        for page_number in range(total_pages):
            # ãƒšãƒ¼ã‚¸å‡¦ç†é–‹å§‹æ™‚é–“ã‚’è¨˜éŒ²
            page_start_time = time.perf_counter()
            
            # åˆæœŸã®ã€Œå‡¦ç†ä¸­...ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            yield {
                "file": file_name, 
                "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1}/{total_pages} å‡¦ç†ä¸­...",
                "page_id": f"page_{page_number + 1}_{total_pages}",  # ãƒšãƒ¼ã‚¸è­˜åˆ¥å­
                "is_progress_update": True  # é€²æ—æ›´æ–°ãƒ•ãƒ©ã‚°
            }
            
            page = doc.load_page(page_number)

            # PyMuPDFæŠ½å‡ºï¼ˆãƒ†ã‚­ã‚¹ãƒˆå±¤ï¼‰
            pdf_text = page.get_text().strip()

            # å®šæœŸçš„ã«çµŒéæ™‚é–“ã‚’æ›´æ–°ã—ãªãŒã‚‰OCRå‡¦ç†
            import threading
            import time as time_module
            
            # OCRå‡¦ç†ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            ocr_result = None
            ocr_exception = None
            
            def run_ocr():
                nonlocal ocr_result, ocr_exception
                try:
                    ocr_result = self.ocr_factory.process_with_settings(
                        engine_id=ocr_engine_id,
                        pdf_path=pdf_path,
                        page_num=page_number,
                        custom_params=ocr_settings or {}
                    )
                except Exception as e:
                    ocr_exception = e
            
            # OCRå‡¦ç†ã‚’é–‹å§‹
            ocr_thread = threading.Thread(target=run_ocr)
            ocr_thread.start()
            
            # OCRå‡¦ç†ä¸­ã¯1ç§’ã”ã¨ã«çµŒéæ™‚é–“ã‚’æ›´æ–°
            progress_count = 0
            while ocr_thread.is_alive():
                elapsed = time.perf_counter() - page_start_time
                progress_count += 1
                
                # æœ€åˆã®é€²æ—æ›´æ–°ã¯è¡¨ç¤ºã—ãªã„ï¼ˆåˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨é‡è¤‡ã‚’é¿ã‘ã‚‹ï¼‰
                if progress_count > 1:
                    yield {
                        "file": file_name, 
                        "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1}/{total_pages} å‡¦ç†ä¸­... ({elapsed:.1f}ç§’çµŒé)",
                        "update_type": "progress_update",
                        "page_id": f"page_{page_number + 1}_{total_pages}",  # ãƒšãƒ¼ã‚¸è­˜åˆ¥å­
                        "is_progress_update": True  # é€²æ—æ›´æ–°ãƒ•ãƒ©ã‚°
                    }
                
                time_module.sleep(1)
                
                # å®‰å…¨è£…ç½®ï¼š60ç§’ä»¥ä¸ŠçµŒéã—ãŸå ´åˆã¯å¼·åˆ¶çµ‚äº†
                if elapsed > 60:
                    print(f"âš ï¸ ãƒšãƒ¼ã‚¸ {page_number + 1} OCRå‡¦ç†ãŒ60ç§’ã‚’è¶…éã—ã¾ã—ãŸ")
                    break
            
            # OCRå‡¦ç†å®Œäº†ã‚’å¾…æ©Ÿ
            ocr_thread.join()
            
            # ä¾‹å¤–ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å†ç™ºç”Ÿ
            if ocr_exception:
                raise ocr_exception
            
            if ocr_result["success"]:
                ocr_text = ocr_result["text"].strip()
                yield {
                    "file": file_name, 
                    "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1} OCRå®Œäº† (å‡¦ç†æ™‚é–“: {ocr_result['processing_time']:.2f}ç§’)"
                }
            else:
                ocr_text = f"OCRã‚¨ãƒ©ãƒ¼: {ocr_result['error']}"
                yield {
                    "file": file_name, 
                    "step": f"ãƒšãƒ¼ã‚¸ {page_number + 1} OCRã‚¨ãƒ©ãƒ¼: {ocr_result['error']}"
                }

            # çµåˆã—ã¦1ãƒšãƒ¼ã‚¸åˆ†ã¨ã—ã¦æ ¼ç´
            merged = f"ã€PDFæŠ½å‡ºã€‘\n{pdf_text}\n\nã€OCRæŠ½å‡ºã€‘\n{ocr_text}"
            text_list.append(merged)

        doc.close()
        yield text_list  # æœ€çµ‚çµæœã‚’è¿”ã™

    def _extract_docx_with_ocr_generator(
        self, 
        docx_path: str, 
        file_name: str, 
        ocr_engine_id: str = None, 
        ocr_settings: Dict = None
    ) -> Generator:
        """Wordãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ§‹é€ ã¨OCRã‚’çµ±åˆã—ã¦æŠ½å‡ºï¼ˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ç‰ˆï¼‰"""
        import docx
        import subprocess
        import tempfile
        
        structured_text = []

        yield {"file": file_name, "step": "Wordæ–‡æ›¸ã®æ§‹é€ ã‚’æŠ½å‡ºä¸­..."}

        # è¡¨ã‚„æ®µè½ãªã©ã€è«–ç†æ§‹é€ ãƒ™ãƒ¼ã‚¹ã§æŠ½å‡º
        doc = docx.Document(docx_path)
        structured_text.extend([p.text for p in doc.paragraphs if p.text])

        for table in doc.tables:
            for row in table.rows:
                row_text = [cell.text.strip().replace("\n", " ") for cell in row.cells if cell.text.strip()]
                if row_text:
                    structured_text.append(" | ".join(row_text))

        # OCRãƒ™ãƒ¼ã‚¹ï¼ˆLibreOfficeçµŒç”±ã§PDFåŒ– â†’ æ–°ã—ã„OCRã‚µãƒ¼ãƒ“ã‚¹ä½¿ç”¨ï¼‰
        with tempfile.TemporaryDirectory() as tmpdir:
            yield {"file": file_name, "step": "Wordæ–‡æ›¸ã‚’PDFã«å¤‰æ›ä¸­..."}
                
            try:
                subprocess.run([
                    "libreoffice", "--headless", "--convert-to", "pdf",
                    "--outdir", tmpdir, docx_path
                ], check=True)
            except subprocess.CalledProcessError:
                raise RuntimeError("LibreOfficeã«ã‚ˆã‚‹PDFå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ")

            basename = os.path.splitext(os.path.basename(docx_path))[0]
            pdf_path = os.path.join(tmpdir, f"{basename}.pdf")
            if os.path.exists(pdf_path):
                ocr_text = []
                for event_or_result in self._extract_pdf_with_ocr_generator(pdf_path, file_name, ocr_engine_id, ocr_settings):
                    if isinstance(event_or_result, dict) and "step" in event_or_result:
                        yield event_or_result
                    else:
                        ocr_text = event_or_result
            else:
                raise FileNotFoundError(f"PDFå¤‰æ›å¾Œãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")

        # çµ±åˆï¼šã‚¿ã‚°ã‚’ä»˜ã‘ã¦ LLM æ•´å½¢ã«æ¸¡ã™
        structured_content = "\n".join(structured_text)
        ocr_content = "\n".join(ocr_text)
        
        # ä¸¡å†…å®¹ã‚’çµ±åˆã—ã¦1ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦è¿”ã™
        combined_text = f"<structured_text>\n{structured_content}\n</structured_text>\n\n<ocr_text>\n{ocr_content}\n</ocr_text>"
        yield [combined_text]

    def _run_with_timeout(self, func, timeout_sec: int):
        """ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§é–¢æ•°ã‚’å®Ÿè¡Œ"""
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
            future = ex.submit(func)
            try:
                return future.result(timeout=timeout_sec)
            except concurrent.futures.TimeoutError:
                raise TimeoutError