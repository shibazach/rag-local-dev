"""
LLMæ•´å½¢ã‚µãƒ¼ãƒ“ã‚¹ - Prototypeçµ±åˆç‰ˆ
LangChain + Ollamaã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ãƒ»ç²¾ç·»åŒ–
"""

import re
import time
from typing import Tuple, Optional, Dict, Any

from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama
from langchain_community.llms.ollama import OllamaEndpointNotFoundError

from app.config import config, logger
from app.services.ocr import get_spell_checker
from .prompt_loader import PromptLoader
from .llm_utils import detect_language
from .scorer import TextScorer

class TextRefiner:
    """LLMãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.ollama_base = config.OLLAMA_BASE_URL
        self.default_model = config.OLLAMA_MODEL
        self.spell_checker = get_spell_checker()
        self.prompt_loader = PromptLoader()
        self.text_scorer = TextScorer()
    
    def normalize_empty_lines(self, text: str) -> str:
        """
        ç©ºç™½ã®ã¿ã®è¡Œã‚’å‰Šé™¤ã—ã€é€£ç¶šç©ºè¡Œã¯æœ€å¤§ï¼‘è¡Œã«åœ§ç¸®ã™ã‚‹
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            æ­£è¦åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        # ç©ºç™½ã®ã¿ã®è¡Œã‚’å‰Šé™¤ï¼ˆå…¨è§’ã‚¹ãƒšãƒ¼ã‚¹å«ã‚€ï¼‰
        text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)
        # é€£ç¶šã™ã‚‹3ã¤ä»¥ä¸Šã®æ”¹è¡Œã‚’2ã¤ã«åœ§ç¸®
        return re.sub(r'\n{3,}', '\n\n', text)
    
    def build_prompt(self, raw_text: str, lang: str = "ja") -> str:
        """
        ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
        
        Args:
            raw_text: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            lang: è¨€èªã‚³ãƒ¼ãƒ‰
            
        Returns:
            çµ„ã¿ç«‹ã¦ã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        """
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå–å¾—
        template = self.prompt_loader.get_prompt_by_lang(lang)
        
        # OCRèª¤å­—è£œæ­£ã¨ç©ºè¡Œæ­£è¦åŒ–
        cleaned = self.normalize_empty_lines(
            self.spell_checker.correct_text(raw_text)
        )
        
        # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®{TEXT}ã‚’ç½®æ›
        return template.replace("{TEXT}", cleaned)
    
    def refine_text_with_llm(
        self,
        raw_text: str,
        model: Optional[str] = None,
        force_lang: Optional[str] = None,
        abort_flag: Optional[Dict[str, bool]] = None,
        temperature: float = 0.7,
        max_new_tokens: int = 1024
    ) -> Tuple[str, str, float, str]:
        """
        LangChain + Ollama ã§ raw_text ã‚’æ•´å½¢
        
        Args:
            raw_text: ç”Ÿãƒ†ã‚­ã‚¹ãƒˆ
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆçœç•¥æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            force_lang: å¼·åˆ¶è¨€èªæŒ‡å®š
            abort_flag: ä¸­æ–­ãƒ•ãƒ©ã‚°
            temperature: ç”Ÿæˆæ¸©åº¦
            max_new_tokens: æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°
            
        Returns:
            (refined_text, lang, quality_score, prompt_used)ã®ã‚¿ãƒ—ãƒ«
        """
        if model is None:
            model = self.default_model
        
        # ä¸­æ–­ãƒã‚§ãƒƒã‚¯é–¢æ•°
        def check_abort():
            if abort_flag and abort_flag.get("flag"):
                raise InterruptedError("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        
        try:
            # 1) OCR èª¤å­—è£œæ­£ï¼‹ç©ºè¡Œåœ§ç¸®
            check_abort()
            corrected = self.normalize_empty_lines(
                self.spell_checker.correct_text(raw_text)
            )
            
            # 2) è¨€èªåˆ¤å®š
            check_abort()
            lang = detect_language(corrected, force_lang) or "ja"
            if force_lang == "ja":
                lang = "ja"
            
            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            text_len = len(corrected)
            token_estimate = int(text_len * 1.6)  # æ—¥æœ¬èªã®å ´åˆã®æ¨å®š
            logger.info(
                f"ğŸ§  LLMæ•´å½¢ã‚’é–‹å§‹ï¼ˆæ–‡å­—æ•°: {text_len}, "
                f"æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: {token_estimate}ï¼‰"
            )
            logger.info(f"ğŸ”¤ æ•´å½¢è¨€èª: {lang}")
            
            # 3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
            check_abort()
            prompt_text = self.build_prompt(raw_text, lang)
            
            # PromptTemplate ç”¨ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
            safe_prompt = prompt_text.replace("{", "{{").replace("}", "}}")
            
            # 4) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            gen_kw = {
                "max_new_tokens": max_new_tokens,
                "min_length": max(1, int(len(corrected) * 0.8)),
                "temperature": temperature,
            }
            
            # 5) LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
            check_abort()
            llm = ChatOllama(
                model=model,
                base_url=self.ollama_base,
                **gen_kw
            )
            
            # 6) LangChain ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
            chain = (
                PromptTemplate.from_template(safe_prompt) |
                llm |
                StrOutputParser()
            )
            
            # 7) æ¨è«–å®Ÿè¡Œ
            check_abort()
            logger.debug(
                f"[DEBUG] LLMå‘¼ã³å‡ºã—: model={model}, "
                f"prompt_len={len(prompt_text)}"
            )
            logger.debug(
                f"[DEBUG] ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:\n---\n"
                f"{prompt_text[:300]}...\n---"
            )
            
            start_time = time.time()
            try:
                refined = chain.invoke({})
            except OllamaEndpointNotFoundError as e:
                # ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰æ™‚ã®æ˜ç¤ºçš„ã‚¨ãƒ©ãƒ¼
                raise RuntimeError(
                    f"Ollama ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚"
                    f"`ollama pull {model}` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                ) from e
            
            elapsed = time.time() - start_time
            logger.debug(f"[DEBUG] LLMæ¨è«–æ™‚é–“: {elapsed:.2f}ç§’")
            
            if not refined.strip():
                logger.warning("[WARNING] LLMãŒç©ºã®å¿œç­”ã‚’è¿”ã—ã¾ã—ãŸ")
                refined = "[EMPTY]"
            
            # 8) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
            check_abort()
            score = self.text_scorer.score_text_quality(
                corrected, refined, lang
            )
            
            logger.info(f"âœ… LLMæ•´å½¢å®Œäº†ï¼ˆå“è³ªã‚¹ã‚³ã‚¢: {score:.2f}ï¼‰")
            
            return refined, lang, score, prompt_text
            
        except InterruptedError:
            raise
        except Exception as e:
            logger.error(f"LLMæ•´å½¢ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def batch_refine_texts(
        self,
        texts: list[str],
        model: Optional[str] = None,
        force_lang: Optional[str] = None,
        **kwargs
    ) -> list[Tuple[str, str, float, str]]:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒæ•´å½¢
        
        Args:
            texts: ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
            force_lang: å¼·åˆ¶è¨€èªæŒ‡å®š
            **kwargs: ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            å„ãƒ†ã‚­ã‚¹ãƒˆã®(refined_text, lang, score, prompt)ã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        for i, text in enumerate(texts):
            logger.info(f"ãƒãƒƒãƒå‡¦ç†: {i+1}/{len(texts)}")
            try:
                result = self.refine_text_with_llm(
                    text,
                    model=model,
                    force_lang=force_lang,
                    **kwargs
                )
                results.append(result)
            except Exception as e:
                logger.error(f"ãƒãƒƒãƒå‡¦ç†ã‚¨ãƒ©ãƒ¼ (ãƒ†ã‚­ã‚¹ãƒˆ {i+1}): {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
                results.append((text, "ja", 0.0, ""))
        
        return results

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ˜ãƒ«ãƒ‘ãƒ¼
def get_text_refiner() -> TextRefiner:
    """ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    return TextRefiner()

def refine_text(text: str, model_name: Optional[str] = None) -> Dict[str, Any]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ï¼ˆç°¡æ˜“ç‰ˆï¼‰
    
    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        model_name: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å
        
    Returns:
        æ•´å½¢çµæœã®è¾æ›¸
    """
    refiner = get_text_refiner()
    try:
        refined_text, lang, score, prompt = refiner.refine_text_with_llm(
            text,
            model=model_name
        )
        return {
            "text": refined_text,
            "language": lang,
            "score": score,
            "prompt": prompt
        }
    except Exception as e:
        logger.error(f"ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "text": text,
            "language": "ja",
            "score": 0.0,
            "prompt": ""
        }