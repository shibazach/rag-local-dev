"""
BERTè£œæ­£ã‚µãƒ¼ãƒ“ã‚¹ - Prototypeçµ±åˆç‰ˆ
æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸãƒ†ã‚­ã‚¹ãƒˆèª¤ã‚Šä¿®æ­£
"""

import torch
from transformers import AutoTokenizer, BertForMaskedLM
from typing import List, Optional, Dict

from app.config import config, logger

# åˆ©ç”¨å¯èƒ½ãªBERTãƒ¢ãƒ‡ãƒ«
AVAILABLE_MODELS = {
    "tohoku": "cl-tohoku/bert-base-japanese",
    "daigo": "daigo/bert-base-japanese-sentiment",
    "japanese-v2": "cl-tohoku/bert-base-japanese-v2",
    "char": "cl-tohoku/bert-base-japanese-char",
}

class BertCorrector:
    """BERTè£œæ­£ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, model_key: str = "tohoku"):
        """
        Args:
            model_key: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚­ãƒ¼
        """
        self.model_key = model_key
        self.tokenizer = None
        self.model = None
        self._loaded_model_key = None
    
    def load_model(self, model_key: Optional[str] = None) -> tuple:
        """
        ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
        
        Args:
            model_key: ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ï¼ˆçœç•¥æ™‚ã¯åˆæœŸåŒ–æ™‚ã®å€¤ã‚’ä½¿ç”¨ï¼‰
            
        Returns:
            (tokenizer, model)ã®ã‚¿ãƒ—ãƒ«
        """
        if model_key is None:
            model_key = self.model_key
        
        # æ—¢ã«ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’è¿”ã™
        if self._loaded_model_key == model_key and self.tokenizer and self.model:
            return self.tokenizer, self.model
        
        model_name = AVAILABLE_MODELS.get(model_key)
        if not model_name:
            raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ã§ã™: {model_key}")
        
        try:
            logger.info(f"BERTãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­: {model_name}")
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = BertForMaskedLM.from_pretrained(model_name)
            
            # GPUä½¿ç”¨å¯èƒ½ãªå ´åˆã¯ç§»å‹•
            if config.CUDA_AVAILABLE and torch.cuda.is_available():
                self.model = self.model.cuda()
                logger.info("BERTãƒ¢ãƒ‡ãƒ«ã‚’GPUã«ç§»å‹•ã—ã¾ã—ãŸ")
            
            self.model.eval()
            self._loaded_model_key = model_key
            
            logger.info(f"âœ… BERTãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†: {model_name}")
            
        except Exception as e:
            logger.error(f"BERTãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—: {e}")
            raise
        
        return self.tokenizer, self.model
    
    def correct_text(
        self,
        text: str,
        model_key: Optional[str] = None,
        top_k: int = 1,
        confidence_threshold: float = 0.0
    ) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®èª¤ã‚Šã‚’ä¿®æ­£
        
        Args:
            text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
            model_key: ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ï¼ˆçœç•¥æ™‚ã¯åˆæœŸåŒ–æ™‚ã®å€¤ã‚’ä½¿ç”¨ï¼‰
            top_k: äºˆæ¸¬å€™è£œã®ä¸Šä½kå€‹ã‚’è€ƒæ…®
            confidence_threshold: ç½®æ›ã‚’è¡Œã†æœ€å°ä¿¡é ¼åº¦
            
        Returns:
            ä¿®æ­£å¾Œãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
            tokenizer, model = self.load_model(model_key)
            
            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            tokens = tokenizer.tokenize(text)
            if not tokens:
                return text
            
            corrected_tokens = tokens.copy()
            correction_count = 0
            
            # å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é †ç•ªã«ãƒã‚¹ã‚¯ã—ã¦äºˆæ¸¬
            for i in range(len(tokens)):
                # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
                if tokens[i].startswith("[") and tokens[i].endswith("]"):
                    continue
                
                # ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³ã§ç½®æ›
                masked_tokens = tokens.copy()
                masked_tokens[i] = "[MASK]"
                
                # å…¥åŠ›IDã«å¤‰æ›
                input_ids = tokenizer.convert_tokens_to_ids(masked_tokens)
                input_tensor = torch.tensor([input_ids])
                
                # GPUä½¿ç”¨å¯èƒ½ãªå ´åˆã¯ç§»å‹•
                if config.CUDA_AVAILABLE and torch.cuda.is_available():
                    input_tensor = input_tensor.cuda()
                
                # äºˆæ¸¬å®Ÿè¡Œ
                with torch.no_grad():
                    predictions = model(input_tensor).logits
                
                # ä¸Šä½kå€‹ã®äºˆæ¸¬ã‚’å–å¾—
                masked_index = i
                top_preds = torch.topk(predictions[0, masked_index], top_k)
                top_indices = top_preds.indices.tolist()
                top_probs = torch.softmax(top_preds.values, dim=0).tolist()
                
                # æœ€ã‚‚ç¢ºç‡ã®é«˜ã„äºˆæ¸¬ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
                predicted_token = tokenizer.convert_ids_to_tokens([top_indices[0]])[0]
                confidence = top_probs[0]
                
                # å…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ç•°ãªã‚Šã€ä¿¡é ¼åº¦ãŒé–¾å€¤ã‚’è¶…ãˆã‚‹å ´åˆã®ã¿ç½®æ›
                if predicted_token != tokens[i] and confidence > confidence_threshold:
                    logger.debug(
                        f"ğŸ” BERTè£œæ­£: {tokens[i]} â†’ {predicted_token} "
                        f"(ä¿¡é ¼åº¦: {confidence:.3f})"
                    )
                    corrected_tokens[i] = predicted_token
                    correction_count += 1
            
            # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã™
            corrected_text = tokenizer.convert_tokens_to_string(corrected_tokens)
            
            if correction_count > 0:
                logger.info(f"âœ… BERTè£œæ­£ã§{correction_count}ç®‡æ‰€ã‚’ä¿®æ­£ã—ã¾ã—ãŸ")
            
            return corrected_text
            
        except Exception as e:
            logger.error(f"BERTè£œæ­£ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
            return text
    
    def batch_correct_texts(
        self,
        texts: List[str],
        model_key: Optional[str] = None,
        top_k: int = 1,
        confidence_threshold: float = 0.0
    ) -> List[str]:
        """
        è¤‡æ•°ãƒ†ã‚­ã‚¹ãƒˆã®ãƒãƒƒãƒè£œæ­£
        
        Args:
            texts: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
            model_key: ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼
            top_k: äºˆæ¸¬å€™è£œã®ä¸Šä½kå€‹
            confidence_threshold: ç½®æ›ã‚’è¡Œã†æœ€å°ä¿¡é ¼åº¦
            
        Returns:
            ä¿®æ­£å¾Œãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰
        self.load_model(model_key)
        
        corrected_texts = []
        for text in texts:
            corrected = self.correct_text(
                text,
                model_key=model_key,
                top_k=top_k,
                confidence_threshold=confidence_threshold
            )
            corrected_texts.append(corrected)
        
        return corrected_texts
    
    def get_available_models(self) -> Dict[str, str]:
        """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
        return AVAILABLE_MODELS.copy()

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆãƒ˜ãƒ«ãƒ‘ãƒ¼
def get_bert_corrector(model_key: str = "tohoku") -> BertCorrector:
    """BERTè£œæ­£ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å–å¾—"""
    return BertCorrector(model_key)