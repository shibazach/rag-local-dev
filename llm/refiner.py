# llm/refiner.py
"""
LLM æ•´å½¢ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
 - normalize_empty_lines: ç©ºè¡Œåœ§ç¸®
 - build_prompt: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚€
 - refine_text_with_llm: LangChain + Ollama ã§æ•´å½¢å®Ÿè¡Œ
"""

# REM: æ¨™æº–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import re
import time

# REM: LangChainï¼Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama
# REM: Ollamaã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼å®šç¾©
from langchain_community.llms.ollama import OllamaEndpointNotFoundError

# REM: OCRï¼LLMãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from ocr import correct_text
from llm.prompt_loader import get_prompt_by_lang
from llm.llm_utils import detect_language
from llm.scorer import score_text_quality

# REM: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šè¨­å®šãƒ»ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from src.config import OLLAMA_BASE, OLLAMA_MODEL
from src.utils import debug_print

# REM: ç©ºè¡Œåœ§ç¸®
def normalize_empty_lines(text: str) -> str:
    """
    ç©ºç™½ã®ã¿ã®è¡Œã‚’å‰Šé™¤ã—ã€é€£ç¶šç©ºè¡Œã¯æœ€å¤§ï¼‘è¡Œã«åœ§ç¸®ã™ã‚‹
    """
    text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)
    return re.sub(r'\n{3,}', '\n\n', text)


# REM: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
def build_prompt(raw_text: str, lang: str = "ja") -> str:
    """
    refine_prompt_multi.txt ã® #lang=lang ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ä½“ã‚’å–å¾—ã—ã€
    {TEXT} ã‚’ç½®æ›ã—ã¦è¿”ã™ã€‚
    """
    template = get_prompt_by_lang(lang)
    cleaned = normalize_empty_lines(correct_text(raw_text))
    return template.replace("{TEXT}", cleaned)


# REM: LLM æ•´å½¢å‡¦ç†
def refine_text_with_llm(
    raw_text: str,
    model: str = OLLAMA_MODEL,
    force_lang: str | None = None,
    abort_flag: dict[str, bool] | None = None
) -> tuple[str, str, float, str]:
    """
    LangChain + Ollama ã§ raw_text ã‚’æ•´å½¢ã€‚
    æˆ»ã‚Šå€¤: (refined_text, lang, quality_score, prompt_used)
    * abort_flag ãŒ True ã«ãªã‚‹ã¨ InterruptedError ã‚’é€å‡ºã—ã¦ä¸­æ–­å¯èƒ½
    """
    # REM: ä¸­æ–­ãƒã‚§ãƒƒã‚¯
    def check_abort():
        if abort_flag and abort_flag.get("flag"):
            raise InterruptedError("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

    # REM: 1) OCR èª¤å­—è£œæ­£ï¼‹ç©ºè¡Œåœ§ç¸®
    check_abort()
    corrected = normalize_empty_lines(correct_text(raw_text))

    # REM: 2) è¨€èªåˆ¤å®š
    check_abort()
    lang = detect_language(corrected, force_lang) or "ja"
    if force_lang == "ja":
        lang = "ja"

    # REM: ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼ˆæ–‡å­—æ•°ãƒ»æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒ»æ•´å½¢è¨€èªï¼‰
    text_len = len(corrected)
    token_estimate = int(text_len * 1.6)
    debug_print(f"ğŸ§  LLMæ•´å½¢ã‚’é–‹å§‹ï¼ˆæ–‡å­—æ•°: {text_len}, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: {token_estimate}ï¼‰")
    debug_print(f"ğŸ”¤ æ•´å½¢è¨€èª: {lang}")

    # REM: 3) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
    check_abort()
    prompt_text = build_prompt(raw_text, lang)

    # REM: PromptTemplate ç”¨ã«ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—
    safe_prompt = prompt_text.replace("{", "{{").replace("}", "}}")

    # REM: 4) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    gen_kw = {
        "max_new_tokens": 1024,
        "min_length":     max(1, int(len(corrected) * 0.8)),
        "temperature":    0.7,
    }

    # REM: 5) LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    check_abort()
    llm = ChatOllama(model=model, base_url=OLLAMA_BASE, **gen_kw)

    # REM: 6) LangChain ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    chain = PromptTemplate.from_template(safe_prompt) | llm | StrOutputParser()

    # REM: 7) æ¨è«–å®Ÿè¡Œï¼‹debugå‡ºåŠ›ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç¢ºèªï¼‹ã‚¿ã‚¤ãƒãƒ¼ï¼‹ç©ºå¿œç­”æ¤œçŸ¥ï¼‰
    check_abort()
    debug_print(f"[DEBUG invoke LLM model={model} prompt_len={len(prompt_text)}]")
    debug_print(f"[DEBUG prompt preview]\n---\n{prompt_text[:300]}...\n---")
    start_time = time.time()
    try:
        refined = chain.invoke({})
    except OllamaEndpointNotFoundError as e:
        # REM: ãƒ¢ãƒ‡ãƒ«æœªãƒ­ãƒ¼ãƒ‰æ™‚ã®æ˜ç¤ºçš„ã‚¨ãƒ©ãƒ¼
        raise RuntimeError(
            f"Ollama ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚`ollama pull {model}` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
        ) from e
    elapsed = time.time() - start_time
    debug_print(f"[DEBUG invoke elapsed: {elapsed:.2f} sec]")
    if not refined.strip():
        debug_print("[WARNING] LLM returned empty response.")
        refined = "[EMPTY]"

    # REM: 8) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
    check_abort()
    score = score_text_quality(corrected, refined, lang)

    # REM: è¿”ã‚Šå€¤ã«ä½¿ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚å«ã‚ã‚‹
    return refined, lang, score, prompt_text
