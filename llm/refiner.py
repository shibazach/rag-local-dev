# /workspace/llm/refiner.py
# REM: LangChain ã«ã‚ˆã‚‹ LLM æ•´å½¢å‡¦ç†ï¼ˆOllama åˆ©ç”¨ï¼‰
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama

from src.config import OLLAMA_BASE, OLLAMA_MODEL
from bin.llm_text_refiner import detect_language
from llm.prompt_loader import get_prompt_by_lang
from llm.scorer import score_text_quality
from ocr import correct_text

import re

# REM: ç©ºç™½ã®ã¿ã®è¡Œã‚’ç©ºè¡Œã«å¤‰æ›ã—ã€é€£ç¶šç©ºè¡Œã‚’æœ€å¤§ 1 è¡Œã«
def normalize_empty_lines(text: str) -> str:
    text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)
    return re.sub(r'\n{2,}', '\n', text)

# REM: åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
def build_prompt(raw_text: str, lang: str = "ja") -> str:
    _, base_prompt = get_prompt_by_lang(lang)
    user_prompt = "æ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n" + base_prompt
    if ("{TEXT}" not in user_prompt) and ("{input_text}" not in user_prompt):
        user_prompt += "\n\nã€åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{TEXT}"
    return (user_prompt.replace("{TEXT}", raw_text)
                       .replace("{input_text}", raw_text))

# REM: LLM ã«ã‚ˆã‚‹æ•´å½¢å‡¦ç†
def refine_text_with_llm(
    raw_text: str,
    model: str = OLLAMA_MODEL,
    force_lang: str | None = None,
    abort_flag: dict[str, bool] | None = None
):
    """LLM æ•´å½¢æœ¬ä½“

    * abort_flag ãŒ True ã«ãªã‚‹ã¨ InterruptedError ã‚’ç™ºç”Ÿã•ã›å³ä¸­æ–­
    * æˆ»ã‚Šå€¤: (refined_text, lang, score, prompt)
    """

    def check_abort():
        if abort_flag and abort_flag.get("flag"):
            raise InterruptedError("å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

    # 1) OCR èª¤å­—è£œæ­£ï¼‹ç©ºè¡Œæ•´ç†
    check_abort()
    corrected = normalize_empty_lines(correct_text(raw_text))

    # 2) è¨€èªåˆ¤å®šï¼ˆforce_lang å„ªå…ˆï¼‰
    check_abort()
    lang = detect_language(corrected, force_lang) or "ja"

    # 3) ãƒ‡ãƒãƒƒã‚°
    txt_len = len(corrected)
    print(f"ğŸ§  LLMæ•´å½¢é–‹å§‹ len={txt_len}", flush=True)

    # 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    check_abort()
    prompt = build_prompt(corrected, lang)

    # 5) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    gen_kw = {
        "max_new_tokens": 1024,
        "min_length":     max(1, int(txt_len * 0.8)),
        "temperature":    0.7,
    }

    # 6) LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    check_abort()
    llm = ChatOllama(model=model, base_url=OLLAMA_BASE, **gen_kw)

    # 7) LangChain ãƒã‚§ãƒ¼ãƒ³
    safe_prompt = prompt.replace("{", "{{").replace("}", "}}")
    chain = PromptTemplate.from_template(safe_prompt) | llm | StrOutputParser()

    # 8) æ¨è«–
    check_abort()
    refined = chain.invoke({})

    # 9) ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
    check_abort()
    score = score_text_quality(corrected, refined, lang)

    return refined, lang, score, prompt
