# llm/refiner.py

# REM: LangChainã«ã‚ˆã‚‹LLMæ•´å½¢å‡¦ç†ï¼ˆOllamaåˆ©ç”¨ï¼‰
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama

from src.config import OLLAMA_BASE, OLLAMA_MODEL
from bin.llm_text_refiner import detect_language
from llm.prompt_loader import get_prompt_by_lang
from llm.scorer import score_text_quality
from ocr import correct_text

# REM: ç©ºç™½ã®ã¿ã®è¡Œã‚’ç©ºè¡Œã«å¤‰æ›ã—ã€é€£ç¶šç©ºè¡Œã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
def normalize_empty_lines(text: str) -> str:
    import re
    # ç©ºç™½ï¼ˆå…¨è§’å«ã‚€ï¼‰ã®ã¿ã®è¡Œã‚’ç©ºè¡Œã«ã™ã‚‹
    text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)
    # é€£ç¶šã™ã‚‹ç©ºè¡Œã‚’1ã¤ã«ã™ã‚‹
    text = re.sub(r'\n{2,}', '\n', text)
    return text


# REM: åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆ
def build_prompt(raw_text: str, lang: str = "ja") -> str:
    """
    - `refine_prompt_multi.txt` å´ã« {TEXT} / {input_text} ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒ
      å­˜åœ¨ã™ã‚‹å ´åˆã¯ãã®ã¾ã¾ç½®æ›
    - å­˜åœ¨ã—ãªã„å ´åˆã¯æœ«å°¾ã« 'ã€åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘\\n{TEXT}' ã‚’è¿½åŠ 
    """
    _, base_prompt = get_prompt_by_lang(lang)
    user_prompt = "æ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n" + base_prompt

    # REM: ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒç„¡ã‘ã‚Œã°æœ«å°¾ã«è¿½åŠ 
    if ("{TEXT}" not in user_prompt) and ("{input_text}" not in user_prompt):
        user_prompt += "\n\nã€åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{TEXT}"

    # REM: 2 ç¨®é¡ã¨ã‚‚ç½®æ›ã—ã¦ãŠã
    prompt_filled = (
        user_prompt.replace("{TEXT}", raw_text)
        .replace("{input_text}", raw_text)
    )
    return prompt_filled


# REM: LLMã«ã‚ˆã‚‹æ•´å½¢å‡¦ç†æœ¬ä½“
def refine_text_with_llm(raw_text: str,
                         model: str = OLLAMA_MODEL,
                         force_lang: str = None):
    # 1) OCRèª¤å­—è£œæ­£ã€ç©ºè¡Œæ­£è¦åŒ–
    corrected = normalize_empty_lines(correct_text(raw_text))

    # 2) è¨€èªåˆ¤å®šï¼å¼·åˆ¶ï¼ˆã“ã“ã§ã¯æ—¥æœ¬èªå›ºå®šï¼‰
    lang = detect_language(corrected, force_lang)
    lang = "ja"

    # 3) ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šæ–‡å­—æ•°ã¨æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    text_len       = len(corrected)
    token_estimate = int(text_len * 1.6)
    print(f"ğŸ§  LLMæ•´å½¢ã‚’é–‹å§‹ï¼ˆæ–‡å­—æ•°: {text_len}, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: {token_estimate}ï¼‰", flush=True)
    print(f"ğŸ”¤ æ•´å½¢è¨€èª: {lang}", flush=True)

    # 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼ˆåŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ï¼‰
    prompt = build_prompt(corrected, lang)

    # 5) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ‹¡å¼µ
    generation_kwargs = {
        "max_new_tokens": 1024,
        "min_length": max(1, int(len(corrected) * 0.8)),
        "temperature": 0.7,
    }

    # 6) Ollama LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    llm = ChatOllama(
        model=model,
        base_url=OLLAMA_BASE,
        **generation_kwargs
    )

    # 7) LangChain ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    # REM: build_prompt ã§ {TEXT} ã‚’æ—¢ã«ç½®æ›æ¸ˆã¿ãªã®ã§
    #      ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ç„¡ã—ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨ã—ã¦æ‰±ã†
    prompt_template = PromptTemplate.from_template(prompt)
    chain = prompt_template | llm | StrOutputParser()

    # 8) æ¨è«–å®Ÿè¡Œ
    refined_text = chain.invoke({})  # è¿½åŠ å¤‰æ•°ãªã—

    # 9) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
    score = score_text_quality(corrected, refined_text, lang)
    return refined_text, lang, score, prompt
