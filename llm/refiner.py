# llm/refiner.py

from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOllama

from src.config import OLLAMA_BASE, OLLAMA_MODEL
from llm.utils import detect_language, detect_language_probs  # utils ã«çµ±åˆæ¸ˆã¿
from llm.prompt_loader import get_prompt_by_lang
from llm.scorer import score_text_quality
from ocr import correct_text

# REM: ç©ºç™½ã®ã¿ã®è¡Œã‚’ç©ºè¡Œã«å¤‰æ›ã—ã€é€£ç¶šç©ºè¡Œã‚’1ã¤ã«ã¾ã¨ã‚ã‚‹
def normalize_empty_lines(text: str) -> str:
    import re
    text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)
    text = re.sub(r'\n{2,}', '\n', text)
    return text

# REM: åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚“ã ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆï¼ˆæ—¥æœ¬èª/è‹±èªå¯¾å¿œï¼‰
def build_prompt(raw_text: str, lang: str = "ja") -> str:
    """
    - æŒ‡å®šè¨€èªã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—
    - æ—¥æœ¬èªã¯ã€Œæ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§â€¦ã€ã‚’å…ˆé ­ã«ä»˜ä¸
    - ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ {TEXT}/{input_text} ã§ç½®æ›
    - ç½®æ›å¯¾è±¡ãŒãªã‘ã‚Œã°æœ«å°¾ã«ã€åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    """
    _, base_prompt = get_prompt_by_lang(lang)
    # prefix ã¯æ—¥æœ¬èªã®ã¿
    prefix = "æ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n" if lang == "ja" else ""
    user_prompt = prefix + base_prompt

    # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãŒç„¡ã‘ã‚Œã°æœ«å°¾è¿½åŠ 
    if ("{TEXT}" not in user_prompt) and ("{input_text}" not in user_prompt):
        user_prompt += "\n\nã€åŸæ–‡ãƒ†ã‚­ã‚¹ãƒˆã€‘\n{TEXT}"

    # å®Ÿãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã‚€
    prompt_filled = (
        user_prompt.replace("{TEXT}", raw_text)
                   .replace("{input_text}", raw_text)
    )
    return prompt_filled

# REM: LLMã«ã‚ˆã‚‹æ•´å½¢å‡¦ç†æœ¬ä½“ï¼ˆè¨€èªæŸ”è»ŸåŒ–å¯¾å¿œï¼‰
def refine_text_with_llm(raw_text: str,
                         model: str = OLLAMA_MODEL,
                         force_lang: str = None):
    # 1) OCRèª¤å­—è£œæ­£ï¼‹ç©ºè¡Œæ­£è¦åŒ–
    corrected = normalize_empty_lines(correct_text(raw_text))

    # 2) ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªç¢ºç‡ã‚’å–å¾—
    probs = detect_language_probs(corrected)
    detected_lang = probs[0].lang if probs else "ja"
    en_prob = next((p.prob for p in probs if p.lang == "en"), 0.0)

    # 3) force_lang ãŒã‚ã‚Œã°åŸºæœ¬ã¯ãã‚Œã‚’ä½¿ã†ãŒã€
    #    force_lang == "ja" ã§ã‚‚è‹±èªç‡ãŒé«˜ã‘ã‚Œã°è‹±èªã«åˆ‡ã‚Šæ›¿ãˆ
    if force_lang:
        lang = force_lang
        if force_lang == "ja" and detected_lang == "en" and en_prob > 0.7:
            lang = "en"
    else:
        lang = detected_lang

    # 4) ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    text_len = len(corrected)
    token_est = int(text_len * 1.6)
    print(f"ğŸ§  LLMæ•´å½¢é–‹å§‹ (æ–‡å­—æ•°: {text_len}, ãƒˆãƒ¼ã‚¯ãƒ³æ¨å®š: {token_est})", flush=True)
    print(f"ğŸ”¤ ä½¿ç”¨è¨€èª: {lang}", flush=True)

    # 5) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    prompt = build_prompt(corrected, lang)

    # 6) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    generation_kwargs = {
        "max_new_tokens": 1024,
        "min_length": max(1, int(text_len * 0.8)),
        "temperature": 0.7,
    }

    # 7) Ollama LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    llm = ChatOllama(model=model, base_url=OLLAMA_BASE, **generation_kwargs)

    # 8) ä¸­æ‹¬å¼§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ— & ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    safe_prompt = prompt.replace("{", "{{").replace("}", "}}")
    prompt_template = PromptTemplate.from_template(safe_prompt)
    chain = prompt_template | llm | StrOutputParser()

    # 9) æ¨è«–å®Ÿè¡Œ
    refined_text = chain.invoke({})

    # 10) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
    score = score_text_quality(corrected, refined_text, lang)
    return refined_text, lang, score, prompt
