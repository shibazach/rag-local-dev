# new/api/ingest.py
# ãƒ‡ãƒ¼ã‚¿ç™»éŒ²å‡¦ç†ãƒ»SSEé€²æ—è¡¨ç¤ºAPI

import asyncio
import json
import logging
import time
from typing import Dict, List, Optional, AsyncGenerator
from fastapi import APIRouter, HTTPException, Request, Depends
from fastapi.responses import StreamingResponse, JSONResponse
from sqlalchemy.engine import Connection

from new.database.connection import get_db_connection
from new.services.ocr import OCREngineFactory
from new.services.processing.pipeline import ProcessingPipeline
from new.config import LOGGER
import os

router = APIRouter(prefix="/ingest", tags=["ingest"])

# ã‚°ãƒ­ãƒ¼ãƒãƒ«çŠ¶æ…‹ç®¡ç†
current_job: Optional[Dict] = None

def _cleanup_temp_files(files: List[Dict]):
    """ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    if not files:
        return
        
    for file_info in files:
        if not file_info:  # None ãƒã‚§ãƒƒã‚¯è¿½åŠ 
            continue
            
        if file_info.get('temp_file') and file_info.get('file_path'):  # ã‚ˆã‚Šå®‰å…¨ãªãƒã‚§ãƒƒã‚¯
            temp_path = file_info['file_path']
            try:
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                    LOGGER.debug(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {temp_path}")
            except Exception as e:
                LOGGER.error(f"ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼ [{temp_path}]: {e}")
cancel_event: Optional[asyncio.Event] = None
processing_pipeline: Optional[ProcessingPipeline] = None

@router.get("/status")
async def get_processing_status() -> JSONResponse:
    """ç¾åœ¨ã®å‡¦ç†çŠ¶æ³ã‚’å–å¾—"""
    global current_job
    
    if current_job:
        return JSONResponse({
            "status": current_job.get("status", "unknown"),
            "files_total": current_job.get("files_total", 0),
            "files_completed": current_job.get("files_completed", 0),
            "current_file": current_job.get("current_file", None),
            "start_time": current_job.get("start_time", None)
        })
    else:
        return JSONResponse({
            "status": "idle",
            "files_total": 0,
            "files_completed": 0,
            "current_file": None,
            "start_time": None
        })

@router.post("/reset")
async def reset_processing_state() -> JSONResponse:
    """å‡¦ç†çŠ¶æ…‹ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆ"""
    global current_job, cancel_event, processing_pipeline
    
    # å‡¦ç†çŠ¶æ…‹ã‚’å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
    current_job = None
    cancel_event = None
    
    if processing_pipeline:
        processing_pipeline.abort_flag = {'flag': True}
        processing_pipeline = None
    
    return JSONResponse({
        "message": "å‡¦ç†çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ",
        "status": "reset_complete"
    })

@router.post("/cancel")
async def cancel_processing() -> JSONResponse:
    """å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
    global current_job, cancel_event, processing_pipeline
    
    if not current_job or current_job.get("status") != "running":
        return JSONResponse({
            "message": "ã‚­ãƒ£ãƒ³ã‚»ãƒ«å¯¾è±¡ã®å‡¦ç†ãŒã‚ã‚Šã¾ã›ã‚“",
            "status": "no_active_job"
        })
    
    # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
    if cancel_event:
        cancel_event.set()
    
    if processing_pipeline:
        processing_pipeline.abort_flag = {'flag': True}
    
    current_job["status"] = "cancelled"
    
    # å‡¦ç†ãƒ­ã‚°ã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«é€šçŸ¥ã‚’é€ä¿¡
    LOGGER.info("ğŸ›‘ å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«è¦æ±‚ã‚’å—ä¿¡ - å‡¦ç†ã‚’åœæ­¢ã—ã¾ã™")
    
    return JSONResponse({
        "message": "å‡¦ç†ã®ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚’è¦æ±‚ã—ã¾ã—ãŸ",
        "status": "cancel_requested"
    })

@router.post("/start")
async def start_processing(
    request_data: Dict,
    connection: Connection = Depends(get_db_connection)
) -> JSONResponse:
    """ãƒ‡ãƒ¼ã‚¿ç™»éŒ²å‡¦ç†ã‚’é–‹å§‹"""
    global current_job, cancel_event
    
    try:
        # æ—¢å­˜ã‚¸ãƒ§ãƒ–ãƒã‚§ãƒƒã‚¯
        if current_job and current_job.get("status") == "running":
            raise HTTPException(status_code=409, detail="å‡¦ç†ãŒæ—¢ã«å®Ÿè¡Œä¸­ã§ã™")
        
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        selected_files = request_data.get("selected_files", [])
        # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã‚’å‰Šé™¤ï¼ˆã†ã–ã„ãŸã‚ï¼‰
        if not selected_files:
            raise HTTPException(status_code=400, detail="å‡¦ç†å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # å‡¦ç†è¨­å®š
        settings = request_data.get("settings", {})
        ocr_engine = settings.get("ocr_engine", "ocrmypdf")
        embedding_models = settings.get("embedding_models", ["intfloat-e5-large-v2"])
        overwrite_existing = settings.get("overwrite_existing", True)
        quality_threshold = settings.get("quality_threshold", 0.0)
        llm_timeout = settings.get("llm_timeout", 300)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
        from sqlalchemy import text
        placeholders = ",".join([f":file_id_{i}" for i in range(len(selected_files))])
        params = {f"file_id_{i}": file_id for i, file_id in enumerate(selected_files)}
        
        query = text(f"""
            SELECT 
                fb.id as file_id,
                fm.file_name,
                fm.size as file_size,
                fb.blob_data
            FROM files_blob fb
            JOIN files_meta fm ON fb.id = fm.blob_id
            WHERE fb.id IN ({placeholders})
            ORDER BY fb.stored_at DESC
        """)
        
        files_raw = connection.execute(query, params).fetchall()
        # DBå–å¾—ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãƒ­ã‚°ã‚’å‰Šé™¤ï¼ˆã†ã–ã„ãŸã‚ï¼‰
        
        # DB blobã‹ã‚‰ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        files = []
        import tempfile
        import os
        from pathlib import Path
        
        for file_row in files_raw:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            file_ext = Path(file_row.file_name).suffix
            temp_fd, temp_path = tempfile.mkstemp(suffix=file_ext, prefix=f"ingest_{file_row.file_id}_")
            
            try:
                # blobãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
                with os.fdopen(temp_fd, 'wb') as temp_file:
                    temp_file.write(file_row.blob_data)
                
                files.append({
                    'file_id': file_row.file_id,
                    'file_name': file_row.file_name,
                    'file_path': temp_path,
                    'file_size': file_row.file_size,
                    'temp_file': True  # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ¼ã‚¯
                })
                
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                
            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‰ã˜ã‚‹
                try:
                    os.close(temp_fd)
                except:
                    pass
                if os.path.exists(temp_path):
                    os.unlink(temp_path)
                raise e
        
        if not files:
            raise HTTPException(status_code=404, detail="æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        # ã‚¸ãƒ§ãƒ–åˆæœŸåŒ–
        current_job = {
            "id": f"job_{int(time.time())}",
            "status": "running",
            "files": files,
            "settings": {
                "ocr_engine": ocr_engine,
                "embedding_models": embedding_models,
                "overwrite_existing": overwrite_existing,
                "quality_threshold": quality_threshold,
                "llm_timeout": llm_timeout
            },
            "progress": {
                "total_files": len(files),
                "processed_files": 0,
                "current_file": None,
                "start_time": time.time()
            },
            "results": []
        }
        
        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚¤ãƒ™ãƒ³ãƒˆãƒªã‚»ãƒƒãƒˆ
        cancel_event = asyncio.Event()
        
        # å‡¦ç†é–‹å§‹ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
        
        return JSONResponse({
            "success": True,
            "data": {
                "job_id": current_job["id"],
                "total_files": len(files),
                "message": f"{len(files)}ä»¶ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã—ãŸ"
            }
        })
        
    except HTTPException:
        raise
    except Exception as e:
        LOGGER.error(f"å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail=f"å‡¦ç†é–‹å§‹ã‚¨ãƒ©ãƒ¼: {str(e)}")





@router.get("/stream")
async def progress_stream(request: Request) -> StreamingResponse:
    """SSEã§é€²æ—ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼ˆå®Ÿéš›ã®å‡¦ç†ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨ï¼‰"""
    global current_job, cancel_event, processing_pipeline
    
    # ã‚¸ãƒ§ãƒ–ãŒãªã„å ´åˆã¯å¾…æ©Ÿï¼ˆSSEæ¥ç¶šç¶­æŒï¼‰
    # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã¯å‡¦ç†é–‹å§‹å‰ã«æ¥ç¶šã™ã‚‹ãŸã‚
    
    # SSEæ¥ç¶šé–‹å§‹ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
    
    async def event_generator() -> AsyncGenerator[str, None]:
        global processing_pipeline
        
        try:
            # SSEæ¥ç¶šç¢ºç«‹é€šçŸ¥
            yield f"data: {json.dumps({'type': 'connected', 'message': 'SSEæ¥ç¶šç¢ºç«‹'})}\n\n"
            
            # ã‚¸ãƒ§ãƒ–ãŒé–‹å§‹ã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆé€²è¡ŒçŠ¶æ³ã‚’é€šçŸ¥ï¼‰
            wait_start = time.time()
            while not current_job:
                if await request.is_disconnected():
                    return
                
                # å¾…æ©ŸçŠ¶æ³ã‚’å®šæœŸçš„ã«é€šçŸ¥
                wait_elapsed = time.time() - wait_start
                if wait_elapsed > 1.0 and int(wait_elapsed) % 5 == 0:  # 5ç§’æ¯ã«é€šçŸ¥
                    yield f"data: {json.dumps({'type': 'waiting', 'message': f'å‡¦ç†è¦æ±‚å—ä¿¡å¾…æ©Ÿä¸­... ({int(wait_elapsed)}ç§’çµŒé)', 'elapsed': int(wait_elapsed)})}\n\n"
                
                await asyncio.sleep(0.5)
            
            # å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–é€šçŸ¥
            yield f"data: {json.dumps({'type': 'status', 'message': 'ğŸ”§ å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ä¸­...'})}\n\n"
            
            # å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
            processing_pipeline = ProcessingPipeline()
            
            # åˆæœŸåŒ–å®Œäº†é€šçŸ¥
            yield f"data: {json.dumps({'type': 'status', 'message': 'âœ… å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–å®Œäº† - ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹'})}\n\n"
            
            # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ‡æ–­ç›£è¦–
            async def monitor_disconnect():
                while current_job and current_job.get("status") == "running":
                    if await request.is_disconnected():
                        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆ‡æ–­æ¤œçŸ¥ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                        if processing_pipeline:
                            processing_pipeline.cancel_processing()
                        break
                    await asyncio.sleep(0.5)
            
            # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ç›£è¦–
            async def monitor_cancellation():
                while current_job and current_job.get("status") == "running":
                    if cancel_event and cancel_event.is_set():
                        # ã‚­ãƒ£ãƒ³ã‚»ãƒ«è¦æ±‚æ¤œçŸ¥ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                        if processing_pipeline:
                            processing_pipeline.cancel_processing()
                        break
                    await asyncio.sleep(0.5)
            
            # ç›£è¦–ã‚¿ã‚¹ã‚¯é–‹å§‹
            asyncio.create_task(monitor_disconnect())
            asyncio.create_task(monitor_cancellation())
            
            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ
            files = current_job["files"]
            settings = current_job["settings"]
            current_job["results"] = []
            
            # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
            async for event in processing_pipeline.process_files(files, settings):
                # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ»åˆ‡æ–­ãƒã‚§ãƒƒã‚¯
                if (cancel_event and cancel_event.is_set()) or \
                   (await request.is_disconnected()):
                    break
                
                # çµæœä¿å­˜
                if event.get('type') == 'file_complete':
                    result = event.get('data', {}).get('result', {})
                    if result:
                        current_job["results"].append(result)
                
                # SSEã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡
                # DEBUG-SSE ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡ï¼ˆãƒ­ã‚°å‰Šé™¤ï¼‰
                yield f"data: {json.dumps(event)}\n\n"
                
                # å®Œäº†ãƒã‚§ãƒƒã‚¯
                if event.get('type') in ['complete', 'error', 'cancelled']:
                    break
            
            # ã‚¸ãƒ§ãƒ–çŠ¶æ…‹æ›´æ–°
            if current_job:
                if cancel_event and cancel_event.is_set():
                    current_job["status"] = "cancelled"
                else:
                    current_job["status"] = "completed"
                    
                # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                _cleanup_temp_files(current_job.get("files", []))
            
        except Exception as e:
            LOGGER.error(f"SSEå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'})}\n\n"
            
            if current_job:
                current_job["status"] = "error"
                # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                _cleanup_temp_files(current_job.get("files", []))
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "Cache-Control"
        }
    )