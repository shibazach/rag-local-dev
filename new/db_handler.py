# new/db_handler.py
# æ—§ç³»db/handler.pyã‚’ç§»æ¤ã—ãŸDBãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆ3ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹æˆï¼‰

from __future__ import annotations

import os
import hashlib
import mimetypes
import tempfile
from typing import Any, Sequence, Optional, List, Dict
from sqlalchemy.sql import text as sql_text
from uuid import UUID

from .database import engine
from .config import DEVELOPMENT_MODE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å†…éƒ¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calc_sha256(file_data: bytes) -> str:
    """ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‹ã‚‰SHA256ã‚’è¨ˆç®—"""
    return hashlib.sha256(file_data).hexdigest()

DEFAULT_MIME = "application/octet-stream"

def _normalize_tags(tags: Optional[Sequence[str]]) -> list[str]:
    return list(dict.fromkeys([t.strip() for t in (tags or []) if t.strip()]))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é–‹ç™ºãƒ¢ãƒ¼ãƒ‰å°‚ç”¨ DB åˆæœŸåŒ–
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def reset_dev_database() -> None:
    """é–‹ç™ºãƒ¢ãƒ¼ãƒ‰æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒªã‚»ãƒƒãƒˆ"""
    if not DEVELOPMENT_MODE:
        return
    
    with engine.begin() as db:
        db.execute(sql_text("TRUNCATE files_blob, files_meta, files_text CASCADE"))
        # åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚ã‚¯ãƒªã‚¢
        rows = db.execute(
            sql_text("""SELECT tablename FROM pg_tables
                       WHERE schemaname='public'
                         AND tablename ~ '^[a-zA-Z0-9_]+_[0-9]+$'""")
        ).fetchall()
        for (tbl,) in rows:
            db.execute(sql_text(f'TRUNCATE "{tbl}"'))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# é‡è¤‡ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def find_existing_by_checksum(checksum: str) -> Optional[str]:
    """checksumã§æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ã—ã€ã‚ã‚Œã°ãã®blob_idã‚’è¿”ã™"""
    with engine.connect() as db:
        row = db.execute(
            sql_text("SELECT id FROM files_blob WHERE checksum = :checksum"),
            {"checksum": checksum}
        ).first()
    return str(row[0]) if row else None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ãƒ•ã‚¡ã‚¤ãƒ«ä¸€æ‹¬ INSERT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def insert_file_blob_only(
    file_name: str,
    file_data: bytes,
    mime_type: str = None
) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ï¼šblobã¨metaã®ã¿ä¿å­˜
    1. checksumè¨ˆç®—
    2. æ—¢å­˜blobæ¤œç´¢ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
    3. files_blob INSERTï¼ˆæ–°è¦ã®å ´åˆï¼‰
    4. files_meta INSERT/UPDATE
    """
    result = insert_file_blob_with_details(file_name, file_data, mime_type)
    return result["blob_id"]

def insert_file_blob_with_details(
    file_name: str,
    file_data: bytes,
    mime_type: str = None
) -> dict:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ï¼šblobã¨metaã®ã¿ä¿å­˜ï¼ˆè©³ç´°æƒ…å ±ä»˜ãï¼‰
    æˆ»ã‚Šå€¤: {blob_id, is_existing, file_info}
    """
    file_hash = _calc_sha256(file_data)
    size = len(file_data)
    mime_type = mime_type or mimetypes.guess_type(file_name)[0] or DEFAULT_MIME
    
    with engine.begin() as db:
        # 1. æ—¢å­˜blobæ¤œç´¢
        existing_blob_id = find_existing_by_checksum(file_hash)
        is_existing = existing_blob_id is not None
        
        if existing_blob_id:
            # æ—¢å­˜blobã‚’ä½¿ç”¨
            blob_id = existing_blob_id
            print(f"[handler] Using existing blob: {blob_id}")
        else:
            # æ–°è¦blobä½œæˆ
            blob_id = db.execute(
                sql_text("""
                    INSERT INTO files_blob (checksum, blob_data)
                    VALUES (:checksum, :data)
                    RETURNING id
                """),
                {"checksum": file_hash, "data": file_data}
            ).scalar_one()
            print(f"[handler] Created new blob: {blob_id}")

        # 2. files_meta INSERT/UPDATE
        db.execute(
            sql_text("""
                INSERT INTO files_meta (blob_id, file_name, mime_type, size)
                VALUES (:blob_id, :fn, :mt, :sz)
                ON CONFLICT (blob_id) DO UPDATE SET
                    file_name = EXCLUDED.file_name,
                    mime_type = EXCLUDED.mime_type,
                    size = EXCLUDED.size,
                    created_at = NOW()
            """),
            {"blob_id": blob_id, "fn": file_name, "mt": mime_type, "sz": size}
        )
        
        # 3. ãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°æƒ…å ±ã‚’å–å¾—
        file_info = db.execute(
            sql_text("""
                SELECT 
                    m.file_name,
                    m.mime_type,
                    m.size,
                    m.created_at,
                    t.raw_text,
                    t.refined_text
                FROM files_meta m
                LEFT JOIN files_text t ON m.blob_id = t.blob_id
                WHERE m.blob_id = :blob_id
            """),
            {"blob_id": blob_id}
        ).mappings().first()
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        raw_text = file_info.get("raw_text", "") if file_info else ""
        refined_text = file_info.get("refined_text", "") if file_info else ""
        has_raw_text = raw_text and raw_text.strip()
        has_refined_text = refined_text and refined_text.strip()
        
        if not has_raw_text:
            status = "pending_processing"
        elif not has_refined_text:
            status = "text_extracted"
        else:
            status = "processed"
        
        # PDFãƒšãƒ¼ã‚¸æ•°ã®è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        page_count = 0
        if mime_type == "application/pdf" and raw_text:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“ãƒšãƒ¼ã‚¸æ•°æ¨å®š
            page_count = max(1, len(raw_text) // 2000)  # 2000æ–‡å­—/ãƒšãƒ¼ã‚¸ã¨ä»®å®š
    
    return {
        "blob_id": str(blob_id),
        "is_existing": is_existing,
        "file_info": {
            "file_name": file_info["file_name"] if file_info else file_name,
            "mime_type": file_info["mime_type"] if file_info else mime_type,
            "size": file_info["size"] if file_info else size,
            "created_at": file_info["created_at"] if file_info else None,
            "status": status,
            "page_count": page_count
        }
    }

def insert_file_full(
    file_name: str,
    file_data: bytes,
    raw_text: str,
    refined_text: str,
    quality_score: float,
    tags: Optional[Sequence[str]] = None,
    mime_type: str = None,
) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿ç™»éŒ²æ™‚ï¼šblobã€metaã€textã‚’å…¨ã¦ä¿å­˜
    """
    file_hash = _calc_sha256(file_data)
    size = len(file_data)
    mime_type = mime_type or mimetypes.guess_type(file_name)[0] or DEFAULT_MIME
    tags_list = _normalize_tags(tags)
    
    with engine.begin() as db:
        # 1. æ—¢å­˜blobæ¤œç´¢
        existing_blob_id = find_existing_by_checksum(file_hash)
        
        if existing_blob_id:
            # æ—¢å­˜blobã‚’ä½¿ç”¨
            blob_id = existing_blob_id
            print(f"[handler] Using existing blob: {blob_id}")
        else:
            # æ–°è¦blobä½œæˆ
            blob_id = db.execute(
                sql_text("""
                    INSERT INTO files_blob (checksum, blob_data)
                    VALUES (:checksum, :data)
                    RETURNING id
                """),
                {"checksum": file_hash, "data": file_data}
            ).scalar_one()
            print(f"[handler] Created new blob: {blob_id}")

        # 2. files_meta INSERT/UPDATE
        db.execute(
            sql_text("""
                INSERT INTO files_meta (blob_id, file_name, mime_type, size)
                VALUES (:blob_id, :fn, :mt, :sz)
                ON CONFLICT (blob_id) DO UPDATE SET
                    file_name = EXCLUDED.file_name,
                    mime_type = EXCLUDED.mime_type,
                    size = EXCLUDED.size,
                    created_at = NOW()
            """),
            {"blob_id": blob_id, "fn": file_name, "mt": mime_type, "sz": size}
        )

        # 3. files_text INSERT/UPDATE
        db.execute(
            sql_text("""
                INSERT INTO files_text (blob_id, raw_text, refined_text, quality_score, tags)
                VALUES (:blob_id, :raw, :ref, :qs, :tags)
                ON CONFLICT (blob_id) DO UPDATE SET
                    raw_text = EXCLUDED.raw_text,
                    refined_text = EXCLUDED.refined_text,
                    quality_score = EXCLUDED.quality_score,
                    tags = EXCLUDED.tags,
                    updated_at = NOW()
            """),
            {"blob_id": blob_id, "raw": raw_text, "ref": refined_text,
             "qs": quality_score, "tags": tags_list}
        )
    
    return str(blob_id)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å–å¾—ç³»
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_file_meta(blob_id: str) -> Optional[dict]:
    """files_meta ã‹ã‚‰ (file_name, mime_type, size, created_at) ã‚’å–å¾—"""
    with engine.connect() as db:
        row = db.execute(
            sql_text("""
                SELECT file_name, mime_type, size, created_at
                  FROM files_meta
                 WHERE blob_id = :blob_id
            """), {"blob_id": blob_id}
        ).mappings().first()
    return dict(row) if row else None

def get_file_blob(blob_id: str) -> Optional[bytes]:
    """files_blob ã‹ã‚‰ãƒã‚¤ãƒŠãƒªã‚’å–å¾—"""
    with engine.connect() as db:
        row = db.execute(
            sql_text("SELECT blob_data FROM files_blob WHERE id = :blob_id"),
            {"blob_id": blob_id}
        ).first()
    return row[0] if row else None

def get_file_text(blob_id: str) -> Optional[dict]:
    """files_text ã‹ã‚‰ (raw_text, refined_text, quality_score, tags) ã‚’å–å¾—"""
    with engine.connect() as db:
        row = db.execute(
            sql_text("""
                SELECT raw_text, refined_text, quality_score, tags
                  FROM files_text
                 WHERE blob_id = :blob_id
            """), {"blob_id": blob_id}
        ).mappings().first()
    return dict(row) if row else None

def get_all_files() -> List[Dict[str, Any]]:
    """å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’å–å¾—ï¼ˆæ­£ç¢ºãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šä»˜ãï¼‰"""
    sql = """
        SELECT b.id as blob_id, m.file_name, m.mime_type, m.size, m.created_at,
               t.raw_text, t.refined_text
        FROM files_blob b
        JOIN files_meta m ON b.id = m.blob_id
        LEFT JOIN files_text t ON b.id = t.blob_id
        ORDER BY m.created_at DESC
    """
    with engine.connect() as db:
        rows = db.execute(sql_text(sql)).mappings().all()
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã«æ­£ç¢ºãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨­å®š
    result = []
    for row in rows:
        file_data = dict(row)
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å®Ÿæ…‹ã‚’ãƒã‚§ãƒƒã‚¯
        raw_text = file_data.get("raw_text", "")
        refined_text = file_data.get("refined_text", "")
        has_raw_text = raw_text and raw_text.strip()
        has_refined_text = refined_text and refined_text.strip()
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®š
        if not has_raw_text:
            status = "pending_processing"  # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆãªã— = æœªå‡¦ç†
        elif not has_refined_text:
            status = "text_extracted"  # ç”Ÿãƒ†ã‚­ã‚¹ãƒˆã‚ã‚Šã€æ•´å½¢ãƒ†ã‚­ã‚¹ãƒˆãªã— = æœªæ•´å½¢
        else:
            status = "processed"  # ä¸¡æ–¹ã‚ã‚Š = å‡¦ç†å®Œäº†
        
        file_data["status"] = status
        file_data["id"] = file_data["blob_id"]  # APIã¨ã®äº’æ›æ€§ã®ãŸã‚
        result.append(file_data)
    
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ›´æ–°ç³»
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def update_file_text(
    blob_id: str,
    refined_text: str | None = None,
    raw_text: str | None = None,
    quality_score: float | None = None,
    tags: Optional[Sequence[str]] | None = None,
) -> None:
    """files_text ã‚’æ›´æ–°"""
    sets, params = [], {"blob_id": blob_id}
    if refined_text is not None: 
        sets.append("refined_text = :ref")
        params["ref"] = refined_text
    if raw_text is not None: 
        sets.append("raw_text = :raw")
        params["raw"] = raw_text
    if quality_score is not None: 
        sets.append("quality_score = :qs")
        params["qs"] = quality_score
    if tags is not None: 
        sets.append("tags = :tags")
        params["tags"] = _normalize_tags(tags)
    
    if not sets:
        return

    sets.append("updated_at = NOW()")
    sql = f"UPDATE files_text SET {', '.join(sets)} WHERE blob_id = :blob_id"
    with engine.begin() as db:
        db.execute(sql_text(sql), params)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å‰Šé™¤ç³»
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def delete_file(blob_id: str) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ï¼ˆCASCADEå‰Šé™¤ï¼‰"""
    with engine.begin() as db:
        result = db.execute(
            sql_text("DELETE FROM files_blob WHERE id = :blob_id"),
            {"blob_id": blob_id}
        )
        return result.rowcount > 0

def drop_trial_tables() -> None:
    """trialã§å§‹ã¾ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤"""
    with engine.begin() as db:
        # trialã§å§‹ã¾ã‚‹ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢
        result = db.execute(sql_text("""
            SELECT tablename FROM pg_tables 
            WHERE tablename LIKE 'trial%'
        """))
        trial_tables = [row[0] for row in result]
        
        if trial_tables:
            print(f"ğŸ—‘ï¸ trialãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ä¸­: {trial_tables}")
            for table in trial_tables:
                db.execute(sql_text(f'DROP TABLE IF EXISTS "{table}" CASCADE'))
            print(f"âœ… {len(trial_tables)}å€‹ã®trialãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        else:
            print("â„¹ï¸ trialãƒ†ãƒ¼ãƒ–ãƒ«ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def cleanup_obsolete_tables() -> None:
    """ä¸è¦ãªæ—§ç³»ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤"""
    obsolete_tables = [
        'file_images',  # æ—§ç³»ç”»åƒãƒ†ãƒ¼ãƒ–ãƒ«
        'file_texts',   # æ—§ç³»ãƒ†ã‚­ã‚¹ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«
        'files'         # æ—§ç³»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ†ãƒ¼ãƒ–ãƒ«
    ]
    
    with engine.begin() as db:
        dropped_tables = []
        for table in obsolete_tables:
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                result = db.execute(sql_text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = '{table}'
                    )
                """))
                exists = result.scalar()
                
                if exists:
                    db.execute(sql_text(f'DROP TABLE IF EXISTS "{table}" CASCADE'))
                    dropped_tables.append(table)
                    print(f"ğŸ—‘ï¸ å‰Šé™¤: {table}")
                else:
                    print(f"â„¹ï¸ å­˜åœ¨ã—ãªã„: {table}")
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ ({table}): {e}")
        
        if dropped_tables:
            print(f"âœ… {len(dropped_tables)}å€‹ã®ä¸è¦ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {dropped_tables}")
        else:
            print("â„¹ï¸ å‰Šé™¤å¯¾è±¡ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

def cleanup_embedder_tables() -> None:
    """embedderå°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ï¼ˆembeddingsãƒ†ãƒ¼ãƒ–ãƒ«ã«çµ±åˆå¾Œï¼‰"""
    embedder_tables = [
        'intfloat_e5_large_v2_1024',
        'intfloat_e5_small_v2_384',
        'nomic_embed_text_768'
    ]
    
    with engine.begin() as db:
        dropped_tables = []
        for table in embedder_tables:
            try:
                # ãƒ†ãƒ¼ãƒ–ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                result = db.execute(sql_text(f"""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = '{table}'
                    )
                """))
                exists = result.scalar()
                
                if exists:
                    db.execute(sql_text(f'DROP TABLE IF EXISTS "{table}" CASCADE'))
                    dropped_tables.append(table)
                    print(f"ğŸ—‘ï¸ å‰Šé™¤: {table}")
                else:
                    print(f"â„¹ï¸ å­˜åœ¨ã—ãªã„: {table}")
            except Exception as e:
                print(f"âš ï¸ ã‚¨ãƒ©ãƒ¼ ({table}): {e}")
        
        if dropped_tables:
            print(f"âœ… {len(dropped_tables)}å€‹ã®embedderå°‚ç”¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ: {dropped_tables}")
            print("ğŸ“ embeddingsãƒ†ãƒ¼ãƒ–ãƒ«ã«çµ±åˆã•ã‚Œã¾ã—ãŸ")
        else:
            print("â„¹ï¸ å‰Šé™¤å¯¾è±¡ã®embedderãƒ†ãƒ¼ãƒ–ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«æ“ä½œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_embedding_table(table_name: str, dim: int) -> None:
    """åŸ‹ã‚è¾¼ã¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼ˆblob_idå‚ç…§ã«å¤‰æ›´ï¼‰"""
    with engine.begin() as db:
        db.execute(sql_text(f"""
            CREATE TABLE IF NOT EXISTS "{table_name}" (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                content TEXT,
                embedding VECTOR({dim}),
                blob_id UUID REFERENCES files_blob(id) ON DELETE CASCADE
            )
        """))

def bulk_insert_embeddings(table_name: str, records: List[Dict[str, Any]]) -> None:
    """åŸ‹ã‚è¾¼ã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ä¸€æ‹¬INSERTï¼ˆblob_idä½¿ç”¨ï¼‰"""
    if not records: 
        return
    with engine.begin() as db:
        db.execute(
            sql_text(f"""INSERT INTO "{table_name}"
                         (content, embedding, blob_id)
                         VALUES (:content, :embedding, :blob_id)"""),
            records
        )

def fetch_top_chunks(query_vec: str, table_name: str, limit: int = 5) -> list[dict]:
    """ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§ã®è¿‘å‚æ¤œç´¢ï¼ˆæ–°ã—ã„JOINæ§‹é€ ï¼‰"""
    sql = f"""
        SELECT e.content   AS snippet,
               b.id        AS blob_id,
               m.file_name AS file_name
          FROM "{table_name}" AS e
          JOIN files_blob AS b ON e.blob_id = b.id
          JOIN files_meta AS m ON b.id = m.blob_id
         ORDER BY e.embedding <-> '{query_vec}'::vector
         LIMIT :limit
    """
    with engine.connect() as db:
        rows = db.execute(sql_text(sql), {"limit": limit}).mappings().all()
    return [dict(r) for r in rows]

def fetch_top_files(query_vec: str, table_name: str, limit: int = 10) -> list[dict]:
    """ãƒ•ã‚¡ã‚¤ãƒ«å˜ä½ã§ã®è¿‘å‚æ¤œç´¢ï¼ˆæ–°ã—ã„JOINæ§‹é€ ï¼‰"""
    sql = f"""
        SELECT DISTINCT
               b.id            AS blob_id,
               m.file_name     AS file_name,
               t.refined_text  AS refined_text,
               MIN(e.embedding <-> '{query_vec}'::vector) AS distance
          FROM "{table_name}" AS e
          JOIN files_blob AS b ON e.blob_id = b.id
          JOIN files_meta AS m ON b.id = m.blob_id
          JOIN files_text AS t ON b.id = t.blob_id
         GROUP BY b.id, m.file_name, t.refined_text
         ORDER BY distance ASC
         LIMIT :limit
    """
    with engine.connect() as db:
        rows = db.execute(sql_text(sql), {"limit": limit}).mappings().all()
    return [dict(r) for r in rows]

def delete_embedding_for_file(table_name: str, blob_id: str) -> None:
    """æŒ‡å®šblob_idã®å¤ã„åŸ‹ã‚è¾¼ã¿ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ï¼ˆoverwriteç”¨ï¼‰"""
    with engine.begin() as db:
        db.execute(sql_text(f'DELETE FROM "{table_name}" WHERE blob_id = :blob_id'),
                   {"blob_id": blob_id})

def migrate_embeddings_table() -> None:
    """embeddingsãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ–°è¨­è¨ˆã«ç§»è¡Œ"""
    with engine.begin() as db:
        # æ—§ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å‰Šé™¤
        db.execute(sql_text("DROP TABLE IF EXISTS embeddings CASCADE"))
        
        # æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆ
        db.execute(sql_text("""
            CREATE TABLE embeddings (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                content TEXT NOT NULL,
                embedding VECTOR(1536),  -- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¬¡å…ƒï¼ˆå¾Œã§å¤‰æ›´å¯èƒ½ï¼‰
                blob_id UUID REFERENCES files_blob(id) ON DELETE CASCADE,
                embedding_model VARCHAR(100) NOT NULL,
                created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
            )
        """))
        
        print("âœ… embeddingsãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ–°è¨­è¨ˆã«ç§»è¡Œã—ã¾ã—ãŸ")
        print("  - content: ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯")
        print("  - embedding: ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆæ¬¡å…ƒã¯å¾Œã§èª¿æ•´å¯èƒ½ï¼‰")
        print("  - blob_id: ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§")
        print("  - embedding_model: embedderå+æ¬¡å…ƒï¼ˆä¾‹: intfloat_e5_large_v2_1024ï¼‰")

def insert_embedding(content: str, embedding: List[float], blob_id: str, embedding_model: str) -> None:
    """æ–°è¨­è¨ˆembeddingsãƒ†ãƒ¼ãƒ–ãƒ«ã«åŸ‹ã‚è¾¼ã¿ã‚’æŒ¿å…¥"""
    with engine.begin() as db:
        db.execute(sql_text("""
            INSERT INTO embeddings (content, embedding, blob_id, embedding_model)
            VALUES (:content, :embedding, :blob_id, :embedding_model)
        """), {
            "content": content,
            "embedding": embedding,
            "blob_id": blob_id,
            "embedding_model": embedding_model
        })

def get_embeddings_by_model(embedding_model: str) -> List[Dict[str, Any]]:
    """æŒ‡å®šã—ãŸembedding_modelã®åŸ‹ã‚è¾¼ã¿ã‚’å–å¾—"""
    with engine.connect() as db:
        result = db.execute(sql_text("""
            SELECT e.content, e.embedding, e.blob_id, m.file_name
            FROM embeddings e
            JOIN files_meta m ON e.blob_id = m.blob_id
            WHERE e.embedding_model = :embedding_model
            ORDER BY e.created_at DESC
        """), {"embedding_model": embedding_model})
        return [dict(row) for row in result.mappings()]

def search_embeddings(query_embedding: List[float], embedding_model: str, limit: int = 5) -> List[Dict[str, Any]]:
    """æŒ‡å®šã—ãŸembedding_modelã§é¡ä¼¼æ¤œç´¢"""
    with engine.connect() as db:
        result = db.execute(sql_text("""
            SELECT e.content AS snippet,
                   e.blob_id,
                   m.file_name,
                   e.embedding <-> :query_embedding::vector AS distance
            FROM embeddings e
            JOIN files_meta m ON e.blob_id = m.blob_id
            WHERE e.embedding_model = :embedding_model
            ORDER BY distance
            LIMIT :limit
        """), {
            "query_embedding": query_embedding,
            "embedding_model": embedding_model,
            "limit": limit
        })
        return [dict(row) for row in result.mappings()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_file_path(blob_id: str) -> Optional[str]:
    """ãƒ•ã‚¡ã‚¤ãƒ«IDã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—ï¼ˆä¸€æ™‚çš„ãªå®Ÿè£…ï¼‰"""
    # ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    blob_data = get_file_blob(blob_id)
    if not blob_data:
        return None
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ±ºå®š
    meta = get_file_meta(blob_id)
    if not meta:
        return None
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
    suffix = os.path.splitext(meta['file_name'])[1] or '.pdf'
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as temp_file:
        temp_file.write(blob_data)
        return temp_file.name 