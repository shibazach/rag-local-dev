# new/services/processing/pipeline.py
# å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆç®¡ç†

import asyncio
import logging
import time
from typing import Dict, List, AsyncGenerator, Optional

from .processor import FileProcessor
from new.config import LOGGER

class ProcessingPipeline:
    """ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆç®¡ç†"""
    
    def __init__(self):
        self.processor = FileProcessor()
        self.logger = logging.getLogger(__name__)
        self.current_job = None
        self.abort_flag = None
    
    async def process_files(
        self,
        files: List[Dict],
        settings: Dict,
        progress_callback: Optional[callable] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †æ¬¡å‡¦ç†
        
        Args:
            files: ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãƒªã‚¹ãƒˆ
            settings: å‡¦ç†è¨­å®š
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Yields:
            å‡¦ç†é€²æ—ã‚¤ãƒ™ãƒ³ãƒˆ
        """
        total_files = len(files)
        self.abort_flag = {'flag': False}
        pipeline_start_time = time.time()  # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹æ™‚åˆ»è¨˜éŒ²
        results = []  # çµæœã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
        
        try:
            # é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆ
            start_event = {
                'type': 'start',
                'data': {
                    'total_files': total_files,
                    'settings': settings
                }
            }
            self.logger.debug(f"[DEBUG-PIPELINE] é–‹å§‹ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ: {total_files}ä»¶")
            yield start_event
            
            for idx, file_info in enumerate(files, 1):
                # ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒã‚§ãƒƒã‚¯
                if self.abort_flag['flag']:
                    yield {
                        'type': 'cancelled',
                        'message': 'å‡¦ç†ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ'
                    }
                    break
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹
                file_id = str(file_info['file_id'])
                file_name = file_info['file_name']
                file_path = file_info['file_path']
                
                yield {
                    'type': 'file_start',
                    'data': {
                        'file_name': file_name,
                        'file_id': file_id,  # file_idã‚’è¿½åŠ 
                        'file_index': idx,
                        'total_files': total_files,
                        'progress': round((idx - 1) / total_files * 100, 1)
                    }
                }
                
                # å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã®é€²æ—å‡¦ç†ã¯å¾Œã§å®Ÿè£…
                # ç¾åœ¨ã¯ç›´æ¥å‡¦ç†å®Ÿè¡Œ
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Ÿè¡Œ + è©³ç´°æ‰‹é †ã‚¤ãƒ™ãƒ³ãƒˆé€ä¿¡ï¼ˆé‡è¤‡ã‚¤ãƒ™ãƒ³ãƒˆå‰Šé™¤ï¼‰
                
                # è©³ç´°æ‰‹é †ã‚’å—ä¿¡ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½œæˆ
                progress_events = []
                
                def progress_callback(event_data):
                    """processor ã‹ã‚‰ã®è©³ç´°æ‰‹é †ã‚’åé›†"""
                    progress_events.append({
                        'type': 'file_progress',
                        'data': {
                            'file_name': file_name,
                            'file_index': idx,
                            'step': event_data.get('step'),
                            'detail': event_data.get('detail'),
                            'progress': event_data.get('progress'),
                            'ocr_text': event_data.get('ocr_text'),  # OCRãƒ†ã‚­ã‚¹ãƒˆ
                            'llm_prompt': event_data.get('llm_prompt'),  # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
                            'llm_result': event_data.get('llm_result')   # LLMçµæœ
                        }
                    })
                
                result = await self.processor.process_file(
                    file_id=file_id,
                    file_name=file_name,
                    file_path=file_path,
                    settings=settings,
                    progress_callback=progress_callback,  # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯æœ‰åŠ¹åŒ–
                    abort_flag=self.abort_flag
                )
                
                # åé›†ã—ãŸè©³ç´°æ‰‹é †ã‚¤ãƒ™ãƒ³ãƒˆã‚’é€ä¿¡ï¼ˆprocessorã‹ã‚‰ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿ï¼‰
                for progress_event in progress_events:
                    yield progress_event
                
                # çµæœã‚’ä¿å­˜
                results.append(result)
                
                # ãƒ•ã‚¡ã‚¤ãƒ«å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆ
                event_data = {
                    'type': 'file_complete',
                    'data': {
                        'file_name': file_name,
                        'file_index': idx,
                        'total_files': total_files,
                        'progress': round(idx / total_files * 100, 1),
                        'result': result
                    }
                }
                self.logger.debug(f"[DEBUG-PIPELINE] ã‚¤ãƒ™ãƒ³ãƒˆç”Ÿæˆ: {event_data['type']}, ãƒ•ã‚¡ã‚¤ãƒ«: {file_name}")
                yield event_data
                
                # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
                if result['status'] == 'error':
                    self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ [{file_name}]: {result['error']}")
                elif result['status'] == 'cancelled':
                    self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ« [{file_name}]")
                    break
            
            # å…¨ä½“å®Œäº†
            if not self.abort_flag['flag']:
                pipeline_end_time = time.time()
                total_pipeline_time = pipeline_end_time - pipeline_start_time
                
                successful_files = len([r for r in results if r.get('success', False)])
                failed_files = len([r for r in results if not r.get('success', False)])
                
                completion_message = f"å…¨{total_files}ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å®Œäº† (æˆåŠŸ: {successful_files}, å¤±æ•—: {failed_files}) - ã‚°ãƒ©ãƒ³ãƒ‰ãƒˆãƒ¼ã‚¿ãƒ«: {total_pipeline_time:.1f}ç§’"
                
                complete_event = {
                    'type': 'complete',
                    'data': {
                        'total_files': total_files,
                        'successful_files': successful_files,
                        'failed_files': failed_files,
                        'total_pipeline_time': total_pipeline_time,
                        'average_time_per_file': total_pipeline_time / total_files if total_files > 0 else 0,
                        'message': completion_message
                    }
                }
                self.logger.info(f"ğŸ“Š ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†: {completion_message}")
                yield complete_event
            
        except Exception as e:
            self.logger.error(f"ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            yield {
                'type': 'error',
                'message': f'å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'
            }
    
    def cancel_processing(self):
        """å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        if self.abort_flag:
            self.abort_flag['flag'] = True
            self.logger.info("å‡¦ç†ã‚­ãƒ£ãƒ³ã‚»ãƒ«è¦æ±‚")
    
    def is_processing(self) -> bool:
        """å‡¦ç†ä¸­ã‹ã©ã†ã‹"""
        return self.current_job is not None