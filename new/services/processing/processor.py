# new/services/processing/processor.py
# ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ—ãƒ­ã‚»ãƒƒã‚µï¼ˆæ–°ç³»ï¼‰

import asyncio
import time
import logging
import unicodedata
from pathlib import Path
from typing import Dict, List, Optional, AsyncGenerator
import uuid

from new.services.ocr.factory import OCREngineFactory
from new.database.connection import get_db_connection
from new.config import LOGGER, DB_ENGINE

class FileProcessor:
    """æ–°ç³»ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ—ãƒ­ã‚»ãƒƒã‚µ"""
    
    def __init__(self):
        self.ocr_factory = OCREngineFactory()
        self.logger = logging.getLogger(__name__)
    
    async def process_file(
        self,
        file_id: str,
        file_name: str,
        file_path: str,
        settings: Dict,
        progress_callback: Optional[callable] = None,
        abort_flag: Optional[Dict] = None,
        save_to_db: bool = True
    ) -> Dict:
        """
        1ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹
        
        Args:
            file_id: ãƒ•ã‚¡ã‚¤ãƒ«ID
            file_name: ãƒ•ã‚¡ã‚¤ãƒ«å
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            settings: å‡¦ç†è¨­å®š
            progress_callback: é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            abort_flag: ä¸­æ–­ãƒ•ãƒ©ã‚°
            
        Returns:
            å‡¦ç†çµæœè¾æ›¸
        """
        start_time = time.perf_counter()
        result = {
            'success': False,
            'file_id': file_id,
            'file_name': file_name,
            'status': 'processing',
            'steps': {},
            'text_length': 0,
            'processing_time': 0,
            'error': None,
            'ocr_result': {},
            'llm_refined_text': '',
            'quality_score': 0.0
        }
        
        try:
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
            if abort_flag and abort_flag.get('flag', False):
                result['status'] = 'cancelled'
                return result
            
            # 1. OCRå‡¦ç†
            self.logger.info(f"ğŸ“„ {file_name}: ğŸ” OCRå‡¦ç†é–‹å§‹ - ã‚¨ãƒ³ã‚¸ãƒ³: {settings.get('ocr_engine', 'ocrmypdf')}")
            await self._emit_progress(progress_callback, file_name, "ğŸ” OCRå‡¦ç†é–‹å§‹", f"ã‚¨ãƒ³ã‚¸ãƒ³: {settings.get('ocr_engine', 'ocrmypdf')}")
            ocr_result = await self._process_ocr(file_path, settings, abort_flag)
            
            if not ocr_result['success']:
                result['status'] = 'error'
                result['error'] = ocr_result['error']
                self.logger.error(f"ğŸ“„ {file_name}: âŒ OCRå‡¦ç†å¤±æ•— - {ocr_result['error']}")
                await self._emit_progress(progress_callback, file_name, "âŒ OCRå‡¦ç†å¤±æ•—", ocr_result['error'])
                return result
            
            result['steps']['ocr'] = {
                'success': True,
                'processing_time': ocr_result['processing_time'],
                'text_length': len(ocr_result['text'])
            }
            
            raw_text = ocr_result['text']
            result['text_length'] = len(raw_text)
            
            ocr_message = f"{len(raw_text)}æ–‡å­—æŠ½å‡º ({ocr_result['processing_time']:.1f}ç§’)"
            self.logger.info(f"ğŸ“„ {file_name}: ğŸ“Š OCRå‡¦ç†å®Œäº† - {ocr_message}")
            
            # OCRãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€è©³ç´°æƒ…å ±ã‚’é€ä¿¡
            await self._emit_progress_with_data(progress_callback, {
                'file_name': file_name,
                'step': 'ğŸ“Š OCRå‡¦ç†å®Œäº†',
                'detail': ocr_message,
                'progress': 30,
                'ocr_text': raw_text  # OCRãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ 
            })
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
            if abort_flag and abort_flag.get('flag', False):
                result['status'] = 'cancelled'
                return result
            
            # 2. ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
            await self._emit_progress(progress_callback, file_name, "ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–", 40)
            normalized_text = self._normalize_text(raw_text)
            
            # 3. LLMæ•´å½¢å‡¦ç†ï¼ˆè©³ç´°é€²æ—ä»˜ãï¼‰
            self.logger.info(f"ğŸ“„ {file_name}: ğŸ¤– LLMç²¾ç·»åŒ–é–‹å§‹ - ãƒ†ã‚­ã‚¹ãƒˆå“è³ªå‘ä¸Šå‡¦ç†")
            
            # LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆã¨é€ä¿¡å‰è¡¨ç¤º
            llm_prompt = self._create_llm_prompt(normalized_text, settings)
            await self._emit_progress_with_data(progress_callback, {
                'file_name': file_name,
                'step': 'ğŸ“ LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆå®Œäº†',
                'detail': f'ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆå®Œäº† - åŸæ–‡{len(normalized_text)}æ–‡å­—',
                'progress': 35,
                'llm_prompt': llm_prompt,
                'llm_result': None
            })
            
            # LLMå‡¦ç†é–‹å§‹è¡¨ç¤º
            await self._emit_progress(progress_callback, file_name, "ğŸ¤– LLMå‡¦ç†ä¸­...", "å“è³ªå‘ä¸Šå‡¦ç†å®Ÿè¡Œä¸­")
            
            # å®Ÿéš›ã®LLMå‡¦ç†å®Ÿè¡Œ
            llm_start_time = time.perf_counter()
            refined_text = await self._process_llm_refinement(normalized_text, settings, abort_flag)
            llm_processing_time = time.perf_counter() - llm_start_time
            
            if not refined_text:
                # LLMå¤±æ•—æ™‚ã¯æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨
                refined_text = normalized_text
                self.logger.warning(f"ğŸ“„ {file_name}: âš ï¸ LLMå‡¦ç†å¤±æ•— - æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨")
                await self._emit_progress(progress_callback, file_name, "âš ï¸ LLMå‡¦ç†å¤±æ•—", "æ­£è¦åŒ–ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨")
            else:
                llm_message = f"{len(refined_text)}æ–‡å­— (å“è³ªå‘ä¸Š) - å‡¦ç†æ™‚é–“: {llm_processing_time:.1f}ç§’"
                self.logger.info(f"ğŸ“„ {file_name}: âœ¨ LLMç²¾ç·»åŒ–å®Œäº† - {llm_message}")
                
                # LLMçµæœã‚’å«ã‚€è©³ç´°æƒ…å ±ã‚’é€ä¿¡
                await self._emit_progress_with_data(progress_callback, {
                    'file_name': file_name,
                    'step': 'âœ¨ LLMç²¾ç·»åŒ–å®Œäº†',
                    'detail': llm_message,
                    'progress': 60,
                    'llm_prompt': llm_prompt,
                    'llm_result': refined_text
                })
            
            result['steps']['llm'] = {
                'success': bool(refined_text),
                'refined_length': len(refined_text) if refined_text else 0
            }
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
            if abort_flag and abort_flag.get('flag', False):
                result['status'] = 'cancelled'
                return result
            
            # 4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ï¼ˆæ¨¡æ“¬å®Ÿè£…ï¼‰
            models = settings.get('embedding_models', ['intfloat-e5-large-v2'])
            embedding_message = f"ãƒ¢ãƒ‡ãƒ«: {', '.join(models)}"
            self.logger.info(f"ğŸ“„ {file_name}: ğŸ§® åŸ‹ã‚è¾¼ã¿ç”Ÿæˆé–‹å§‹ - {embedding_message}")
            await self._emit_progress(progress_callback, file_name, "ğŸ§® åŸ‹ã‚è¾¼ã¿ç”Ÿæˆé–‹å§‹", embedding_message)
            embedding_result = await self._process_embedding(refined_text, settings, abort_flag)
            
            result['steps']['embedding'] = {
                'success': embedding_result['success'],
                'models': embedding_result.get('models', [])
            }
            
            if embedding_result['success']:
                embedding_complete_msg = f"{len(models)}ãƒ¢ãƒ‡ãƒ«å‡¦ç†å®Œäº†"
                self.logger.info(f"ğŸ“„ {file_name}: âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº† - {embedding_complete_msg}")
                await self._emit_progress(progress_callback, file_name, "âœ… åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå®Œäº†", embedding_complete_msg)
            else:
                self.logger.error(f"ğŸ“„ {file_name}: âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•— - ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼")
                await self._emit_progress(progress_callback, file_name, "âŒ åŸ‹ã‚è¾¼ã¿ç”Ÿæˆå¤±æ•—", "ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼")
            
            # 5. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            if save_to_db:
                self.logger.info(f"ğŸ“„ {file_name}: ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­ - ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜")
                await self._emit_progress(progress_callback, file_name, "ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ä¸­", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨ãƒ™ã‚¯ãƒˆãƒ«ä¿å­˜")
                await self._save_to_database(file_id, raw_text, refined_text, settings)
                self.logger.info(f"ğŸ“„ {file_name}: ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº† - å…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ¸ˆã¿")
                await self._emit_progress(progress_callback, file_name, "ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜å®Œäº†", "å…¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜æ¸ˆã¿")
            else:
                result['db_save'] = {'success': True, 'message': 'DBä¿å­˜ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰'}
            
            # å®Œäº†
            processing_time = time.perf_counter() - start_time
            result['success'] = True
            result['status'] = 'completed'
            
            complete_message = f"åˆè¨ˆ {processing_time:.1f}ç§’"
            self.logger.info(f"ğŸ“„ {file_name}: ğŸ‰ å‡¦ç†å®Œäº† - {complete_message}")
            await self._emit_progress(progress_callback, file_name, "ğŸ‰ å‡¦ç†å®Œäº†", complete_message)
            result['processing_time'] = time.perf_counter() - start_time
            result['ocr_result'] = {'text': raw_text, 'success': True}
            result['llm_refined_text'] = refined_text
            
            await self._emit_progress(progress_callback, file_name, "å‡¦ç†å®Œäº†", 100)
            
        except Exception as e:
            self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼ [{file_name}]: {e}")
            result['status'] = 'error'
            result['error'] = str(e)
            result['processing_time'] = time.perf_counter() - start_time
        
        return result
    
    async def _process_ocr(self, file_path: str, settings: Dict, abort_flag: Optional[Dict]) -> Dict:
        """OCRå‡¦ç†ã‚’å®Ÿè¡Œï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰"""
        start_time = time.perf_counter()
        
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
            if not Path(file_path).exists():
                return {
                    'success': False,
                    'error': f'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}',
                    'text': '',
                    'processing_time': 0
                }
            
            file_ext = Path(file_path).suffix.lower()
            
            # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ç›´æ¥èª­ã¿è¾¼ã¿
            if file_ext in ['.txt', '.csv', '.json']:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        text = f.read()
                    
                    processing_time = time.perf_counter() - start_time
                    
                    return {
                        'success': True,
                        'error': None,
                        'text': text,
                        'processing_time': processing_time
                    }
                except Exception as e:
                    return {
                        'success': False,
                        'error': f'ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}',
                        'text': '',
                        'processing_time': time.perf_counter() - start_time
                    }
            
            # PDF/ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯OCRã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨
            elif file_ext in ['.pdf', '.png', '.jpg', '.jpeg', '.tiff', '.bmp']:
                # OCRã‚¨ãƒ³ã‚¸ãƒ³å–å¾—
                engine_id = settings.get('ocr_engine', 'ocrmypdf')
                self.logger.info(f"OCRã‚¨ãƒ³ã‚¸ãƒ³ {engine_id} ã§å‡¦ç†é–‹å§‹ä¸­...")
                
                # å‡¦ç†ä¸­ã®é€²æ—è¡¨ç¤ºï¼ˆ22ç§’ç„¡éŸ³å¯¾ç­–ï¼‰
                processing_start = time.perf_counter()
                ocr_result = self.ocr_factory.process_file(file_path, engine_id=engine_id)
                processing_time = time.perf_counter() - processing_start
                
                self.logger.info(f"OCRå‡¦ç†å®Œäº† - å‡¦ç†æ™‚é–“: {processing_time:.1f}ç§’")
                
                # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
                if abort_flag and abort_flag.get('flag', False):
                    return {
                        'success': False,
                        'error': 'ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ',
                        'text': '',
                        'processing_time': ocr_result.processing_time
                    }
                
                return {
                    'success': ocr_result.success,
                    'error': ocr_result.error,
                    'text': ocr_result.text or '',
                    'processing_time': ocr_result.processing_time
                }
            
            else:
                return {
                    'success': False,
                    'error': f'ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {file_ext}',
                    'text': '',
                    'processing_time': time.perf_counter() - start_time
                }
            
        except Exception as e:
            return {
                'success': False,
                'error': f'OCRå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}',
                'text': '',
                'processing_time': time.perf_counter() - start_time
            }
    
    def _normalize_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–"""
        if not text:
            return ""
        
        # Unicodeæ­£è¦åŒ–
        normalized = unicodedata.normalize("NFKC", text)
        
        # æ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
        lines = []
        for line in normalized.split('\n'):
            line = line.strip()
            if line:
                lines.append(line)
        
        return '\n'.join(lines)
    
    async def _process_llm_refinement(self, text: str, settings: Dict, abort_flag: Optional[Dict]) -> str:
        """LLMæ•´å½¢å‡¦ç†ï¼ˆOllamaçµ±åˆç‰ˆï¼‰"""
        try:
            # Ollamaã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
            from new.services.llm import OllamaRefiner, OllamaClient
            
            # LLMè¨­å®šå–å¾—
            llm_model = settings.get('llm_model', 'phi4-mini')
            language = settings.get('language', 'ja')
            quality_threshold = settings.get('quality_threshold', 0.7)
            
            # ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«åãƒã‚§ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ¤å®šï¼‰
            if 'invalid' in llm_model.lower():
                self.logger.info(f"ç„¡åŠ¹ãªãƒ¢ãƒ‡ãƒ«æŒ‡å®šæ¤œå‡º: {llm_model} â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†")
                return self._fallback_text_refinement(text)
            
            # Ollamaæ¥ç¶šç¢ºèª
            client = OllamaClient(model=llm_model)
            is_available = await client.is_available()
            
            if not is_available:
                self.logger.warning(f"Ollamaåˆ©ç”¨ä¸å¯ â†’ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†")
                return self._fallback_text_refinement(text)
            
            refiner = OllamaRefiner(client)
            
            self.logger.info(f"LLMæ•´å½¢é–‹å§‹: ãƒ¢ãƒ‡ãƒ«={llm_model}, è¨€èª={language}")
            
            # å®Ÿéš›ã®Ollamaæ•´å½¢å®Ÿè¡Œ
            refined_text, detected_lang, quality_score = await refiner.refine_text(
                raw_text=text,
                abort_flag=abort_flag,
                language=language,
                quality_threshold=quality_threshold
            )
            
            self.logger.info(f"LLMæ•´å½¢å®Œäº†: å“è³ªã‚¹ã‚³ã‚¢={quality_score:.2f}, è¨€èª={detected_lang}")
            
            return refined_text
                
        except ImportError as e:
            self.logger.warning(f"Ollamaçµ±åˆæœªä½¿ç”¨ï¼ˆä¾å­˜é–¢ä¿‚ä¸è¶³ï¼‰: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ­£è¦åŒ–å‡¦ç†ã®ã¿ + ç¾å®Ÿçš„ãªå‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            # æ–‡å­—æ•°ã«å¿œã˜ãŸå‡¦ç†æ™‚é–“ï¼ˆ1000æ–‡å­—ã‚ãŸã‚Š3-5ç§’ï¼‰
            char_count = len(text) if text else 0
            processing_time = max(3.0, (char_count / 1000) * 4.0)  # æœ€ä½3ç§’ã€1000æ–‡å­—ã§4ç§’
            await asyncio.sleep(processing_time)
            self.logger.info(f"ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œäº† - {char_count}æ–‡å­—ã€{processing_time:.1f}ç§’")
            return self._fallback_text_refinement(text)
        except Exception as e:
            self.logger.error(f"LLMæ•´å½¢ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ­£è¦åŒ–å‡¦ç†ã®ã¿ + ç¾å®Ÿçš„ãªå‡¦ç†æ™‚é–“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            char_count = len(text) if text else 0
            processing_time = max(3.0, (char_count / 1000) * 4.0)
            await asyncio.sleep(processing_time)
            return self._fallback_text_refinement(text)
    
    def _create_llm_prompt(self, text: str, settings: Dict) -> str:
        """LLMãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆé«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨ï¼‰"""
        try:
            from new.services.llm.prompt_loader import get_prompt_loader
            
            language = settings.get('language', 'ja')
            
            # é«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å–å¾—
            prompt_loader = get_prompt_loader()
            prompt_template = prompt_loader.load_prompt("refine_prompt_advanced", language)
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            prompt = prompt_loader.format_prompt(prompt_template, TEXT=text)
            
            self.logger.info(f"é«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½¿ç”¨: {len(prompt_template)}æ–‡å­—, è¨€èª={language}")
            return prompt
            
        except Exception as e:
            self.logger.warning(f"é«˜å“è³ªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä½¿ç”¨: {e}")
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç°¡æ˜“ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            language = settings.get('language', 'ja')
            quality_threshold = settings.get('quality_threshold', 0.7)
            
            prompt = f"""ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å“è³ªå‘ä¸Šã—ã¦ãã ã•ã„ã€‚

è¨€èª: {language}
å“è³ªé–¾å€¤: {quality_threshold}

åŸæ–‡:
{text}

ä¿®æ­£æŒ‡ç¤º:
- OCRèª¤å­—ãƒ»è„±å­—ã®ä¿®æ­£
- ä¸è‡ªç„¶ãªæ”¹è¡Œãƒ»ç©ºç™½ã®æ•´ç†
- æ–‡ç« æ§‹é€ ã®æ”¹å–„
- æ„å‘³ã‚’ä¿æŒã—ã¤ã¤èª­ã¿ã‚„ã™ãæ•´å½¢

ä¿®æ­£å¾Œãƒ†ã‚­ã‚¹ãƒˆ:"""
            
            return prompt

    def _fallback_text_refinement(self, text: str) -> str:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ï¼ˆå®Ÿéš›ã®å“è³ªå‘ä¸Šå‡¦ç†ï¼‰"""
        import re
        
        # ã‚ˆã‚Šå®Ÿè³ªçš„ãªå“è³ªå‘ä¸Šå‡¦ç†
        # 1. Unicodeæ­£è¦åŒ–
        import unicodedata
        text = unicodedata.normalize('NFKC', text)
        
        # 2. OCRèª¤å­—ã®ä¸€èˆ¬çš„ãªä¿®æ­£
        ocr_corrections = {
            r'(\d+)\s*å¹´\s*(\d+)\s*æœˆ': r'\1å¹´\2æœˆ',  # å¹´æœˆã®ç©ºç™½é™¤å»
            r'(\d+)\s*æ—¥': r'\1æ—¥',  # æ—¥ä»˜ã®ç©ºç™½é™¤å»
            r'æ ªå¼ä¼š\s*ç¤¾': 'æ ªå¼ä¼šç¤¾',  # æ ªå¼ä¼šç¤¾ã®ä¿®æ­£
            r'æœ‰é™ä¼š\s*ç¤¾': 'æœ‰é™ä¼šç¤¾',  # æœ‰é™ä¼šç¤¾ã®ä¿®æ­£
            r'([ã‚-ã‚“])\s+([ã‚-ã‚“])': r'\1\2',  # ã²ã‚‰ãŒãªé–“ã®ä¸è¦ãªç©ºç™½
            r'([ã‚¢-ãƒ³])\s+([ã‚¢-ãƒ³])': r'\1\2',  # ã‚«ã‚¿ã‚«ãƒŠé–“ã®ä¸è¦ãªç©ºç™½
        }
        
        for pattern, replacement in ocr_corrections.items():
            text = re.sub(pattern, replacement, text)
        
        # 3. æ”¹è¡Œãƒ»ç©ºç™½ã®æ•´ç†
        text = re.sub(r'^[\s\u3000]+$', '', text, flags=re.MULTILINE)  # ç©ºç™½ã®ã¿ã®è¡Œå‰Šé™¤
        text = re.sub(r'\n{3,}', '\n\n', text)  # 3ã¤ä»¥ä¸Šã®é€£ç¶šæ”¹è¡Œã‚’2ã¤ã«
        text = re.sub(r'[ \t]{2,}', ' ', text)  # è¤‡æ•°ã®åŠè§’ç©ºç™½ã‚’1ã¤ã«
        text = re.sub(r'[\u3000]{2,}', 'ã€€', text)  # è¤‡æ•°ã®å…¨è§’ç©ºç™½ã‚’1ã¤ã«
        
        # 4. å¥èª­ç‚¹ã®æ­£è¦åŒ–
        text = re.sub(r'[ã€ï¼Œ]', 'ã€', text)  # ã‚«ãƒ³ãƒã‚’èª­ç‚¹ã«
        text = re.sub(r'[ã€‚ï¼]', 'ã€‚', text)  # ãƒ”ãƒªã‚ªãƒ‰ã‚’å¥ç‚¹ã«
        
        return text.strip()
    
    async def _process_embedding(self, text: str, settings: Dict, abort_flag: Optional[Dict]) -> Dict:
        """ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ï¼ˆæ¨¡æ“¬å®Ÿè£…ï¼‰"""
        try:
            await asyncio.sleep(1.0)  # ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            
            # ä¸­æ–­ãƒã‚§ãƒƒã‚¯
            if abort_flag and abort_flag.get('flag', False):
                return {'success': False, 'models': []}
            
            # æ¨¡æ“¬å®Ÿè£…
            embedding_models = settings.get('embedding_models', ['intfloat-e5-large-v2'])
            
            return {
                'success': True,
                'models': embedding_models,
                'vector_count': len(text.split()) // 10  # æ¨¡æ“¬ãƒ™ã‚¯ãƒˆãƒ«æ•°
            }
            
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return {'success': False, 'models': []}
    
    async def _save_to_database(self, file_id: str, raw_text: str, refined_text: str, settings: Dict):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            conn = DB_ENGINE.connect()
            try:
                from sqlalchemy import text
                
                # files_textãƒ†ãƒ¼ãƒ–ãƒ«ã«ä¿å­˜/æ›´æ–°
                query = text("""
                    INSERT INTO files_text (blob_id, raw_text, refined_text, quality_score, updated_at)
                    VALUES (:blob_id, :raw_text, :refined_text, :quality_score, NOW())
                    ON CONFLICT (blob_id) 
                    DO UPDATE SET 
                        raw_text = EXCLUDED.raw_text,
                        refined_text = EXCLUDED.refined_text,
                        quality_score = EXCLUDED.quality_score,
                        updated_at = NOW()
                """)
                
                conn.execute(query, {
                    'blob_id': file_id,
                    'raw_text': raw_text,
                    'refined_text': refined_text,
                    'quality_score': 0.8  # æ¨¡æ“¬å“è³ªã‚¹ã‚³ã‚¢
                })
                conn.commit()
                
            finally:
                conn.close()
                
        except Exception as e:
            self.logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã‚¨ãƒ©ãƒ¼ [{file_id}]: {e}")
            raise
    
    async def _emit_progress(self, callback: Optional[callable], file_name: str, step: str, progress: int):
        """é€²æ—é€šçŸ¥ã‚’é€å‡º"""
        if callback is None:
            return  # callbackãŒNoneã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
            
        try:
            # callbackãŒé€šå¸¸ã®é–¢æ•°ã‹asyncé–¢æ•°ã‹ã‚’åˆ¤å®š
            import asyncio
            result = callback({
                'file_name': file_name,
                'step': step,
                'progress': progress
            })
            # asyncé–¢æ•°ã®å ´åˆã®ã¿await
            if asyncio.iscoroutine(result):
                await result
        except Exception as e:
            self.logger.error(f"é€²æ—é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _emit_progress_with_data(self, callback: Optional[callable], event_data: Dict):
        """è©³ç´°ãƒ‡ãƒ¼ã‚¿ä»˜ãé€²æ—é€šçŸ¥ã‚’é€å‡º"""
        if callback is None:
            return  # callbackãŒNoneã®å ´åˆã¯ä½•ã‚‚ã—ãªã„
            
        try:
            # callbackãŒé€šå¸¸ã®é–¢æ•°ã‹asyncé–¢æ•°ã‹ã‚’åˆ¤å®š
            import asyncio
            result = callback(event_data)
            # asyncé–¢æ•°ã®å ´åˆã®ã¿await
            if asyncio.iscoroutine(result):
                await result
        except Exception as e:
            self.logger.error(f"è©³ç´°é€²æ—é€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")