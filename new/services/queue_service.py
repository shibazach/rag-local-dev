#!/usr/bin/env python3
# new/services/queue_service.py
# å‡¦ç†ã‚­ãƒ¥ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹

import logging
import uuid
from datetime import datetime
from typing import List, Dict, Any, Optional
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, desc
from pathlib import Path

from ..models import ProcessingQueue, File, FileText, FileImage, Embedding
from ..config import LOGGER
from .text_processor import TextProcessor
from .image_processor import ImageProcessor
from .embedding_service import EmbeddingService

class QueueService:
    """å‡¦ç†ã‚­ãƒ¥ãƒ¼ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        self.text_processor = TextProcessor()
        self.image_processor = ImageProcessor()
        self.embedding_service = EmbeddingService()
    
    def add_to_queue(self, db: Session, file_id: str, task_type: str, priority: int = 0) -> bool:
        """ã‚­ãƒ¥ãƒ¼ã«ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ """
        try:
            LOGGER.info(f"ğŸ“‹ ã‚­ãƒ¥ãƒ¼ã«ã‚¿ã‚¹ã‚¯è¿½åŠ : ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}, ã‚¿ã‚¹ã‚¯={task_type}")
            
            # æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            existing_task = db.query(ProcessingQueue).filter(
                and_(
                    ProcessingQueue.file_id == file_id,
                    ProcessingQueue.task_type == task_type,
                    ProcessingQueue.status.in_(["pending", "processing"])
                )
            ).first()
            
            if existing_task:
                LOGGER.warning(f"âš ï¸ æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ãŒå­˜åœ¨: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}, ã‚¿ã‚¹ã‚¯={task_type}")
                return False
            
            # æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
            task = ProcessingQueue(
                file_id=file_id,
                task_type=task_type,
                priority=priority,
                status="pending"
            )
            
            db.add(task)
            db.commit()
            
            LOGGER.info(f"âœ… ã‚¿ã‚¹ã‚¯è¿½åŠ å®Œäº†: ID={task.id}")
            return True
            
        except Exception as e:
            LOGGER.error(f"ã‚¿ã‚¹ã‚¯è¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
            db.rollback()
            return False
    
    def get_pending_tasks(self, db: Session, task_type: str = None, limit: int = 10) -> List[Dict[str, Any]]:
        """å¾…æ©Ÿä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—"""
        try:
            query = db.query(ProcessingQueue).filter(
                ProcessingQueue.status == "pending"
            )
            
            if task_type:
                query = query.filter(ProcessingQueue.task_type == task_type)
            
            tasks = query.order_by(desc(ProcessingQueue.priority), ProcessingQueue.created_at).limit(limit).all()
            
            return [
                {
                    "id": task.id,
                    "file_id": str(task.file_id),
                    "task_type": task.task_type,
                    "priority": task.priority,
                    "status": task.status,
                    "created_at": task.created_at.isoformat() if task.created_at else None
                }
                for task in tasks
            ]
            
        except Exception as e:
            LOGGER.error(f"å¾…æ©Ÿã‚¿ã‚¹ã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def process_task(self, db: Session, task_id: int) -> bool:
        """ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        try:
            task = db.query(ProcessingQueue).filter(ProcessingQueue.id == task_id).first()
            if not task:
                LOGGER.error(f"ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ID={task_id}")
                return False
            
            LOGGER.info(f"ğŸ”„ ã‚¿ã‚¹ã‚¯å‡¦ç†é–‹å§‹: ID={task_id}, ãƒ•ã‚¡ã‚¤ãƒ«ID={task.file_id}, ã‚¿ã‚¹ã‚¯={task.task_type}")
            
            # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            task.status = "processing"
            task.started_at = datetime.now()
            db.commit()
            
            success = False
            
            try:
                if task.task_type == "text_extraction":
                    success = self._process_text_extraction(db, task.file_id)
                elif task.task_type == "image_extraction":
                    success = self._process_image_extraction(db, task.file_id)
                elif task.task_type == "vectorization":
                    success = self._process_vectorization(db, task.file_id)
                elif task.task_type == "indexing":
                    success = self._process_indexing(db, task.file_id)
                else:
                    LOGGER.error(f"æœªå¯¾å¿œã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task.task_type}")
                    success = False
                
            except Exception as e:
                LOGGER.error(f"ã‚¿ã‚¹ã‚¯å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                success = False
            
            # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            if success:
                task.status = "completed"
                task.completed_at = datetime.now()
                LOGGER.info(f"âœ… ã‚¿ã‚¹ã‚¯å®Œäº†: ID={task_id}")
            else:
                task.status = "failed"
                task.error_message = str(e) if 'e' in locals() else "Unknown error"
                task.retry_count += 1
                LOGGER.error(f"âŒ ã‚¿ã‚¹ã‚¯å¤±æ•—: ID={task_id}")
            
            db.commit()
            return success
            
        except Exception as e:
            LOGGER.error(f"ã‚¿ã‚¹ã‚¯å‡¦ç†å…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _process_text_extraction(self, db: Session, file_id: str) -> bool:
        """ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå‡¦ç†"""
        try:
            file = db.query(File).filter(File.id == file_id).first()
            if not file:
                return False
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            from ..utils.file_converter import FileConverter
            file_path = Path(file.file_path)
            
            text_content = FileConverter.extract_text_from_file(file_path)
            if not text_content:
                LOGGER.warning(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå¤±æ•—: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
                return False
            
            # ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†
            processed_text = self.text_processor.process_text(text_content, str(file_id))
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            file_text = FileText(
                file_id=file_id,
                raw_text=processed_text["raw_text"],
                refined_text=processed_text["cleaned_text"],
                text_chunks=processed_text["chunks"],
                quality_score=processed_text["metadata"].get("quality_score", 0.0),
                language=processed_text["metadata"].get("language", "ja")
            )
            
            db.add(file_text)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            file.status = "text_extracted"
            file.processing_stage = "text_extracted"
            file.file_metadata = processed_text["metadata"]
            
            db.commit()
            LOGGER.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
            return True
            
        except Exception as e:
            LOGGER.error(f"ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _process_image_extraction(self, db: Session, file_id: str) -> bool:
        """ç”»åƒæŠ½å‡ºå‡¦ç†"""
        try:
            file = db.query(File).filter(File.id == file_id).first()
            if not file:
                return False
            
            file_path = Path(file.file_path)
            if file_path.suffix.lower() != '.pdf':
                LOGGER.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {file_path}")
                return False
            
            # PDFã‹ã‚‰ç”»åƒã‚’æŠ½å‡º
            images = self.image_processor.extract_images_from_pdf(file_path)
            
            if not images:
                LOGGER.info(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
                return True  # ç”»åƒãŒãªã„å ´åˆã¯æˆåŠŸã¨ã™ã‚‹
            
            # å„ç”»åƒã‚’å‡¦ç†
            for img_data in images:
                processed_image = self.image_processor.process_image(
                    img_data["image_data"],
                    img_data["page_number"],
                    img_data["image_number"],
                    str(file_id)
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
                file_image = FileImage(
                    file_id=file_id,
                    page_number=processed_image["page_number"],
                    image_number=processed_image["image_number"],
                    image_size=processed_image["image_info"],
                    image_format=processed_image["image_info"]["format"],
                    ocr_text=processed_image["ocr_text"],
                    llm_description=processed_image["llm_description"],
                    llm_analysis=processed_image["llm_analysis"],
                    confidence_score=processed_image["ocr_confidence"],
                    processing_status=processed_image["processing_status"]
                )
                
                db.add(file_image)
            
            db.commit()
            LOGGER.info(f"âœ… ç”»åƒæŠ½å‡ºå®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}, {len(images)}å€‹ã®ç”»åƒ")
            return True
            
        except Exception as e:
            LOGGER.error(f"ç”»åƒæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _process_vectorization(self, db: Session, file_id: str) -> bool:
        """ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†"""
        try:
            # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
            file_text = db.query(FileText).filter(FileText.file_id == file_id).first()
            if not file_text or not file_text.text_chunks:
                LOGGER.warning(f"ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
                return False
            
            # æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ã‚’å‰Šé™¤
            db.query(Embedding).filter(Embedding.file_id == file_id).delete()
            
            # ãƒãƒ£ãƒ³ã‚¯ã‚’ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            embeddings = self.embedding_service.batch_create_embeddings(file_text.text_chunks)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            for embedding_data in embeddings:
                embedding = Embedding(
                    file_id=file_id,
                    chunk_id=embedding_data["chunk_id"],
                    text_chunk=embedding_data["text"],
                    embedding_model=embedding_data["embedding_model"],
                    embedding_vector=embedding_data["embedding_vector"],
                    chunk_index=embedding_data["chunk_index"],
                    chunk_size=embedding_data["size"]
                )
                db.add(embedding)
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
            file = db.query(File).filter(File.id == file_id).first()
            file.processing_stage = "vectorized"
            
            db.commit()
            LOGGER.info(f"âœ… ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}, {len(embeddings)}å€‹ã®ãƒ™ã‚¯ãƒˆãƒ«")
            return True
            
        except Exception as e:
            LOGGER.error(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def _process_indexing(self, db: Session, file_id: str) -> bool:
        """ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå‡¦ç†"""
        try:
            # ç¾åœ¨ã¯ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨åŒã˜å‡¦ç†
            # å°†æ¥çš„ã«ã¯pgvectorã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆãªã©ã‚’è¡Œã†
            file = db.query(File).filter(File.id == file_id).first()
            file.processing_stage = "indexed"
            file.status = "completed"
            
            db.commit()
            LOGGER.info(f"âœ… ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆå®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
            return True
            
        except Exception as e:
            LOGGER.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def process_all_pending_tasks(self, db: Session) -> Dict[str, int]:
        """å…¨å¾…æ©Ÿã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        try:
            pending_tasks = self.get_pending_tasks(db, limit=100)
            LOGGER.info(f"ğŸ”„ ä¸€æ‹¬å‡¦ç†é–‹å§‹: {len(pending_tasks)}å€‹ã®ã‚¿ã‚¹ã‚¯")
            
            success_count = 0
            error_count = 0
            
            for task_data in pending_tasks:
                if self.process_task(db, task_data["id"]):
                    success_count += 1
                else:
                    error_count += 1
            
            LOGGER.info(f"ğŸ‰ ä¸€æ‹¬å‡¦ç†å®Œäº†: æˆåŠŸ={success_count}ä»¶, å¤±æ•—={error_count}ä»¶")
            return {
                "success": success_count,
                "error": error_count,
                "total": len(pending_tasks)
            }
            
        except Exception as e:
            LOGGER.error(f"ä¸€æ‹¬å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": 0, "error": 0, "total": 0} 