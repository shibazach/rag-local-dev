#!/usr/bin/env python3
# new/services/text_processor.py
# ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹

import re
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import hashlib

from ..config import LOGGER

class TextProcessor:
    """ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.chunk_size = 1000  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        self.overlap_size = 200  # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚µã‚¤ã‚º
    
    def clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not text:
            return ""
        
        # åŸºæœ¬çš„ãªã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        text = re.sub(r'\s+', ' ', text)  # è¤‡æ•°ã®ç©ºç™½ã‚’å˜ä¸€ã«
        text = re.sub(r'\n\s*\n', '\n', text)  # ç©ºè¡Œã®å‰Šé™¤
        text = text.strip()
        
        return text
    
    def detect_language(self, text: str) -> str:
        """è¨€èªæ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not text:
            return "unknown"
        
        # æ—¥æœ¬èªæ–‡å­—ã®æ¤œå‡º
        japanese_chars = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text))
        total_chars = len(text)
        
        if japanese_chars / total_chars > 0.1:
            return "ja"
        else:
            return "en"
    
    def split_into_chunks(self, text: str, chunk_size: int = None, overlap: int = None) -> List[Dict[str, Any]]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
        if not text:
            return []
        
        chunk_size = chunk_size or self.chunk_size
        overlap = overlap or self.overlap_size
        
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            
            # æ–‡ã®å¢ƒç•Œã§åˆ†å‰²
            if end < len(text):
                # å¥èª­ç‚¹ã§åˆ†å‰²ã‚’è©¦è¡Œ
                for punct in ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', '\n']:
                    last_punct = text.rfind(punct, start, end)
                    if last_punct > start + chunk_size // 2:  # åŠåˆ†ä»¥ä¸Šé€²ã‚“ã§ã„ã‚Œã°åˆ†å‰²
                        end = last_punct + 1
                        break
            
            chunk_text = text[start:end].strip()
            if chunk_text:
                chunk_id = self._generate_chunk_id(chunk_text, start)
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "start_pos": start,
                    "end_pos": end,
                    "size": len(chunk_text)
                })
            
            start = end - overlap
            if start >= len(text):
                break
        
        return chunks
    
    def _generate_chunk_id(self, text: str, position: int) -> str:
        """ãƒãƒ£ãƒ³ã‚¯IDã‚’ç”Ÿæˆ"""
        content_hash = hashlib.md5(text.encode('utf-8')).hexdigest()[:8]
        return f"chunk_{position}_{content_hash}"
    
    def extract_metadata(self, text: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        metadata = {
            "total_length": len(text),
            "word_count": len(text.split()),
            "line_count": len(text.split('\n')),
            "language": self.detect_language(text),
            "has_japanese": bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', text)),
            "has_english": bool(re.search(r'[a-zA-Z]', text)),
            "has_numbers": bool(re.search(r'\d', text))
        }
        
        # æ®µè½æ•°
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        metadata["paragraph_count"] = len(paragraphs)
        
        return metadata
    
    def process_text(self, text: str, file_id: str) -> Dict[str, Any]:
        """ãƒ†ã‚­ã‚¹ãƒˆã®å®Œå…¨å‡¦ç†"""
        try:
            LOGGER.info(f"ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†é–‹å§‹: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            cleaned_text = self.clean_text(text)
            LOGGER.info(f"âœ… ãƒ†ã‚­ã‚¹ãƒˆã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†: {len(cleaned_text)}æ–‡å­—")
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
            metadata = self.extract_metadata(cleaned_text)
            LOGGER.info(f"âœ… ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºå®Œäº†: {metadata}")
            
            # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
            chunks = self.split_into_chunks(cleaned_text)
            LOGGER.info(f"âœ… ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²å®Œäº†: {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯")
            
            # çµæœã‚’æ§‹ç¯‰
            result = {
                "file_id": file_id,
                "raw_text": text,
                "cleaned_text": cleaned_text,
                "chunks": chunks,
                "metadata": metadata,
                "total_chunks": len(chunks),
                "processing_info": {
                    "chunk_size": self.chunk_size,
                    "overlap_size": self.overlap_size,
                    "language": metadata["language"]
                }
            }
            
            LOGGER.info(f"ğŸ‰ ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†å®Œäº†: ãƒ•ã‚¡ã‚¤ãƒ«ID={file_id}")
            return result
            
        except Exception as e:
            LOGGER.error(f"ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            raise 