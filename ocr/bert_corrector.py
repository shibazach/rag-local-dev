# ocr/bert_corrector.py
import torch
from transformers import AutoTokenizer, BertForMaskedLM

from src import bootstrap
from src.utils import debug_print

# REM: ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€BERTãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®èª¤ã‚Šã‚’ä¿®æ­£ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã™ã€‚
AVAILABLE_MODELS = {
    "tohoku": "cl-tohoku/bert-base-japanese",
    "daigo": "daigo/bert-base-japanese-sentiment"
    # âš ï¸ "yasuo": å‰Šé™¤ã€‚éå…¬é–‹ or å­˜åœ¨ã—ãªã„ãŸã‚ç„¡åŠ¹
}

# REM: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨ãƒ†ã‚­ã‚¹ãƒˆã®ä¿®æ­£ã‚’è¡Œã†é–¢æ•°
def load_model(model_key):
    model_name = AVAILABLE_MODELS.get(model_key)
    if not model_name:
        raise ValueError(f"æœªå¯¾å¿œã®ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ¼ã§ã™: {model_key}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)  # âœ… è‡ªå‹•ã§æœ€é©ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’é¸ã¶
    model = BertForMaskedLM.from_pretrained(model_name)
    model.eval()
    return tokenizer, model

# REM: ãƒ†ã‚­ã‚¹ãƒˆã®èª¤ã‚Šã‚’ä¿®æ­£ã™ã‚‹é–¢æ•°
def correct_text_bert(text, model_key="tohoku", top_k=1):
    tokenizer, model = load_model(model_key)
    tokens = tokenizer.tokenize(text)
    corrected_tokens = tokens.copy()

    for i in range(len(tokens)):
        masked_tokens = tokens.copy()
        masked_tokens[i] = "[MASK]"

        input_ids = tokenizer.convert_tokens_to_ids(masked_tokens)
        input_tensor = torch.tensor([input_ids])

        with torch.no_grad():
            predictions = model(input_tensor).logits

        masked_index = i
        top_preds = torch.topk(predictions[0, masked_index], top_k).indices.tolist()
        predicted_token = tokenizer.convert_ids_to_tokens([top_preds[0]])[0]

        if predicted_token != tokens[i]:
            debug_print(f"ğŸ” [{model_key}] å€™è£œ: {tokens[i]} â†’ {predicted_token}")
            corrected_tokens[i] = predicted_token

    corrected_text = tokenizer.convert_tokens_to_string(corrected_tokens)
    return corrected_text
