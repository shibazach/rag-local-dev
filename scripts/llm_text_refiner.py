# scripts/llm_text_refiner.py

from src.ocr_utils.spellcheck import correct_text
from scripts.refine_prompter import get_prompt_by_lang
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langdetect import detect_langs
from langchain_ollama import ChatOllama

from src import bootstrap  # ãƒ‘ã‚¹è¨­å®šã®ã¿
from src.error_handler import install_global_exception_handler
from src.config import OLLAMA_BASE, OLLAMA_MODEL

# REM: ä¾‹å¤–ç™ºç”Ÿæ™‚ã®ãƒ­ã‚°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨˜éŒ²ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ã‚’æœ‰åŠ¹åŒ–
install_global_exception_handler()

def detect_language(text: str, force_lang=None) -> str:
    if force_lang:
        return force_lang
    try:
        lang = detect_langs(text)[0].lang
        return "en" if lang == "en" else "ja"
    except Exception:
        return "ja"

def refine_text_with_llm(raw_text: str,
                         model: str = OLLAMA_MODEL,
                         force_lang: str = None):
    # 1) OCRèª¤å­—è£œæ­£
    corrected = correct_text(raw_text)

    # 2) è¨€èªåˆ¤å®šï¼å¼·åˆ¶ï¼ˆã“ã“ã§ã¯æ—¥æœ¬èªå›ºå®šï¼‰
    lang = detect_language(corrected, force_lang)
    lang = "ja"

    # 3) ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ï¼šæ–‡å­—æ•°ã¨æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    text_len       = len(corrected)
    token_estimate = int(text_len * 1.6)
    print(f"ğŸ§  LLMæ•´å½¢ã‚’é–‹å§‹ï¼ˆæ–‡å­—æ•°: {text_len}, æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: {token_estimate}ï¼‰", flush=True)
    print(f"ğŸ”¤ æ•´å½¢è¨€èª: {lang}", flush=True)

    # 4) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå–å¾—
    _, base_prompt = get_prompt_by_lang(lang)

    # 5) æ—¥æœ¬èªå›ç­”ã‚’å¼·åˆ¶
    user_prompt = "æ¬¡ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚\n" + base_prompt

    # 6) ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆï¼ˆ{TEXT} ã¨ {input_text} ã®ä¸¡æ–¹ã«å¯¾å¿œï¼‰
    prompt = user_prompt.replace("{TEXT}", corrected).replace("{input_text}", corrected)
    print(f"ğŸ§¾ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆåˆæˆå¾Œã®æ–‡å­—æ•°: {len(prompt)}", flush=True)

    # 7) ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ‹¡å¼µ
    generation_kwargs = {
        "max_new_tokens": 1024,
        "min_length": max(1, int(len(corrected) * 0.8)),
        "temperature": 0.7,
        # "top_p": 0.9,  # å¿…è¦ãªã‚‰è¿½åŠ 
    }

    # 8) Ollama LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    llm = ChatOllama(
        model=model,
        base_url=OLLAMA_BASE,
        **generation_kwargs
    )

    # 9) LangChain ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰
    prompt_template = PromptTemplate.from_template(user_prompt)
    chain = prompt_template | llm | StrOutputParser()

    # 10) æ¨è«–å®Ÿè¡Œ
    refined_text = chain.invoke({"TEXT": corrected})

    # 11) å“è³ªã‚¹ã‚³ã‚¢ç®—å‡º
    score = score_text_quality(corrected, refined_text, lang)
    return refined_text, lang, score

def score_text_quality(original: str, refined: str, lang: str) -> float:
    import re
    from langdetect import detect_langs

    def safe_len(s: str) -> int:
        return len(s.strip()) if s else 0

    orig_len = safe_len(original)
    ref_len  = safe_len(refined)
    compression = ref_len / orig_len if orig_len > 0 else 1.0

    # å¥ç‚¹ã§çµ‚ã‚ã‚‹è¡Œã®æ¯”ç‡
    lines = [line for line in refined.splitlines() if line.strip()]
    end_punct_rate = len([l for l in lines if re.search(r"[ã€‚ï¼ï¼!ï¼Ÿ?]$", l)]) / max(1, len(lines))

    # è‹±èªæ··å…¥ã®æ¯”ç‡
    lang_ratio = detect_langs(refined)
    en_ratio = next((r.prob for r in lang_ratio if r.lang == "en"), 0.0)

    score = 1.0
    if compression < 0.8:
        score -= 0.2
    if end_punct_rate < 0.3:
        score -= 0.2
    if lang == "ja" and en_ratio > 0.4:
        score -= 0.2

    return round(max(score, 0.0), 3)
