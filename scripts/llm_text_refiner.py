# scripts/llm_text_refiner.py

from src.ocr_utils.spellcheck import correct_text
from scripts.refine_prompter import get_prompt_by_lang
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langdetect import detect_langs

from src.config import DEFAULT_LLM_MODEL
from langchain_ollama import ChatOllama

# REM: Ë®ÄË™ûÂà§ÂÆöÔºàja/enÔºâ
def detect_language(text: str, force_lang=None):
    if force_lang:
        return force_lang
    try:
        lang = detect_langs(text)[0].lang
        return "en" if lang == "en" else "ja"
    except Exception:
        return "ja"

# REM: LLM„ÅßÊï¥ÂΩ¢„Åó„ÄÅË®ÄË™û„ÉªÂìÅË≥™„Çπ„Ç≥„Ç¢„ÇÇËøî„Åô
def refine_text_with_llm(raw_text: str, model=DEFAULT_LLM_MODEL, force_lang=None):
    # REM: OCRË™§Â≠óË£úÊ≠£ + Ë®ÄË™ûÂà§ÂÆö
    corrected = correct_text(raw_text)
    lang = detect_language(corrected, force_lang)
    lang = "ja"  # REM: Ëã±Ë™ûÊï¥ÂΩ¢„ÅØË°å„Çè„Åö„ÄÅÊó•Êú¨Ë™ûÊï¥ÂΩ¢„Å´Âõ∫ÂÆö

    # REM: Êï¥ÂΩ¢ÂØæË±°„ÅÆÊñáÂ≠óÊï∞„Å®Êé®ÂÆö„Éà„Éº„ÇØ„É≥Êï∞„Çí‰∫ãÂâç„Å´Ë°®Á§∫
    text_len = len(corrected)
    token_estimate = int(text_len * 1.6)

    # ‚ÄªÂÖàÈ†≠„ÅÆÊîπË°å„ÇíÂâäÈô§„Åó„ÄÅÁõ¥Âæå„Å´Á©∫Ë°å„ÇíÂÖ•„Çå„Åü„ÅÑÂ†¥Âêà„ÅØÂëº„Å≥Âá∫„ÅóÂÅ¥„Åß print("") „ÅßÂØæÂøú
    print(f"üß† LLMÊï¥ÂΩ¢„ÇíÈñãÂßãÔºàÊñáÂ≠óÊï∞: {text_len}, Êé®ÂÆö„Éà„Éº„ÇØ„É≥: {token_estimate}Ôºâ", flush=True)

    # REM: Êï¥ÂΩ¢Ë®ÄË™û„ÇíÂá∫ÂäõÔºàja/enÔºâ
    print(f"üî§ Êï¥ÂΩ¢Ë®ÄË™û: {lang}", flush=True)

    # REM: „Éó„É≠„É≥„Éó„Éà„ÉÜ„É≥„Éó„É¨„Éº„Éà„Å´ÂøÖË¶Å„Å™Â§âÊï∞ÂêçÔºàPromptTemplate ÂÅ¥„Å®Áµ±‰∏ÄÔºâ
    prompt_variable = "TEXT"

    # REM: „Éó„É≠„É≥„Éó„ÉàÂèñÂæó„Å®ÁµÑ„ÅøÁ´ã„Å¶
    _, user_prompt = get_prompt_by_lang(lang)
    full_prompt = PromptTemplate.from_template(user_prompt)

    # REM: „Éó„É¨„Éº„Çπ„Éõ„É´„ÉÄ„Å´„ÉÜ„Ç≠„Çπ„Éà„ÇíÂüã„ÇÅËæº„Åø„ÄÅÊúÄÁµÇ„Éó„É≠„É≥„Éó„ÉàÁîüÊàê
    prompt = user_prompt.replace("{TEXT}", corrected).replace("{input_text}", corrected)

    # REM: „Éó„É≠„É≥„Éó„ÉàÂêàÊàêÂæå„ÅÆÊñáÂ≠óÊï∞„ÇíË°®Á§∫Ôºà„É¢„Éá„É´Ë≤†Ëç∑„ÅÆÊåáÊ®ôÔºâ
    print(f"üßæ „Éó„É≠„É≥„Éó„ÉàÂêàÊàêÂæå„ÅÆÊñáÂ≠óÊï∞: {len(prompt)}", flush=True)

    # REM: base_url „ÇíÊòéÁ§∫„Åó„Å¶Ollama„Å®ÂÆâÂÆöÊé•Á∂ö
    llm = ChatOllama(model=DEFAULT_LLM_MODEL, base_url="http://ollama:11434")

    # REM: LangChain„ÅÆ„ÉÅ„Çß„Éº„É≥„ÅßÊï¥ÂΩ¢ÂÆüË°å
    chain = full_prompt | llm | StrOutputParser()
    refined_text = chain.invoke({prompt_variable: corrected})

    # REM: Êï¥ÂΩ¢ÂìÅË≥™„Çπ„Ç≥„Ç¢„ÇíÁÆóÂá∫„Åó„Å¶ËøîÂç¥
    score = score_text_quality(corrected, refined_text, lang)
    return refined_text, lang, score

# REM: LLMÊï¥ÂΩ¢ÁµêÊûú„Å´ÂØæ„Åô„ÇãÂìÅË≥™„Çπ„Ç≥„Ç¢„ÇíË®àÁÆó„Åô„Çã
def score_text_quality(original: str, refined: str, lang: str) -> float:
    import re
    from langdetect import detect_langs

    def safe_len(s):
        return len(s.strip()) if s else 0

    orig_len = safe_len(original)
    ref_len = safe_len(refined)
    compression = ref_len / orig_len if orig_len > 0 else 1.0

    end_punct_rate = len(
        [line for line in refined.splitlines() if re.search(r"[„ÄÇÔºéÔºÅ!Ôºü?]$", line)]
    ) / max(1, len(refined.splitlines()))

    lang_ratio = detect_langs(refined)
    en_ratio = next((r.prob for r in lang_ratio if r.lang == "en"), 0.0)

    score = 1.0
    if compression < 0.5:
        score -= 0.2
    if end_punct_rate < 0.3:
        score -= 0.2
    if lang == "ja" and en_ratio > 0.4:
        score -= 0.2

    return round(max(score, 0.0), 3)
