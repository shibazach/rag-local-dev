# scripts/views/chat_view.py

import streamlit as st
import time, datetime, threading
import numpy as np
import uuid, os
import base64  # REM: PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã«base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
from sqlalchemy import text
from io import BytesIO
from collections import defaultdict  # REM: ãƒãƒ£ãƒ³ã‚¯çµ±åˆãƒ¢ãƒ¼ãƒ‰ã®ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ç”¨
from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from sentence_transformers import SentenceTransformer

from src import bootstrap
from src.embedder import embed_and_insert
from src.config import DB_ENGINE as engine
from src.error_handler import install_global_exception_handler
from src.config import OLLAMA_MODEL, OLLAMA_BASE, EMBEDDING_OPTIONS

install_global_exception_handler()

LLM = OllamaLLM(model=OLLAMA_MODEL, base_url=OLLAMA_BASE)

# REM: Ollamaã‚’ç¶­æŒã™ã‚‹ãŸã‚ã®pingé€ä¿¡ã‚¹ãƒ¬ãƒƒãƒ‰
def keep_ollama_warm():
    while True:
        try:
            LLM.invoke("ping")
            print("âœ… Ollama keep-alive sent.")
        except Exception as e:
            print("âš ï¸ Ollama keep-alive failed:", e)
        time.sleep(600)

threading.Thread(target=keep_ollama_warm, daemon=True).start()

# REM: numpyé…åˆ—ã‚’PostgreSQLã®pgvectorå½¢å¼ã«å¤‰æ›
def to_pgvector_literal(vec):
    if isinstance(vec, np.ndarray):
        vec = vec.tolist()
    return "[" + ",".join(f"{float(x):.6f}" for x in vec) + "]"

# REM: UIéƒ¨å“ç”¨ã®ä¸€æ„ãªã‚­ãƒ¼ã‚’ç”Ÿæˆ
def make_unique_key(file_id, filename):
    safe = filename.replace(".", "_").replace(" ", "_")
    return f"{file_id}_{safe}_{uuid.uuid4().hex[:6]}"

# REM: æ¤œç´¢ç”¨ã®é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æŠ½å‡ºï¼ˆãƒãƒ£ãƒ³ã‚¯çµ±åˆãƒ¢ãƒ¼ãƒ‰ï¼‰
def search_similar_documents(query_embedding, tablename, top_k=5):
    embedding_str = to_pgvector_literal(query_embedding)
    sql = f"""
        SELECT e.content AS snippet,
               e.file_id,
               f.filename,
               f.content AS full_text,
               f.file_blob
        FROM "{tablename}" AS e
        JOIN files AS f ON e.file_id = f.file_id
        ORDER BY e.embedding <-> '{embedding_str}'::vector
        LIMIT :top_k
    """
    with engine.connect() as conn:
        result = conn.execute(text(sql), {"top_k": top_k})
        # REM: SQLAlchemy Row â†’ dict ã§ã¯ãªã mappings() ã®çµæœã‚’è¿”ã™
        return result.mappings().all()

# REM: ãƒ•ã‚¡ã‚¤ãƒ«å…¨æ–‡å–å¾—ï¼ˆfile_idæŒ‡å®šï¼‰
def get_file_content(file_id):
    with engine.connect() as conn:
        result = conn.execute(
            text("SELECT content FROM files WHERE file_id = :fid"), {"fid": file_id}
        )
        row = result.fetchone()
        return row[0] if row else ""

# REM: è³ªå•ã¨æ–‡æ›¸ã®ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢ã¨è¦ç´„ã‚’å–å¾—
def llm_summarize_with_score(query, content):
    prompt = f"""
ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨æ–‡æ›¸ã®å†…å®¹ã§ã™ã€‚

ã€è³ªå•ã€‘
{query}

ã€æ–‡æ›¸ã€‘
{content[:3000]}

ã“ã®æ–‡æ›¸ãŒè³ªå•ã«ã©ã®ç¨‹åº¦ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ï¼ˆ0.0ã€œ1.0ï¼‰ã§è©•ä¾¡ã—ã€æ¬¡ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

ä¸€è‡´åº¦: <score>
è¦ç´„: <summary>
"""
    result = LLM.invoke(prompt)
    import re
    m = re.search(r"ä¸€è‡´åº¦[:ï¼š]\s*([0-9.]+).*?è¦ç´„[:ï¼š]\s*(.+)", result, re.DOTALL)
    if m:
        return m.group(2).strip(), float(m.group(1))
    return result.strip(), 0.0

# REM: æ¤œç´¢å±¥æ­´ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆtxt/rtf/docxï¼‰
def generate_history_file(format_type="txt"):
    buffer = BytesIO()
    if format_type == "txt":
        content = ""
        for idx, h in enumerate(st.session_state.history, 1):
            content += (
                f"{idx}. è³ªå•: {h['query']}\n"
                f"ãƒ¢ãƒ‡ãƒ«: {h['model']}\n"
                f"å›ç­”ï¼ˆ{h['time']} ç§’ï¼‰:\n{h['response']}\n\n"
            )
        buffer.write(content.encode("utf-8"))
    elif format_type == "rtf":
        content = "{\\rtf1\\ansi\n"
        for idx, h in enumerate(st.session_state.history, 1):
            content += (
                f"\\b {idx}. è³ªå•: {h['query']}\\b0\\line\n"
                f"ãƒ¢ãƒ‡ãƒ«: {h['model']}\\line\n"
                f"\\i å›ç­”ï¼ˆ{h['time']} ç§’ï¼‰ï¼š\\i0\\line\n"
                f"{h['response']}\\line\\line\n"
            )
        content += "}"
        buffer.write(content.encode("utf-8"))
    else:
        from docx import Document
        from docx.shared import Pt
        doc = Document()
        for idx, h in enumerate(st.session_state.history, 1):
            doc.add_heading(f"{idx}. è³ªå•: {h['query']}", level=2)
            doc.add_paragraph(f"ãƒ¢ãƒ‡ãƒ«: {h['model']}")
            p = doc.add_paragraph()
            run = p.add_run(f"å›ç­”ï¼ˆ{h['time']} ç§’ï¼‰:\n{h['response']}")
            run.font.size = Pt(11)
        doc.save(buffer)
    buffer.seek(0)
    return buffer

# REM: ãƒ¡ã‚¤ãƒ³ç”»é¢ã®æç”»
def render_chat_view():
    st.title("\U0001F50D ãƒ­ãƒ¼ã‚«ãƒ«RAGãƒãƒ£ãƒƒãƒˆ")

    # REM: æ¤œç´¢ä¸­ãƒ•ãƒ©ã‚°åˆæœŸåŒ–
    if "searching" not in st.session_state:
        st.session_state.searching = False

    if "history" not in st.session_state:
        st.session_state.history = []
    if "query_input" not in st.session_state:
        st.session_state.query_input = ""

    rag_mode = st.radio(
        "æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠï¼š", ("ãƒãƒ£ãƒ³ã‚¯çµ±åˆ", "ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ï¼ˆè¦ç´„+ä¸€è‡´åº¦ï¼‰"), horizontal=True
    )
    model_choices = {
        f"{v['model_name']} ({v['embedder']})": k
        for k, v in EMBEDDING_OPTIONS.items()
    }
    selected_label = st.selectbox(
        "ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š", list(model_choices.keys())
    )
    selected_key = model_choices[selected_label]
    selected_model = EMBEDDING_OPTIONS[selected_key]
    model_safe = selected_model["model_name"].replace("/", "_").replace("-", "_")
    tablename = f"{model_safe}_{selected_model['dimension']}"

    left_col, right_col = st.columns([1, 1])

    with left_col:
        st.markdown("#### \U0001F50E è³ªå•ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œ")
        st.session_state.query_input = st.text_area(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", value=st.session_state.query_input, height=100
        )

        # REM: æ¤œç´¢ï¼ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒœã‚¿ãƒ³ã€‚æ¤œç´¢ä¸­ã¯æ¤œç´¢å®Ÿè¡Œã‚’ç„¡åŠ¹åŒ–ã€ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã®ã¿æœ‰åŠ¹åŒ–
        run_search = st.button(
            "æ¤œç´¢å®Ÿè¡Œ",
            key="run_search",
            disabled=st.session_state.searching
        )
        cancel_search = st.button(
            "ã‚­ãƒ£ãƒ³ã‚»ãƒ«",
            key="cancel_search",
            disabled=not st.session_state.searching
        )
        if run_search:
            st.session_state.searching = True
        if cancel_search:
            st.session_state.searching = False

        if st.session_state.searching:
            # REM: æ¤œç´¢ä¸­ã‚¤ãƒ³ã‚¸ã‚±ãƒ¼ã‚¿
            with st.spinner("ğŸ”„ æ¤œç´¢ä¸­..."):
                query = st.session_state.query_input.strip()
                if query:
                    start_time = time.time()

                    # REM: Embedding ã®å–å¾—
                    if selected_model["embedder"] == "OllamaEmbeddings":
                        embedder = OllamaEmbeddings(
                            model=selected_model["model_name"], base_url=OLLAMA_BASE
                        )
                        query_embedding = embedder.embed_query(query)
                    else:
                        embedder = SentenceTransformer(selected_model["model_name"])
                        query_embedding = embedder.encode(
                            [query], convert_to_numpy=True
                        )[0]

                    if rag_mode == "ãƒãƒ£ãƒ³ã‚¯çµ±åˆ":
                        # REM: ãƒãƒ£ãƒ³ã‚¯çµ±åˆãƒ¢ãƒ¼ãƒ‰
                        docs = search_similar_documents(
                            query_embedding, tablename
                        )
                        # REM: å¯¾è±¡ã¨ãªã£ãŸãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º
                        file_list = sorted({d["filename"] for d in docs})
                        st.markdown("**å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**: " + ", ".join(file_list))

                        # REM: ãƒ•ã‚¡ã‚¤ãƒ«æ¯ã«ãƒãƒ£ãƒ³ã‚¯ã‚’ã¾ã¨ã‚ã¦è¡¨ç¤º
                        st.markdown("### ğŸ” æ¤œç´¢çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                        docs_by_file = defaultdict(list)
                        for d in docs:
                            docs_by_file[(d["file_id"], d["filename"])].append(d)

                        for (file_id, filename), group in docs_by_file.items():
                            st.markdown(f"**{filename}**")
                            # REM: å„ãƒãƒ£ãƒ³ã‚¯ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’ãƒªã‚¹ãƒˆè¡¨ç¤º
                            for d in group:
                                st.write(f"- {d['snippet']}")
                            with st.expander("â–¶ï¸ å…¨æ–‡ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                                unique_key = make_unique_key(file_id, filename)
                                # REM: PDFãªã‚‰ãƒ–ãƒ©ã‚¦ã‚¶å†…ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                                if filename.lower().endswith(".pdf"):
                                    b64 = base64.b64encode(
                                        group[0]["file_blob"]
                                    ).decode("utf-8")
                                    iframe = (
                                        f'<iframe src="data:application/pdf;base64,{b64}" '
                                        'width="100%" height="600"></iframe>'
                                    )
                                    st.markdown(iframe, unsafe_allow_html=True)

                                # REM: ç·¨é›†ç”»é¢ã¸é·ç§»ã™ã‚‹ãƒœã‚¿ãƒ³
                                if st.button(
                                    "âœï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã™ã‚‹",
                                    key=f"gotoedit_{unique_key}",
                                ):
                                    st.session_state.edit_target_file_id = file_id
                                    st.session_state.mode = "ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†"
                                    st.experimental_rerun()

                                # REM: å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç·¨é›†å¯èƒ½ï¼‰
                                edited_text = st.text_area(
                                    "å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç·¨é›†å¯èƒ½ï¼‰",
                                    value=group[0]["full_text"],
                                    height=200,
                                    key=f"edit_{unique_key}",
                                )
                                if st.button(
                                    "ä¿å­˜ã—ã¦å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–",
                                    key=f"save_{unique_key}",
                                ):
                                    try:
                                        with engine.begin() as conn:
                                            conn.execute(
                                                text(
                                                    "UPDATE files SET content = :content WHERE file_id = :file_id"
                                                ),
                                                {
                                                    "content": edited_text,
                                                    "file_id": file_id,
                                                },
                                            )
                                            conn.execute(
                                                text(
                                                    f'DELETE FROM "{tablename}" WHERE file_id = :file_id'
                                                ),
                                                {"file_id": file_id},
                                            )
                                        st.success(
                                            "âœ… contentã‚’æ›´æ–°ã—ã€æ—§ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ"
                                        )
                                        embed_and_insert(
                                            texts=[edited_text],
                                            filename=filename,
                                            truncate_done_tables=set(),
                                        )
                                        st.success("âœ… å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†ï¼")
                                    except Exception as e:
                                        st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

                                # REM: å…ƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                                st.download_button(
                                    label="å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                    data=bytes(group[0]["file_blob"]),
                                    file_name=filename,
                                    mime="application/octet-stream",
                                    key=f"download_{unique_key}",
                                )

                    else:
                        # REM: ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ï¼ˆè¦ç´„ï¼‹ä¸€è‡´åº¦ï¼‰ãƒ¢ãƒ¼ãƒ‰
                        embedding_str = to_pgvector_literal(query_embedding)
                        sql = f"""
                            SELECT DISTINCT
                                f.file_id,
                                f.filename,
                                f.content,
                                f.file_blob,
                                MIN(e.embedding <-> '{embedding_str}'::vector) AS distance
                            FROM "{tablename}" AS e
                            JOIN files AS f ON e.file_id = f.file_id
                            GROUP BY
                                f.file_id,
                                f.filename,
                                f.content,
                                f.file_blob
                            ORDER BY distance ASC
                            LIMIT 10
                        """
                        with engine.connect() as conn:
                            # REM: mappings() ã§ RowMapping ã®ãƒªã‚¹ãƒˆã‚’å–å¾—
                            rows = conn.execute(text(sql)).mappings().all()

                        summaries = []
                        for row in rows:
                            summary, score = llm_summarize_with_score(
                                query, row["content"]
                            )
                            summaries.append(
                                {
                                    "file_id": row["file_id"],
                                    "filename": row["filename"],
                                    "summary": summary,
                                    "score": score,
                                    "file_blob": row["file_blob"],
                                }
                            )

                        summaries.sort(
                            key=lambda x: x["score"], reverse=True
                        )

                        st.markdown("### \U0001F4D1 ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®çµæœï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰")
                        for s in summaries:
                            # REM: ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦æ–°ã‚¿ãƒ–ã§PDFãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                            if s["filename"].lower().endswith(".pdf"):
                                b64 = base64.b64encode(
                                    s["file_blob"]
                                ).decode("utf-8")
                                pdf_data = f"data:application/pdf;base64,{b64}"
                                st.markdown(
                                    f'**ğŸ“„ <a href="{pdf_data}" target="_blank">{s["filename"]}</a> '
                                    f'ï¼ˆä¸€è‡´åº¦: {s["score"]:.2f}ï¼‰**',
                                    unsafe_allow_html=True,
                                )
                            else:
                                st.markdown(
                                    f"**ğŸ“„ {s['filename']}ï¼ˆä¸€è‡´åº¦: {s['score']:.2f}ï¼‰**"
                                )
                            st.write(s["summary"])

                            # REM: ç·¨é›†ç”»é¢ã¸é·ç§»ã™ã‚‹ãƒœã‚¿ãƒ³
                            if st.button(
                                "âœï¸ ç·¨é›†ã™ã‚‹", key=f"edit_{s['file_id']}"
                            ):
                                st.session_state.edit_target_file_id = s["file_id"]
                                st.session_state.mode = "ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†"
                                st.experimental_rerun()

                # REM: æ¤œç´¢å®Œäº†å¾Œã«ãƒ•ãƒ©ã‚°ã‚’æˆ»ã™
                st.session_state.searching = False

    with right_col:
        st.markdown("### ğŸ“œ æ¤œç´¢å±¥æ­´")
        if st.session_state.history:
            for i, item in enumerate(
                reversed(st.session_state.history), 1
            ):
                with st.expander(
                    f"{i}. è³ªå•: {item['query']}ï¼ˆãƒ¢ãƒ‡ãƒ«: {item['model']}ï¼‰",
                    expanded=False,
                ):
                    st.markdown(
                        f"ğŸ§  å›ç­”ï¼ˆ{item['time']} ç§’ï¼‰:\n\n{item['response']}"
                    )
        else:
            st.write("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

        st.markdown("### ğŸ’¾ å±¥æ­´ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜")
        format_type = st.selectbox(
            "ä¿å­˜å½¢å¼ã‚’é¸æŠã—ã¦ãã ã•ã„", ["txt", "rtf", "docx"]
        )
        if st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
            file_buffer = generate_history_file(format_type)
            mime_map = {
                "txt": "text/plain",
                "rtf": "application/rtf",
                "docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",  # noqa: E501
            }
            today_str = datetime.date.today().strftime("%Y-%m-%d")
            st.download_button(
                "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                data=file_buffer,
                file_name=f"search_history_{today_str}.{format_type}",
                mime=mime_map[format_type],
            )
