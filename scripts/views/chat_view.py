# chat_view.pyï¼ˆæœ€æ–°ç‰ˆï¼šå‡¦ç†ãƒ•ãƒ­ãƒ¼ãƒ»è£œåŠ©é–¢æ•°åæ˜ ã€æ—¢å­˜æ©Ÿèƒ½ï¼‹ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰

import streamlit as st
import time, datetime, threading
import numpy as np
import uuid, os
from sqlalchemy import create_engine, text
from io import BytesIO
from langchain_community.embeddings import OllamaEmbeddings
from langchain_ollama import OllamaLLM
from sentence_transformers import SentenceTransformer

from src import bootstrap
from src.embedder import embed_and_insert
from src.config import DB_ENGINE as engine
from src.error_handler import install_global_exception_handler
from src.config import OLLAMA_MODEL, OLLAMA_BASE, EMBEDDING_OPTIONS

install_global_exception_handler()

LLM = OllamaLLM(model=OLLAMA_MODEL, base_url=OLLAMA_BASE)

# REM: Ollamaã‚’ç¶­æŒã™ã‚‹ãŸã‚ã®pingé€ä¿¡ã‚¹ãƒ¬ãƒƒãƒ‰
def keep_ollama_warm():
    while True:
        try:
            LLM.invoke("ping")
            print("âœ… Ollama keep-alive sent.")
        except Exception as e:
            print("âš ï¸ Ollama keep-alive failed:", e)
        time.sleep(600)

threading.Thread(target=keep_ollama_warm, daemon=True).start()

# REM: numpyé…åˆ—ã‚’PostgreSQLã®pgvectorå½¢å¼ã«å¤‰æ›
def to_pgvector_literal(vec):
    if isinstance(vec, np.ndarray):
        vec = vec.tolist()
    return "[" + ",".join(f"{float(x):.6f}" for x in vec) + "]"

# REM: UIéƒ¨å“ç”¨ã®ä¸€æ„ãªã‚­ãƒ¼ã‚’ç”Ÿæˆ
def make_unique_key(file_id, filename):
    safe = filename.replace(".", "_").replace(" ", "_")
    return f"{file_id}_{safe}_{uuid.uuid4().hex[:6]}"

# REM: æ¤œç´¢ç”¨ã®é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯æŠ½å‡ºï¼ˆãƒãƒ£ãƒ³ã‚¯çµ±åˆãƒ¢ãƒ¼ãƒ‰ï¼‰
def search_similar_documents(query_embedding, tablename, top_k=5):
    embedding_str = to_pgvector_literal(query_embedding)
    sql = f"""
        SELECT e.content AS snippet, e.file_id, f.filename, f.content AS full_text, f.file_blob
        FROM "{tablename}" AS e
        JOIN files AS f ON e.file_id = f.file_id
        ORDER BY e.embedding <-> '{embedding_str}'::vector
        LIMIT :top_k
    """
    with engine.connect() as conn:
        result = conn.execute(text(sql), {"top_k": top_k})
        return [dict(row) for row in result.mappings()]

# REM: ãƒ•ã‚¡ã‚¤ãƒ«å…¨æ–‡å–å¾—ï¼ˆfile_idæŒ‡å®šï¼‰
def get_file_content(file_id):
    with engine.connect() as conn:
        result = conn.execute(text("SELECT content FROM files WHERE file_id = :fid"), {"fid": file_id})
        row = result.fetchone()
        return row[0] if row else ""

# REM: è³ªå•ã¨æ–‡æ›¸ã®ä¸€è‡´åº¦ã‚¹ã‚³ã‚¢ã¨è¦ç´„ã‚’å–å¾—
def llm_summarize_with_score(query, content):
    prompt = f"""
ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã¨æ–‡æ›¸ã®å†…å®¹ã§ã™ã€‚

ã€è³ªå•ã€‘
{query}

ã€æ–‡æ›¸ã€‘
{content[:3000]}

ã“ã®æ–‡æ›¸ãŒè³ªå•ã«ã©ã®ç¨‹åº¦ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ï¼ˆ0.0ã€œ1.0ï¼‰ã§è©•ä¾¡ã—ã€æ¬¡ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

ä¸€è‡´åº¦: <score>
è¦ç´„: <summary>
"""
    result = LLM.invoke(prompt)
    import re
    m = re.search(r"ä¸€è‡´åº¦[:ï¼š]\s*([0-9.]+).*?è¦ç´„[:ï¼š]\s*(.+)", result, re.DOTALL)
    if m:
        return m.group(2).strip(), float(m.group(1))
    return result.strip(), 0.0

# REM: æ¤œç´¢å±¥æ­´ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆï¼ˆtxt/rtf/docxï¼‰
def generate_history_file(format_type="txt"):
    buffer = BytesIO()
    if format_type == "txt":
        content = ""
        for idx, h in enumerate(st.session_state.history, 1):
            content += f"{idx}. è³ªå•: {h['query']}\nãƒ¢ãƒ‡ãƒ«: {h['model']}\nå›ç­”ï¼ˆ{h['time']} ç§’ï¼‰:\n{h['response']}\n\n"
        buffer.write(content.encode("utf-8"))
    elif format_type == "rtf":
        content = "{\\rtf1\\ansi\n"
        for idx, h in enumerate(st.session_state.history, 1):
            content += f"\\b {idx}. è³ªå•: {h['query']}\\b0\\line\nãƒ¢ãƒ‡ãƒ«: {h['model']}\\line\n\\i å›ç­”ï¼ˆ{h['time']} ç§’ï¼‰ï¼š\\i0\\line\n{h['response']}\\line\\line\n"
        content += "}"
        buffer.write(content.encode("utf-8"))
    else:
        from docx import Document
        from docx.shared import Pt
        doc = Document()
        for idx, h in enumerate(st.session_state.history, 1):
            doc.add_heading(f"{idx}. è³ªå•: {h['query']}", level=2)
            doc.add_paragraph(f"ãƒ¢ãƒ‡ãƒ«: {h['model']}")
            p = doc.add_paragraph()
            run = p.add_run(f"å›ç­”ï¼ˆ{h['time']} ç§’ï¼‰:\n{h['response']}")
            run.font.size = Pt(11)
        doc.save(buffer)
    buffer.seek(0)
    return buffer

# REM: ãƒ¡ã‚¤ãƒ³ç”»é¢ã®æç”»

def render_chat_view():
    st.title("\U0001F50D ãƒ­ãƒ¼ã‚«ãƒ«RAGãƒãƒ£ãƒƒãƒˆ")

    if "history" not in st.session_state:
        st.session_state.history = []
    if "query_input" not in st.session_state:
        st.session_state.query_input = ""

    rag_mode = st.radio("æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠï¼š", ("ãƒãƒ£ãƒ³ã‚¯çµ±åˆ", "ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ï¼ˆè¦ç´„+ä¸€è‡´åº¦ï¼‰"), horizontal=True)
    model_choices = {f"{v['model_name']} ({v['embedder']})": k for k, v in EMBEDDING_OPTIONS.items()}
    selected_label = st.selectbox("ä½¿ç”¨ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š", list(model_choices.keys()))
    selected_key = model_choices[selected_label]
    selected_model = EMBEDDING_OPTIONS[selected_key]
    model_safe = selected_model["model_name"].replace("/", "_").replace("-", "_")
    tablename = f"{model_safe}_{selected_model['dimension']}"

    left_col, right_col = st.columns([1, 1])

    with left_col:
        st.markdown("#### \U0001F50E è³ªå•ã‚’å…¥åŠ›ã—ã¦æ¤œç´¢ã‚’å®Ÿè¡Œ")
        st.session_state.query_input = st.text_area("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:", value=st.session_state.query_input, height=100)

        if st.button("æ¤œç´¢å®Ÿè¡Œ"):
            query = st.session_state.query_input.strip()
            if query:
                start_time = time.time()

                if selected_model["embedder"] == "OllamaEmbeddings":
                    embedder = OllamaEmbeddings(model=selected_model["model_name"], base_url=OLLAMA_BASE)
                    query_embedding = embedder.embed_query(query)
                else:
                    embedder = SentenceTransformer(selected_model["model_name"])
                    query_embedding = embedder.encode([query], convert_to_numpy=True)[0]

                if rag_mode == "ãƒãƒ£ãƒ³ã‚¯çµ±åˆ":
                    docs = search_similar_documents(query_embedding, tablename)
                    context = "\n\n".join(f"\U0001F4C4 **{d['filename']}**: {d['snippet']}" for d in docs)
                    prompt = f"""
ä»¥ä¸‹ã®æƒ…å ±ã‚’å‚è€ƒã«ã—ã¦ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

æƒ…å ±:
{context}

è³ªå•:
{query}
""".strip()
                    response = LLM.invoke(prompt)
                    elapsed_time = round(time.time() - start_time, 2)

                    st.markdown(f"### \U0001F9E0 å›ç­”ï¼ˆ{elapsed_time} ç§’ï¼‰")
                    st.write(response)

                    st.session_state.history.append({
                        "query": query,
                        "response": response,
                        "model": selected_label,
                        "time": elapsed_time
                    })

                    st.markdown("### \U0001F4D1 æ¤œç´¢çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                    for d in docs:
                        st.markdown(f"**{d['filename']}**")
                        st.write(d["snippet"])
                        with st.expander("â–¶ï¸ å…¨æ–‡ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
                            unique_key = make_unique_key(d['file_id'], d['filename'])
                            if st.button("âœï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã™ã‚‹", key=f"gotoedit_{unique_key}"):
                                st.session_state.edit_target_file_id = d["file_id"]
                                st.session_state.mode = "ãƒ•ã‚¡ã‚¤ãƒ«ç·¨é›†"
                                st.rerun()
                            edited_text = st.text_area("å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç·¨é›†å¯èƒ½ï¼‰", value=d["full_text"], height=200, key=f"edit_{unique_key}")
                            if st.button("ä¿å­˜ã—ã¦å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–", key=f"save_{unique_key}"):
                                try:
                                    with engine.begin() as conn:
                                        conn.execute(text("UPDATE files SET content = :content WHERE file_id = :file_id"),
                                                     {"content": edited_text, "file_id": d["file_id"]})
                                        conn.execute(text(f'DELETE FROM "{tablename}" WHERE file_id = :file_id'),
                                                     {"file_id": d["file_id"]})
                                    st.success("âœ… contentã‚’æ›´æ–°ã—ã€æ—§ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
                                    embed_and_insert(texts=[edited_text], filename=d["filename"], truncate_done_tables=set())
                                    st.success("âœ… å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†ï¼")
                                except Exception as e:
                                    st.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")

                            st.download_button(
                                label="å…ƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                                data=bytes(d["file_blob"]),
                                file_name=d["filename"],
                                mime="application/octet-stream",
                                key=f"download_{unique_key}"
                            )

                else:
                    embedding_str = to_pgvector_literal(query_embedding)
                    sql = f"""
                        SELECT DISTINCT f.file_id, f.filename, f.content,
                        MIN(e.embedding <-> '{embedding_str}'::vector) AS distance
                        FROM "{tablename}" AS e
                        JOIN files AS f ON e.file_id = f.file_id
                        GROUP BY f.file_id, f.filename, f.content
                        ORDER BY distance ASC
                        LIMIT 10
                    """
                    with engine.connect() as conn:
                        # REM: SQLAlchemy ã® Row â†’ dict å¤‰æ›ã«ã¯ mappings() ã‚’ä½¿ã†
                        rows = conn.execute(text(sql)).mappings().all()

                    summaries = []
                    for row in rows:
                        summary, score = llm_summarize_with_score(query, row["content"])
                        summaries.append({"file_id": row["file_id"], "filename": row["filename"], "summary": summary, "score": score})

                    summaries.sort(key=lambda x: x["score"], reverse=True)

                    st.markdown("### \U0001F4D1 ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®çµæœï¼ˆã‚¹ã‚³ã‚¢é †ï¼‰")
                    for s in summaries:
                        st.markdown(f"**ğŸ“„ {s['filename']}ï¼ˆä¸€è‡´åº¦: {s['score']:.2f}ï¼‰**")
                        st.write(s["summary"])

    with right_col:
        st.markdown("### ğŸ“œ æ¤œç´¢å±¥æ­´")
        if st.session_state.history:
            for i, item in enumerate(reversed(st.session_state.history), 1):
                with st.expander(f"{i}. è³ªå•: {item['query']}ï¼ˆãƒ¢ãƒ‡ãƒ«: {item['model']}ï¼‰", expanded=False):
                    st.markdown(f"ğŸ§  å›ç­”ï¼ˆ{item['time']} ç§’ï¼‰:\n\n{item['response']}")
        else:
            st.write("ã¾ã å±¥æ­´ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")

        st.markdown("### ğŸ’¾ å±¥æ­´ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜")
        format_type = st.selectbox("ä¿å­˜å½¢å¼ã‚’é¸æŠã—ã¦ãã ã•ã„", ["txt", "rtf", "docx"])
        if st.button("ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"):
            file_buffer = generate_history_file(format_type)
            mime_map = {
                "txt": "text/plain",
                "rtf": "application/rtf",
                "docx": "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            }
            today_str = datetime.date.today().strftime("%Y-%m-%d")
            st.download_button("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=file_buffer, file_name=f"search_history_{today_str}.{format_type}", mime=mime_map[format_type])
