# scripts/views/result_chunk.py
# =======================================================
# REM: ãƒãƒ£ãƒ³ã‚¯çµ±åˆãƒ¢ãƒ¼ãƒ‰ï¼ˆå…¨æ–‡æ¤œç´¢æ’¤å»ï¼‹ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿ï¼‰
#       - LEFT JOIN åŒ–ã§ file_blobs æ¬ æãƒ¬ã‚³ãƒ¼ãƒ‰ã‚‚ãƒ’ãƒƒãƒˆ
#       - ãƒ†ã‚­ã‚¹ãƒˆå…¨æ–‡ç·¨é›† ï¼‹ å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒœã‚¿ãƒ³ã‚’ç¶­æŒ
# =======================================================

import streamlit as st
import base64, uuid, time
import numpy as np
from collections import defaultdict
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError
from langchain_community.embeddings import OllamaEmbeddings
from sentence_transformers import SentenceTransformer

from src.config import DB_ENGINE, EMBEDDING_OPTIONS, OLLAMA_BASE   # REM: å…±é€šè¨­å®š

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# REM: ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def to_pgvector_literal(vec: np.ndarray) -> str:
    """numpyâ†’pgvector æ–‡å­—åˆ—ã«å¤‰æ›"""
    return "[" + ",".join(f"{float(x):.6f}" for x in vec.tolist()) + "]"

def make_unique_key(fid: int, fname: str) -> str:
    """Streamlit ã‚­ãƒ¼é‡è¤‡å›é¿ç”¨ãƒ¦ãƒ‹ãƒ¼ã‚¯æ–‡å­—åˆ—"""
    safe = fname.replace(".", "_")
    return f"{fid}_{safe}_{uuid.uuid4().hex[:6]}"

def vector_search(qvec: np.ndarray, table: str, top_k: int = 5) -> list[dict]:
    """ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ï¼ˆLEFT JOIN ç‰ˆï¼‰"""
    sql = f"""
      SELECT
        e.content      AS snippet,
        e.file_id,
        f.filename,
        c.refined_text AS full_text,
        b.file_blob
      FROM "{table}" AS e
      JOIN file_contents AS c ON e.file_id = c.file_id
      JOIN files          AS f ON e.file_id = f.file_id
      LEFT JOIN file_blobs AS b ON e.file_id = b.file_id        -- â˜… INNERâ†’LEFT ã«å¤‰æ›´
      ORDER BY e.embedding <-> '{to_pgvector_literal(qvec)}'::vector
      LIMIT :k
    """
    with DB_ENGINE.connect() as conn:
        return conn.execute(text(sql), {"k": top_k}).mappings().all()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# REM: ãƒ¡ã‚¤ãƒ³æç”»é–¢æ•°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_chunk_mode():
    # åˆæœŸåŒ–
    if "history" not in st.session_state:
        st.session_state.history = []

    query = st.session_state.get("query_input", "").strip()
    if not query:
        return

    key        = st.session_state["selected_model_key"]
    model_cfg  = EMBEDDING_OPTIONS[key]
    table_name = st.session_state["embedding_tablename"]

    # ãƒ‡ãƒãƒƒã‚°ï¼šãƒ†ãƒ¼ãƒ–ãƒ«ä»¶æ•°ãƒ»ã‚¯ã‚¨ãƒª
    try:
        with DB_ENGINE.connect() as conn:
            total = conn.execute(text(f'SELECT COUNT(*) FROM "{table_name}"')).scalar()
        st.write(f"ğŸ” DEBUG â€” ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ç·ãƒãƒ£ãƒ³ã‚¯: {total} ä»¶")
        st.write(f"ğŸ” DEBUG â€” ã‚¯ã‚¨ãƒª: â€œ{query}â€")
    except SQLAlchemyError as e:
        st.error(f"âŒ ä»¶æ•°å–å¾—å¤±æ•—: {e}")

    # ã‚¯ã‚¨ãƒªåŸ‹ã‚è¾¼ã¿
    if model_cfg["embedder"] == "OllamaEmbeddings":
        emb = OllamaEmbeddings(
            model    = model_cfg["model_name"],
            base_url = OLLAMA_BASE                    # REM: å›ºå®š URL
        )
        qvec = emb.embed_query(query)
    else:
        qvec = SentenceTransformer(model_cfg["model_name"]).encode(
            [query], convert_to_numpy=True
        )[0]

    # ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢
    docs = vector_search(qvec, table_name, top_k=5)
    st.success(f"ğŸ” æ¤œç´¢çµæœ: {len(docs)} ä»¶")
    if not docs:
        st.warning("âš ï¸ 0 ä»¶ãƒ’ãƒƒãƒˆã€‚ãƒ‡ãƒ¼ã‚¿ç™»éŒ²ã¾ãŸã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    # çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    grouped = defaultdict(list)
    for d in docs:
        grouped[(d["file_id"], d["filename"])].append(d)

    st.markdown("### ğŸ” æ¤œç´¢çµæœãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
    for (fid, fname), items in grouped.items():
        st.markdown(f"**ğŸ“„ {fname}**")
        for it in items:
            st.write(f"- {it['snippet']}")

        with st.expander("å…¨æ–‡ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼"):
            # PDF ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆfile_blob ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
            if fname.lower().endswith(".pdf") and items[0]["file_blob"]:
                iframe_b64 = base64.b64encode(items[0]["file_blob"]).decode()
                st.components.v1.html(
                    f'<iframe src="data:application/pdf;base64,{iframe_b64}" '
                    'width="100%" height="600" style="border:none;"></iframe>',
                    height=600
                )

            # å…¨æ–‡ç·¨é›†ï¼‹å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–
            uk = make_unique_key(fid, fname)
            edited = st.text_area("å…¨æ–‡ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç·¨é›†å¯ï¼‰",
                                  items[0]["full_text"],
                                  height=200,
                                  key=f"edit_{uk}")

            if st.button("å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–", key=f"save_{uk}"):
                from src.file_embedder import embed_and_insert
                embed_and_insert(
                    texts=[edited],
                    filepath=fname,
                    model_keys=[key],
                    ocr_raw_text=edited
                )
                st.success("âœ… å†ãƒ™ã‚¯ãƒˆãƒ«åŒ–å®Œäº†ï¼")

    # å±¥æ­´è¿½åŠ 
    st.session_state.history.append({
        "query": query,
        "response": "\n".join(d["snippet"] for d in docs),
        "model": key,
        "time": round(time.time(), 2)
    })
    st.session_state.searching = False
