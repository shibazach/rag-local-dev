# src/embedder.py

# REM: ãƒ™ã‚¯ãƒˆãƒ«åŒ–å‡¦ç†ã¨DBç™»éŒ²ã‚’è¡Œã†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
import hashlib
import os

import numpy as np
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from sentence_transformers import SentenceTransformer
from sqlalchemy.sql import text as sql_text

from src import bootstrap
from src.config import (DB_ENGINE, DEVELOPMENT_MODE, EMBEDDING_OPTIONS,
                        OLLAMA_BASE)
from src.error_handler import install_global_exception_handler

# REM: ä¾‹å¤–ç™ºç”Ÿæ™‚ã®ãƒ­ã‚°ã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«è¨˜éŒ²ã™ã‚‹ãƒãƒ³ãƒ‰ãƒ©ã‚’æœ‰åŠ¹åŒ–
install_global_exception_handler()

# REM: numpyé…åˆ—ã‚’pgvectoræ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã«å¤‰æ›
def to_pgvector_literal(vec):
    if isinstance(vec, np.ndarray):
        vec = vec.tolist()
    return "[" + ",".join(f"{float(x):.6f}" for x in vec) + "]"

# REM: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
def compute_hash(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

# REM: files ãƒ†ãƒ¼ãƒ–ãƒ«ã® TRUNCATE çŠ¶æ…‹ç®¡ç†ï¼ˆåˆå›ã®ã¿å®Ÿè¡Œï¼‰
_truncate_files_done = False

# REM: embedding_* ãƒ†ãƒ¼ãƒ–ãƒ«ã® TRUNCATE çŠ¶æ…‹ç®¡ç†ï¼ˆãƒ¢ãƒ‡ãƒ«ã”ã¨ã«1å›ã®ã¿ï¼‰
_truncate_done_tables = set()

# REM: filesãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²ã—file_idã‚’è¿”ã™
def insert_file_and_get_id(filepath, refined_ja, score, truncate_once=False):
    global _truncate_files_done

    with open(filepath, "rb") as f:
        file_blob = f.read()

    file_hash = hashlib.sha256(file_blob).hexdigest()

    with DB_ENGINE.begin() as conn:
        # REM: files ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆï¼ˆå­˜åœ¨ã—ãªã‘ã‚Œã°ï¼‰
        conn.execute(sql_text("""
            CREATE TABLE IF NOT EXISTS files (
                file_id SERIAL PRIMARY KEY,
                filename TEXT,
                content TEXT,
                file_blob BYTEA,
                quality_score FLOAT,
                file_hash TEXT UNIQUE
            )
        """))

        # REM: é–‹ç™ºãƒ¢ãƒ¼ãƒ‰ã‹ã¤åˆå›ã®ã¿ TRUNCATE å®Ÿè¡Œ
        if DEVELOPMENT_MODE and truncate_once and not _truncate_files_done:
            print("ğŸ§¨ DEVELOPMENT_MODE: files ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ TRUNCATEï¼ˆåˆå›ã®ã¿ï¼‰")
            conn.execute(sql_text("TRUNCATE TABLE files CASCADE"))
            _truncate_files_done = True

        # REM: åŒä¸€ãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã§ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®IDã‚’è¿”ã™
        existing = conn.execute(sql_text(
            "SELECT file_id FROM files WHERE file_hash = :hash"
        ), {"hash": file_hash}).fetchone()

        if existing:
            print(f"ğŸ“ file_id {existing[0]} ã‚’ files ãƒ†ãƒ¼ãƒ–ãƒ«ã‚ˆã‚Šå–å¾—")
            return existing[0]

        # REM: æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²
        result = conn.execute(sql_text("""
            INSERT INTO files (filename, content, file_blob, quality_score, file_hash)
            VALUES (:filename, :content, :file_blob, :score, :hash)
            RETURNING file_id
        """), {
            "filename": os.path.basename(filepath),
            "content": refined_ja,
            "file_blob": file_blob,
            "score": score,
            "hash": file_hash
        })
        file_id = result.scalar()
        print(f"ğŸ“ file_id {file_id} ã‚’æ–°è¦ç™»éŒ²")
        return file_id

# REM: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨DBç™»éŒ²ï¼ˆãƒ¢ãƒ‡ãƒ«æŒ‡å®šå¯¾å¿œï¼‰
def embed_and_insert(texts, filename, model_keys=None, return_data=False, quality_score=0.0):
    global _truncate_done_tables

    # REM: ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆ500æ–‡å­— + 50é‡è¤‡ï¼‰
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = [splitter.split_text(t) for t in texts]
    flat_chunks = [s for c in chunks for s in c]
    full_text = "\n".join(flat_chunks)

    # REM: files ãƒ†ãƒ¼ãƒ–ãƒ«ã¸ã®ç™»éŒ²ï¼ˆåˆå›ã®ã¿ TRUNCATEï¼‰
    file_id = insert_file_and_get_id(filename, full_text, quality_score, truncate_once=True)

    all_chunks = []
    all_embeddings = []

    for key, config in EMBEDDING_OPTIONS.items():
        # REM: æŒ‡å®šãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
        if model_keys is not None and key not in model_keys:
            continue

        print(f"ğŸ” ãƒ¢ãƒ‡ãƒ« {key}: {config['model_name']} ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ä¸­...")

        # REM: ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦åŸ‹ã‚è¾¼ã¿ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆ
        if config["embedder"] == "OllamaEmbeddings":
            embedder = OllamaEmbeddings(
                model=config["model_name"],
                base_url=OLLAMA_BASE
            )
            embeddings = embedder.embed_documents(flat_chunks)

        elif config["embedder"] == "SentenceTransformer":
            device = "cuda" if torch.cuda.is_available() else "cpu"
            embedder = SentenceTransformer(config["model_name"], device=device)
            embeddings = embedder.encode(flat_chunks, convert_to_numpy=True)

        else:
            print(f"âš ï¸ æœªå¯¾å¿œã®åŸ‹ã‚è¾¼ã¿: {config['embedder']}")
            continue

        # REM: ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ãƒ¢ãƒ‡ãƒ«åã¨æ¬¡å…ƒã‹ã‚‰ç”Ÿæˆ
        table_name = config["model_name"].replace("/", "_").replace("-", "_") + f"_{config['dimension']}"

        # REM: ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã¨åˆæœŸåŒ–ï¼ˆåˆå›ã®ã¿ï¼‰
        with DB_ENGINE.begin() as conn:
            conn.execute(sql_text(f"""
                CREATE TABLE IF NOT EXISTS "{table_name}" (
                    id SERIAL PRIMARY KEY,
                    content TEXT,
                    embedding VECTOR({config['dimension']}),
                    file_id INTEGER REFERENCES files(file_id)
                )
            """))

            if table_name not in _truncate_done_tables:
                conn.execute(sql_text(f'TRUNCATE TABLE "{table_name}" CASCADE'))
                print(f"ğŸ§¹ ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
                _truncate_done_tables.add(table_name)

            # REM: ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ãƒ™ã‚¯ãƒˆãƒ«ã¨ä¸€ç·’ã«ç™»éŒ²
            insert_sql = sql_text(f"""
                INSERT INTO "{table_name}" (content, embedding, file_id)
                VALUES (:content, :embedding, :file_id)
            """)
            records = [
                {"content": chunk, "embedding": to_pgvector_literal(vec), "file_id": file_id}
                for chunk, vec in zip(flat_chunks, embeddings)
            ]
            conn.execute(insert_sql, records)
            print(f"âœ… {len(records)} ä»¶ã‚’ {table_name} ã«æŒ¿å…¥å®Œäº†")

        if return_data:
            all_chunks.extend(flat_chunks)
            all_embeddings.extend(embeddings)

    if return_data:
        return all_chunks, all_embeddings
