# src/embedder.py
import os
import numpy as np
import hashlib
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from sentence_transformers import SentenceTransformer
from sqlalchemy import create_engine
from sqlalchemy.sql import text as sql_text

from src import bootstrap
from src.embedding_config import embedding_options

engine = create_engine("postgresql://raguser:ragpass@pgvector-db:5432/ragdb")

# REM: numpyé…åˆ—ã‚’pgvectoræ–‡å­—åˆ—ãƒªãƒ†ãƒ©ãƒ«ã«å¤‰æ›
def to_pgvector_literal(vec):
    if isinstance(vec, np.ndarray):
        vec = vec.tolist()
    return "[" + ",".join(f"{float(x):.6f}" for x in vec) + "]"

# REM: ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
def compute_hash(data: bytes) -> str:
    return hashlib.sha256(data).hexdigest()

# REM: filesãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç™»éŒ²ã—file_idã‚’è¿”ã™
def insert_file_and_get_id(filepath, refined_ja, score):
    from src.config import DEVELOPMENT_MODE
    from sqlalchemy import text as sql_text

    with open(filepath, "rb") as f:
        file_blob = f.read()

    file_hash = hashlib.sha256(file_blob).hexdigest()

    with engine.begin() as conn:
        # REM: files ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–å‰ã«å¿…ãšä½œæˆ
        conn.execute(sql_text("""
            CREATE TABLE IF NOT EXISTS files (
                file_id SERIAL PRIMARY KEY,
                filename TEXT,
                content TEXT,
                file_blob BYTEA,
                quality_score FLOAT,
                file_hash TEXT UNIQUE
            )
        """))

        if DEVELOPMENT_MODE:
            print("ğŸ§¨ DEVELOPMENT_MODE: files ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ TRUNCATE")
            conn.execute(sql_text("TRUNCATE TABLE files CASCADE"))

        existing = conn.execute(sql_text(
            "SELECT file_id FROM files WHERE file_hash = :hash"
        ), {"hash": file_hash}).fetchone()

        if existing:
            print(f"ğŸ“ file_id {existing[0]} ã‚’ files ãƒ†ãƒ¼ãƒ–ãƒ«ã‚ˆã‚Šå–å¾—")
            return existing[0]

        result = conn.execute(sql_text("""
            INSERT INTO files (filename, content, file_blob, quality_score, file_hash)
            VALUES (:filename, :content, :file_blob, :score, :hash)
            RETURNING file_id
        """), {
            "filename": os.path.basename(filepath),
            "content": refined_ja,
            "file_blob": file_blob,
            "score": score,
            "hash": file_hash
        })
        file_id = result.scalar()
        print(f"ğŸ“ file_id {file_id} ã‚’æ–°è¦ç™»éŒ²")
        return file_id

# REM: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¨DBç™»éŒ²ï¼ˆãƒ¢ãƒ‡ãƒ«æŒ‡å®šå¯¾å¿œï¼‰
def embed_and_insert(texts, filename, model_keys=None, truncate_done_tables=None, return_data=False, quality_score=0.0):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = [splitter.split_text(t) for t in texts]
    flat_chunks = [s for c in chunks for s in c]
    full_text = "\n".join(flat_chunks)

    file_id = insert_file_and_get_id(filename, full_text, quality_score)

    if truncate_done_tables is None:
        truncate_done_tables = set()

    all_chunks = []
    all_embeddings = []

    for key, config in embedding_options.items():
        # REM: æŒ‡å®šãƒ¢ãƒ‡ãƒ«ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—
        if model_keys is not None and key not in model_keys:
            continue

        print(f"ğŸ” ãƒ¢ãƒ‡ãƒ« {key}: {config['model_name']} ã«ã‚ˆã‚‹åŸ‹ã‚è¾¼ã¿ä¸­...")

        # REM: ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦åŸ‹ã‚è¾¼ã¿ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆ
        if config["embedder"] == "OllamaEmbeddings":
            embedder = OllamaEmbeddings(
                model=config["model_name"],
                base_url="http://host.docker.internal:11434"
            )
            embeddings = embedder.embed_documents(flat_chunks)
        elif config["embedder"] == "SentenceTransformer":
            device = "cuda" if torch.cuda.is_available() else "cpu"
            embedder = SentenceTransformer(config["model_name"], device=device)
            embeddings = embedder.encode(flat_chunks, convert_to_numpy=True)
        else:
            print(f"âš ï¸ æœªå¯¾å¿œã®åŸ‹ã‚è¾¼ã¿: {config['embedder']}")
            continue

        # REM: ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ç”Ÿæˆ
        table_name = config["model_name"].replace("/", "_").replace("-", "_") + f"_{config['dimension']}"

        # REM: ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã¨åˆæœŸåŒ–
        with engine.begin() as conn:
            conn.execute(sql_text(f"""
                CREATE TABLE IF NOT EXISTS "{table_name}" (
                    id SERIAL PRIMARY KEY,
                    content TEXT,
                    embedding VECTOR({config['dimension']}),
                    file_id INTEGER REFERENCES files(file_id)
                )
            """))

            if table_name not in truncate_done_tables:
                conn.execute(sql_text(f'TRUNCATE TABLE "{table_name}" CASCADE'))
                print(f"ğŸ§¹ ãƒ†ãƒ¼ãƒ–ãƒ« {table_name} ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")
                truncate_done_tables.add(table_name)

            # REM: ãƒãƒ£ãƒ³ã‚¯æ¯ã«æŒ¿å…¥
            insert_sql = sql_text(f"""
                INSERT INTO "{table_name}" (content, embedding, file_id)
                VALUES (:content, :embedding, :file_id)
            """)
            records = [
                {"content": chunk, "embedding": to_pgvector_literal(vec), "file_id": file_id}
                for chunk, vec in zip(flat_chunks, embeddings)
            ]
            conn.execute(insert_sql, records)
            print(f"âœ… {len(records)} ä»¶ã‚’ {table_name} ã«æŒ¿å…¥å®Œäº†")

        if return_data:
            all_chunks.extend(flat_chunks)
            all_embeddings.extend(embeddings)

    if return_data:
        return all_chunks, all_embeddings
